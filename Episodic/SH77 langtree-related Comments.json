{
    "guid": "7d8c298e-6319-4aef-98e9-46e0dc096a1a",
    "creation_utc": "2024-04-09T02:01:28.464275+00:00",
    "code": "SH77",
    "title": "langtree-related Comments",
    "notes": [
        {
            "guid": "e3a6c0fc-587f-43bb-b302-845061a64ad0",
            "creation_utc": "2024-04-09T02:05:03.131403+00:00",
            "code": "BX24",
            "content": [
                "Some common practice for pyddle_openai_langtree.py:",
                "",
                "* Required fields are included in the constructor (even if they are inherited; for clarity)",
                "* Some required fields have None as a default value, indicating they will be initialized internally OR default instances will be used",
                "* For \"client\", \"chat_settings\" and \"timeout\", 1) Parameter values, 2) Class field values, 3) Default values (some of which are lazy loaded) are used in this order",
                "* The classes are far from thread-safe, but each element, if accessed exclusively, should be thread-safe => Meaning we should be able to auto-generate attributes and translations in parallel",
                "* Methods that access the AI to have things generated, must NOT damage the data structure if the operation fails and an exception is raised => Fields must be updated only at the very end of the operation",
                "* As nobody wants to wait for every API call to generate the entire response, 1) For messages, we implement waiting (slow) methods and chunk streaming methods, 2) For attributes and translations, we keep the classes open and leave room for future async methods",
                "* Attributes must be extensible => Explained later",
                "* Class fields that would cause circular references are not serialized",
                "",
                "I initially thought about \"title\" and \"description\" or \"summary\" as required attributes, but if I do so and occasionally require the latter to construct the context for each API call, these fields will have to coexist with what we will add later, making the design less flexible.",
                "",
                "That is also why, not a single prompt is embedded in any of the classes.",
                "",
                "This module must be no more than an open data structure with some utility methods."
            ]
        },
        {
            "guid": "cc1b6bc3-3889-4327-9e59-37875936e0a6",
            "creation_utc": "2024-04-09T02:13:05.497472+00:00",
            "code": "GL84",
            "content": [
                "By default, all system messages are included entirely. Therefore, their summaries arent.",
                "",
                "Only the last 2 user messages are included entirely. Additionally, 2 more are, but only their summaries are.",
                "",
                "And only the last 1 assistant message is included entirely. Additionally, like user messages, 2 of their summaries are included.",
                "",
                "These are merely default values. They can and should be adjusted.",
                "",
                "So, the default values are designed to be simple, neutral and reasonable."
            ],
            "notes": [
                {
                    "guid": "062c21fd-abb6-466c-8997-08ae13736c5e",
                    "creation_utc": "2024-04-09T02:55:36.144591+00:00",
                    "code": "ON06",
                    "content": [
                        "This is merely how I personally use ChatGPT and may not be applicable to everybody else, but there must be a starting point for anything.",
                        "",
                        "When I use the web version of ChatGPT, meaning not the API, I often get a few questions about each response from the AI. I ask them one by one. The AI usually remembers the responses that have provoked the questions, but not always.",
                        "",
                        "I played with the AI a number of times. I naturally conversed with it and asked questions that were relevant to what we had discussed many messages ago. In many cases, the AI didnt remember the context.",
                        "",
                        "But when I asked the AI to make a list of (relatively) widely spoken languages that were not in the list I supplied to the AI initially, the AI responded at least 3 times with about 10 more languages each time without repeating any of the languages.",
                        "",
                        "It's safe to assume their context management algorithms are highly complex.",
                        "",
                        "They technically have unlimited access to the AI. I'm fairly confident there are version of the models that are optimized for context management, determining which messages (more precisely the internally tokenized data of them) may remain relevant to the conversation, recalling old messages when necessary, etc.",
                        "",
                        "There must be things that the AI is designed NOT to do so that it wont sound unwise. Like, repeating what the user has explained to the AI as if the AI was introducing it to the user for the first time, suggesting the same joke or anything that would emotionally relate to the user multiple times in a short period of time, etc.",
                        "",
                        "With all this and much more considered, their context management algorithms must be highly complex. That should be why the AI sometimes doesnt remember something we have just discussed while there are times when it remembers more than we expect it to.",
                        "",
                        "Unfortunately, we developers dont always have the luxury to process each response further to determine which ones are most relevant to the current state of the conversation, tripling the cost of the API usage.",
                        "",
                        "Soon, there will be a context manager module with a very lightweight local language model embedded so that we can have it choose (for free) what to send back to the API. Until then, we probably should keep it simple and stupid."
                    ]
                },
                {
                    "guid": "7185ccfe-2211-451d-9759-420332c236b0",
                    "creation_utc": "2024-04-09T03:12:24.349867+00:00",
                    "code": "UG17",
                    "content": [
                        "The reasons for the default values:",
                        "",
                        "System messages are instructions. The official sample code says \"You are a helpful assistant.\" It's just an example. We can tell the AI which language it should stick to, in what kind of tone, whether it should generate short responses or long and detailed ones, etc. These instructions must persist until the end of the conversation. So, they are all included and, therefore, their summaries arent.",
                        "",
                        "In a normal conversation, the user asks something to start it and the AI responds. At any moment when the user is asking another question, we must consider the last assistant message and the user message that has led the AI to generate it. So, there must be 2 user messages and the 1 assistant message between them. That should be the very minimum that the AI requires to avoid repeating what it has just said.",
                        "",
                        "Plus, as summaries are short and they are anyway generated for usability, it shouldnt hurt to send 2 more interactions of them. Why 2 is because the summaries are significantly shorter and one would kind of defeat the purpose of summarization while 3 wouldnt have a particular reason to be there. Short, then not one, but not three. As a result, the AI will (roughly) remember what it has said 3 messages ago.",
                        "",
                        "3 is often a good number. Only very smart people can \"really\" deal with 4, 5, 6... concepts in their mind simultaneously. For ordinary people like myself, \"room for 3\" is often enough in a casual conversation. And if we need more (like we carefully design the conversation with a pen and paper), we can easily increase the room size."
                    ]
                },
                {
                    "guid": "0de96ff6-3356-4d75-9e30-126971e69a3f",
                    "creation_utc": "2024-04-10T08:22:35.557057+00:00",
                    "code": "HD86",
                    "content": [
                        "langtree now doesnt generate or support titles or summaries.",
                        "",
                        "I tried a lot of prompts found in a lot of places, but I never found one that could work properly \"almost\" always.",
                        "",
                        "It didnt have to be 100%, but no matter what I used, it was like only 60% accurate.",
                        "",
                        "The rest were like: 1) The AI, no matter what the prompt said, explained why the input was improper and therefore it couldnt generate a title/summary, 2) Got controlled by the content of the text and started doing something else (such as answering a question the AI thought it found in it, elaborating the text just because it was short, etc), 3) Asked the user to give text to condense when one was already given (because the AI thought it's not worth condensing), 4) Generated a summary that was just as long as the original text, which kind of defeated the purpose, etc.",
                        "",
                        "I have spent hours and failed. I wont try again anytime soon.",
                        "",
                        "I think, if the text is from the AI, generally, there's not much point in trying to condense it because it's already been condensed. If it's an user input, especially something like a random thought, there should be a good point. langtree is basically a tool that saves what the AI can generate. We dont write a lot to ask questions; not much to summarize in our questions either."
                    ]
                },
                {
                    "guid": "b50cf8fc-ab2d-4ce1-81cb-8a7f687b9ac8",
                    "creation_utc": "2024-04-10T08:35:20.859299+00:00",
                    "code": "FB10",
                    "content": [
                        "So, the default values have changed as well.",
                        "",
                        "Each message's content is now split into tokens and then counted internally.",
                        "",
                        "As all current GPT models (including the turbo version of the GPT 3.5 model) can take at least 8,192 tokens, this allowance is split into 2 for user/assistant messages. Usually, our questions are a lot shorter than the answers, but we sometimes copy and paste something long and ask questions about it. So, 50-50 should be good.",
                        "",
                        "It was also an option to consider only the very total token count (of system/user/assistant messages) and set a limit on it, but then we might end up repeatedly sending excessively long AI-generated messages back to the AI and go bankrupt.",
                        "",
                        "When I freely conversed and monitored the token counts of everything realtimely, even very long responses contained like up to 1,000 tokens. So, 4,096 should be enough room.",
                        "",
                        "There's no limit on system messages. We can send as many as we want and they can be as long as they are. This is because we generally dont set long system messages and, IF we do, we are definitely doing it for a good reason.",
                        "",
                        "If we know what we are doing and send very large system messages to the AI using the turbo version of the GPT-4 model's 128,000 token context window, our choice must be respected.",
                        "",
                        "Of course, there were security concerns too. But this is Python. We can modify whatever we want and do whatever we want. So, nothing is secure anyway.",
                        "",
                        "As for numbers, system messages can be as many as they are. User/assistant messages, by default, are limited to up to 3 / request. Usually, a conversation starts with an user message and, when we are asking a question, we have one more user messages than the assistant messages, but I dont think that's very important to consider; while I was testing the module, the AI anyway responded well based on its own last 3 messages."
                    ]
                }
            ]
        }
    ]
}