<!DOCTYPE html>
<html>
    <head>
        <title>学習</title>
        <meta name="viewport" content="width=device-width, initial-scale=1" />
        <style>@import url('https://fonts.googleapis.com/css2?family=IBM+Plex+Mono&family=IBM+Plex+Sans+JP&display=swap');body{margin:0;background-color:#111;color:#ddd;font-family:'IBM Plex Sans JP',sans-serif;font-size:1rem;line-height:1.5;overflow-wrap:break-word;word-wrap:break-word}a{color:#69c;text-decoration:underline}a:hover,a:active,a:focus{font-weight:bold}div.task,div.note,div.file,div.image{margin:10px;border:1px solid transparent}div.title{background-color:#333;padding:20px 30px;font-size:2rem;font-weight:bold}@media(width< 576px){div.title{padding:15px 22.5px;font-size:1.5rem;}}a.title{color:#ddd;text-decoration:none}a.title:hover,a.title:active,a.title:focus{text-decoration:underline}div.done{background-color:#115}div.done:hover{background-color:#337}div.canceled{background-color:#511}div.canceled:hover{background-color:#733}div.task_contents{margin:10px 15px}span.task_contents{font-weight:bold}a.permalink{visibility:hidden}div.task:hover a.permalink{visibility:visible}div.note{background-color:#111}p.note_contents{margin:15px 20px}div.file{background-color:#111;padding:15px 20px}div.image{background-color:#111;padding:10px}img.image{box-sizing:border-box;vertical-align:top;max-width:100%}div.note_ai_generated{margin:10px;border:1px solid transparent;background-color:#333}div.note_ai_generated>*{margin:15px 20px}pre{background-color:#555;padding:15px 20px;white-space:pre-wrap}code{font-family:'IBM Plex Mono','IBM Plex Sans JP',monospace;font-size:1rem;line-height:1.5}</style>
    </head>
    <body>
        <div class="title">
            <a href="Learnings.htm" class="title">学習</a>
        </div>
        <div class="entries">
            <div id="6b62e6be-c450-45f3-84c7-b85d02200106" class="task done">
                <div class="task_contents">&check;
                    <span class="task_contents">作りたい複数のウェブシステムの作り分けについて AI に聞いてみる</span>
                    <a href="#6b62e6be-c450-45f3-84c7-b85d02200106" class="permalink">&infin;</a>
                </div>
                <div class="notes">
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>Deciding whether to develop a Customer Relationship Management (CRM) system, Content Management System (CMS), and a Product Database as separate systems or to integrate them into a single unified platform depends on various factors including your organization's needs, future scalability, budget, and the complexity of each system. Here are some pros and cons for each approach to help you make an informed decision.</p>
<h3>Separate Systems</h3>
<h4>Pros:</h4>
<ol>
<li><strong>Specialization:</strong> Each system can be highly specialized for its respective purpose.</li>
<li><strong>Flexibility:</strong> You can select the best-of-breed systems for each function.</li>
<li><strong>Ease of Replacement:</strong> If you want to replace one system, you don't have to make changes to the others.</li>
<li><strong>Isolated Failures:</strong> If one system goes down, the others can still function.</li>
<li><strong>Parallel Development:</strong> Different teams can work on each system simultaneously.</li>
</ol>
<h4>Cons:</h4>
<ol>
<li><strong>Data Silos:</strong> Information may become isolated in each system, making it difficult to have a unified view of data.</li>
<li><strong>Integration Complexity:</strong> Separate systems may require complex integrations and data synchronization, which could be error-prone and time-consuming.</li>
<li><strong>User Experience:</strong> Users may need to switch between multiple systems to perform their duties, reducing productivity and user satisfaction.</li>
<li><strong>Maintenance:</strong> More systems mean more platforms to update, secure, and maintain.</li>
</ol>
<h3>Unified System</h3>
<h4>Pros:</h4>
<ol>
<li><strong>Single Source of Truth:</strong> A unified system can provide a centralized location for all data and functionality.</li>
<li><strong>Ease of Use:</strong> Users have to learn and interface with only one system.</li>
<li><strong>Simplified Data Management:</strong> A single system can reduce the complexity of data integration and provide easier data analytics.</li>
<li><strong>Lower Total Cost of Ownership:</strong> In the long run, maintaining a single system could be cheaper than maintaining multiple specialized systems.</li>
<li><strong>Streamlined Workflows:</strong> Better process integration can lead to more efficient workflows.</li>
</ol>
<h4>Cons:</h4>
<ol>
<li><strong>Complexity:</strong> A unified system might be highly complex to develop and maintain.</li>
<li><strong>Lack of Specialization:</strong> A one-size-fits-all system might not meet the specialized needs that individual systems can provide.</li>
<li><strong>Single Point of Failure:</strong> If the unified system goes down, all functions go down.</li>
<li><strong>Time to Market:</strong> Developing a comprehensive, integrated system might take more time initially.</li>
</ol>
<h3>Considerations:</h3>
<ol>
<li><strong>Organizational Needs:</strong> How tightly are these systems' functionalities linked to your organization's day-to-day operations?</li>
<li><strong>Budget:</strong> What are the short-term and long-term costs involved in both scenarios?</li>
<li><strong>Scalability:</strong> How quickly is your organization growing, and what are the scalability requirements for each system?</li>
<li><strong>Customization:</strong> How much customization is needed in each system?</li>
<li><strong>Compliance and Security:</strong> Are there any compliance or security issues that might affect your decision?</li>
</ol>
<h3>Conclusion</h3>
<p>Both approaches have merits. Separate systems offer flexibility and specialization, whereas a unified system offers a more streamlined user experience and potentially lower long-term costs. Your specific needs, priorities, and constraints will dictate the best choice for your situation. You could also consider a hybrid approach where you develop separate systems but design them with strong interoperability features, allowing them to be more easily integrated in the future.</p>
                            </div>
                        </div>
                    </div>
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>If you choose to develop separate systems for CRM, CMS, and a Product Database, you'll need a well-thought-out strategy for enabling these systems to sync and share data. Below are some commonly used methods for achieving synchronization and integration among disparate systems:</p>
<h3>Application Programming Interfaces (APIs)</h3>
<ol>
<li><strong>RESTful APIs</strong>: APIs enable one software to fetch data from or send data to another. RESTful APIs are stateless and work over HTTP(s), making them easy to implement.</li>
<li><strong>GraphQL APIs</strong>: Provides more flexibility than REST APIs by allowing clients to request only the data they need.</li>
<li><strong>SOAP APIs</strong>: These are protocol-based APIs often used for more complex transactions and may offer features like enhanced security.</li>
</ol>
<h3>Batch Processing</h3>
<ol>
<li><strong>Scheduled Jobs</strong>: Perform batch updates at specific times (e.g., nightly). This is often slower and may result in data being slightly out-of-date.</li>
<li><strong>CSV Import/Export</strong>: Useful for bulk data transfer but requires manual intervention.</li>
</ol>
<h3>Real-time Synchronization</h3>
<ol>
<li><strong>Webhooks</strong>: These are real-time HTTP callbacks. When a change occurs in one system, it sends an HTTP request to another system to notify it of the change.</li>
<li><strong>Streaming APIs / WebSockets</strong>: These allow real-time, two-way communication between systems.</li>
<li><strong>Message Queues</strong>: Systems like RabbitMQ, Kafka, or AWS SQS can queue messages that are then processed to synchronize data.</li>
</ol>
<h3>Middleware</h3>
<ol>
<li><strong>Integration Platforms</strong>: Platforms like Apache Camel, MuleSoft, or Dell Boomi can act as a middle layer to handle communication between systems.</li>
<li><strong>Enterprise Service Bus (ESB)</strong>: Facilitates communication among various services within an enterprise application.</li>
</ol>
<h3>Database-Level Integration</h3>
<ol>
<li><strong>Database Views</strong>: Creating common views that draw data from multiple databases.</li>
<li><strong>Database Triggers</strong>: These can be used to automatically update one database when changes are made to another.</li>
<li><strong>Master Data Management (MDM)</strong>: Centralized data repository that syncs data across different systems.</li>
</ol>
<h3>Best Practices</h3>
<ol>
<li><strong>Data Mapping</strong>: Ensure that data fields in different systems correspond correctly to each other.</li>
<li><strong>Data Transformation</strong>: Data from one system may need to be transformed to be compatible with another.</li>
<li><strong>Error Handling</strong>: Implement robust error-handling to manage sync failures.</li>
<li><strong>Audit Logs</strong>: Keep logs for tracking changes, failures, or unusual activities.</li>
<li><strong>Security</strong>: Ensure that data transfers are secure, using encryption and secure channels.</li>
<li><strong>Compliance</strong>: Ensure that data sharing complies with legal and business policies, such as GDPR for data protection.</li>
</ol>
<p>The specific synchronization approach you choose will depend on various factors including:</p>
<ul>
<li>The volume of data to be synced</li>
<li>Real-time vs batch processing needs</li>
<li>Complexity and type of data</li>
<li>Budget and resources</li>
<li>Expertise in certain technologies</li>
</ul>
<p>Often, a combination of these methods will offer the most robust and flexible solution. Keep in mind that data synchronization can be complex, so it's often worth investing in a well-planned architecture and possibly consultation with experts in the field.</p>
                            </div>
                        </div>
                    </div>
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>In C#, there are several techniques to share functionality between two separate systems without duplicating code. Here are some approaches you might consider:</p>
<h3>1. Shared Libraries</h3>
<p>Create a shared library (DLL) that contains the common functionality and reference that library in both systems. This is one of the most straightforward ways to reuse code.</p>
<pre><code class="language-csharp">// In SharedLibrary
public class SharedFunctionality
{
    public void DoSomething()
    {
        // ... shared code here
    }
}

// In System1 and System2
SharedFunctionality shared = new SharedFunctionality();
shared.DoSomething();
</code></pre>
<h3>2. NuGet Packages</h3>
<p>If your shared code needs to be version-controlled and possibly shared among multiple projects or even teams, consider packaging it as a NuGet package.</p>
<h3>3. Inheritance</h3>
<p>If the two systems have classes that need to share functionality but also have unique behaviors, you can use inheritance.</p>
<pre><code class="language-csharp">public class BaseFunctionality
{
    public void CommonMethod()
    {
        // ... common code
    }
}

public class System1Specific : BaseFunctionality
{
    public void UniqueMethod()
    {
        // ... unique code for System1
    }
}

public class System2Specific : BaseFunctionality
{
    public void AnotherUniqueMethod()
    {
        // ... unique code for System2
    }
}
</code></pre>
<h3>4. Composition</h3>
<p>You can use composition to build classes with shared behaviors without having to inherit from a common base class.</p>
<pre><code class="language-csharp">public class SharedFunctionality
{
    public void DoSomething()
    {
        // ... shared code
    }
}

public class System1Class
{
    private readonly SharedFunctionality _sharedFunctionality;

    public System1Class(SharedFunctionality sharedFunctionality)
    {
        _sharedFunctionality = sharedFunctionality;
    }

    public void DoSomething()
    {
        _sharedFunctionality.DoSomething();
    }
}
</code></pre>
<h3>5. Dependency Injection</h3>
<p>Dependency Injection (DI) can be used to inject shared functionality into various parts of your applications, making it easier to manage shared code.</p>
<h3>6. Partial Classes</h3>
<p>C# supports partial classes, which allows you to split the definition of a class over multiple files. Though generally not recommended for this purpose, it can sometimes be useful when auto-generated code is involved.</p>
<h3>7. Interfaces and Abstract Classes</h3>
<p>If the shared functionality involves a contract (methods that must be implemented but can have different implementations), interfaces or abstract classes may be used.</p>
<pre><code class="language-csharp">public interface ISharedFunctionality
{
    void DoSomething();
}
</code></pre>
<h3>8. Source Generators</h3>
<p>With C# 9.0 and later, you can use source generators to generate source code that can be included in multiple projects. This approach is more advanced and might be overkill for simple scenarios.</p>
<h3>Best Practices:</h3>
<ul>
<li>Keep the Single Responsibility Principle in mind to make sure that your shared code modules do one thing and do it well.</li>
<li>Use version control to manage your shared code.</li>
<li>Write unit tests for the shared code to ensure that changes don't break functionality in the systems that use it.</li>
</ul>
<p>By using one or a combination of these methods, you can significantly reduce code duplication across multiple systems in C#.</p>
                            </div>
                        </div>
                    </div>
                    <div class="note">
                        <div class="note_contents">
                            <p class="note_contents">技術的なところは分かった。それぞれについて「こう動いてほしい」という UI の検討に入る。</p>
                        </div>
                    </div>
                </div>
            </div>
            <div id="15cb0aa7-8476-4cfc-90b3-beb1dcf5ad80" class="task done">
                <div class="task_contents">&check;
                    <span class="task_contents">RESTful について調べる</span>
                    <a href="#15cb0aa7-8476-4cfc-90b3-beb1dcf5ad80" class="permalink">&infin;</a>
                </div>
                <div class="notes">
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>In the context of computer science and web development, RESTful refers to an application or service that adheres to the principles of REST (Representational State Transfer). REST is an architectural style for creating networked applications. RESTful services and APIs (Application Programming Interfaces) are designed to be stateless and to use HTTP methods explicitly, among other principles.</p>
<p>Here are some of the key principles and characteristics of a RESTful system:</p>
<h3>Stateless</h3>
<p>Each request from a client to a server must contain all the information needed to understand and process the request. There is no stored &quot;session&quot; on the server side; all session state is kept on the client.</p>
<h3>Client-Server Architecture</h3>
<p>The client and server are separate entities that communicate over a network. The separation allows each to be developed and deployed independently of one another.</p>
<h3>Cacheability</h3>
<p>Responses from the server can be explicitly marked as cacheable or not cacheable. If a response is cacheable, then the client cache can store and reuse the response for equivalent future requests.</p>
<h3>Uniform Interface</h3>
<p>RESTful services have a uniform interface, which helps to decouple the client and server. The primary or most commonly used HTTP methods in RESTful systems are GET, POST, PUT, and DELETE.</p>
<ul>
<li><code>GET</code> retrieves a resource.</li>
<li><code>POST</code> creates a new resource.</li>
<li><code>PUT</code> updates an existing resource.</li>
<li><code>DELETE</code> removes a resource.</li>
</ul>
<h3>Resource-Based</h3>
<p>In a RESTful system, resources are key abstractions. Each resource is identified by a specific URI (Uniform Resource Identifier). These resources can be manipulated using the standard HTTP methods.</p>
<h3>Statelessness and Connective State</h3>
<p>Because RESTful services are stateless, they are often easier to scale horizontally. Each request is independent and can be handled by any available server.</p>
<h3>Layered System</h3>
<p>REST allows for a layered architecture where each layer has a specific functionality and operates independently of the other layers. This allows for greater flexibility and maintainability.</p>
<h3>Code-On-Demand (Optional)</h3>
<p>Server can extend the functionality of a client by transferring executable code.</p>
<h3>Use of Standard Conventions</h3>
<p>RESTful services often use standard conventions for actions (HTTP methods), paths (URL structure), and payload formats (usually JSON or XML), although these are not strictly enforced.</p>
<p>RESTful APIs have become a popular choice for building web services due to their simplicity, scalability, and ease of integration.</p>
                            </div>
                        </div>
                    </div>
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>Creating a RESTful API in C# is often done using the ASP.NET Core framework, although there are other libraries and frameworks that can be used. ASP.NET Core offers a robust set of features for building server-side web applications, and it provides excellent support for building RESTful APIs.</p>
<p>Here's a simple example of how to create a RESTful API using ASP.NET Core. This example assumes that you have some familiarity with C# and Visual Studio.</p>
<h3>Setting up the Project</h3>
<ol>
<li>Open Visual Studio and create a new project.</li>
<li>Choose the &quot;ASP.NET Core Web API&quot; template.</li>
<li>Name your project and click &quot;Create&quot;.</li>
</ol>
<h3>Building a Simple Model</h3>
<p>Create a simple <code>TodoItem</code> class. This class will serve as the model for our API.</p>
<pre><code class="language-csharp">public class TodoItem
{
    public long Id { get; set; }
    public string Name { get; set; }
    public bool IsComplete { get; set; }
}
</code></pre>
<h3>Creating the Controller</h3>
<ol>
<li>In the &quot;Controllers&quot; folder, add a new controller called <code>TodoController</code>.</li>
<li>Populate it as follows:</li>
</ol>
<pre><code class="language-csharp">using Microsoft.AspNetCore.Mvc;
using System.Collections.Generic;

namespace YourNamespace.Controllers
{
    [Route(&quot;api/[controller]&quot;)]
    [ApiController]
    public class TodoController : ControllerBase
    {
        // In-memory data store for simplicity
        private static readonly List&lt;TodoItem&gt; TodoItems = new List&lt;TodoItem&gt;()
        {
            new TodoItem() { Id = 1, Name = &quot;Item 1&quot;, IsComplete = false }
        };

        // GET: api/Todo
        [HttpGet]
        public ActionResult&lt;IEnumerable&lt;TodoItem&gt;&gt; GetTodoItems()
        {
            return TodoItems;
        }

        // GET: api/Todo/1
        [HttpGet(&quot;{id}&quot;)]
        public ActionResult&lt;TodoItem&gt; GetTodoItem(long id)
        {
            var item = TodoItems.Find(x =&gt; x.Id == id);
            if (item == null)
            {
                return NotFound();
            }

            return item;
        }

        // POST: api/Todo
        [HttpPost]
        public ActionResult&lt;TodoItem&gt; CreateTodoItem(TodoItem item)
        {
            item.Id = TodoItems.Count + 1;
            TodoItems.Add(item);
            return CreatedAtAction(nameof(GetTodoItem), new { id = item.Id }, item);
        }

        // PUT: api/Todo/1
        [HttpPut(&quot;{id}&quot;)]
        public IActionResult UpdateTodoItem(long id, TodoItem item)
        {
            var existingItem = TodoItems.Find(x =&gt; x.Id == id);
            if (existingItem == null)
            {
                return NotFound();
            }

            existingItem.Name = item.Name;
            existingItem.IsComplete = item.IsComplete;

            return NoContent();
        }

        // DELETE: api/Todo/1
        [HttpDelete(&quot;{id}&quot;)]
        public IActionResult DeleteTodoItem(long id)
        {
            var item = TodoItems.Find(x =&gt; x.Id == id);
            if (item == null)
            {
                return NotFound();
            }

            TodoItems.Remove(item);
            return NoContent();
        }
    }
}
</code></pre>
<h3>Running the API</h3>
<ol>
<li>Build the project.</li>
<li>Run the application.</li>
<li>Use Postman or a similar tool to test your API endpoints.</li>
</ol>
<p>You can make HTTP requests to <code>http://localhost:&lt;port&gt;/api/Todo</code> to interact with the API. Use GET, POST, PUT, and DELETE requests to test the functionalities.</p>
<p>This is a very basic example just to get you started. In a real-world application, you would typically use a database for storing your <code>TodoItem</code> objects and implement additional features like authentication, validation, logging, etc.</p>
                            </div>
                        </div>
                    </div>
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>To consume a RESTful API in a local C# program, you can use the <code>HttpClient</code> class provided by the .NET framework. The <code>HttpClient</code> class is a part of the <code>System.Net.Http</code> namespace and is designed to send HTTP requests and receive HTTP responses from a resource identified by a URI.</p>
<p>Here's a simple example that shows how you can perform CRUD (Create, Read, Update, Delete) operations on the <code>Todo</code> API we discussed earlier.</p>
<h3>Steps to Follow:</h3>
<ol>
<li>Create a new C# Console App (.NET Core or .NET 5/6).</li>
<li>Add the <code>System.Net.Http</code> NuGet package if it's not already added.</li>
</ol>
<h3>Create a <code>TodoItem</code> class</h3>
<p>This class should match the model used in your API.</p>
<pre><code class="language-csharp">public class TodoItem
{
    public long Id { get; set; }
    public string Name { get; set; }
    public bool IsComplete { get; set; }
}
</code></pre>
<h3>Use <code>HttpClient</code> to Access the API</h3>
<p>Here's sample code that demonstrates how to perform CRUD operations using <code>HttpClient</code>.</p>
<pre><code class="language-csharp">using System;
using System.Net.Http;
using System.Text;
using System.Threading.Tasks;
using Newtonsoft.Json;

namespace TodoApiClient
{
    class Program
    {
        static async Task Main(string[] args)
        {
            HttpClient client = new HttpClient();

            // Base URL for your API
            string baseUrl = &quot;http://localhost:5000/api/Todo/&quot;;

            // GET all TodoItems
            var response = await client.GetStringAsync(baseUrl);
            Console.WriteLine($&quot;All items: {response}&quot;);

            // POST a new TodoItem
            TodoItem newItem = new TodoItem { Name = &quot;New Item&quot;, IsComplete = false };
            var json = JsonConvert.SerializeObject(newItem);
            var data = new StringContent(json, Encoding.UTF8, &quot;application/json&quot;);
            var postResponse = await client.PostAsync(baseUrl, data);
            Console.WriteLine($&quot;POST: {postResponse.StatusCode}&quot;);

            // Update (PUT) a TodoItem
            TodoItem updateItem = new TodoItem { Id = 1, Name = &quot;Updated Item&quot;, IsComplete = true };
            var updateJson = JsonConvert.SerializeObject(updateItem);
            var updateData = new StringContent(updateJson, Encoding.UTF8, &quot;application/json&quot;);
            var putResponse = await client.PutAsync(baseUrl + &quot;1&quot;, updateData);
            Console.WriteLine($&quot;PUT: {putResponse.StatusCode}&quot;);

            // DELETE a TodoItem
            var deleteResponse = await client.DeleteAsync(baseUrl + &quot;1&quot;);
            Console.WriteLine($&quot;DELETE: {deleteResponse.StatusCode}&quot;);
        }
    }
}
</code></pre>
<p>In this example, the base URL <code>http://localhost:5000/api/Todo/</code> assumes that your API is running locally on port 5000. You'll need to update this to the actual URL and port where your API is running.</p>
<p>I used Newtonsoft.Json to serialize and deserialize JSON objects. You can install this package from NuGet Package Manager Console with:</p>
<pre><code class="language-bash">Install-Package Newtonsoft.Json -Version 13.0.1
</code></pre>
<p>Don't forget to update the version number to the latest version if needed.</p>
<p>This is just a basic example. In a real-world application, you'd probably want to add proper error handling, possibly use async/await more extensively, and perhaps encapsulate the API calls into a dedicated service class.</p>
                            </div>
                        </div>
                    </div>
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>When interacting with a remote API, several types of errors can occur. It's good practice to account for these errors and handle them gracefully. Here are some common issues that might arise, along with techniques to handle them:</p>
<h3>1. Network Errors</h3>
<p>The application might not be able to establish a connection due to network issues or if the API server is down.</p>
<h4>Handling</h4>
<p>You can catch the <code>HttpRequestException</code> to handle such errors:</p>
<pre><code class="language-csharp">try
{
    var response = await client.GetStringAsync(baseUrl);
}
catch (HttpRequestException e)
{
    Console.WriteLine($&quot;Request error: {e.Message}&quot;);
}
</code></pre>
<h3>2. Timeout</h3>
<p>The request might timeout if the server takes too long to respond.</p>
<h4>Handling</h4>
<p>You can set a timeout duration for the <code>HttpClient</code>:</p>
<pre><code class="language-csharp">client.Timeout = TimeSpan.FromSeconds(30);
</code></pre>
<h3>3. Non-success Status Codes</h3>
<p>Even if the request is sent successfully, the server might return a status code indicating failure (like 404 Not Found, 401 Unauthorized, etc.)</p>
<h4>Handling</h4>
<p>You can check the <code>IsSuccessStatusCode</code> property of the <code>HttpResponseMessage</code>:</p>
<pre><code class="language-csharp">var response = await client.GetAsync(baseUrl);
if (!response.IsSuccessStatusCode)
{
    Console.WriteLine($&quot;Server returned error: {response.StatusCode}&quot;);
}
</code></pre>
<h3>4. Deserialization Errors</h3>
<p>There might be an error while deserializing the response if the server response does not match the expected format.</p>
<h4>Handling</h4>
<p>Catch <code>JsonSerializationException</code> or <code>JsonReaderException</code> if you're using Newtonsoft.Json:</p>
<pre><code class="language-csharp">try
{
    var todo = JsonConvert.DeserializeObject&lt;TodoItem&gt;(responseString);
}
catch (JsonSerializationException e)
{
    Console.WriteLine($&quot;Error deserializing response: {e.Message}&quot;);
}
</code></pre>
<h3>5. Unhandled Exceptions</h3>
<p>There might be other kinds of exceptions like <code>OutOfMemoryException</code>, <code>ArgumentNullException</code>, etc., based on the situation.</p>
<h4>Handling</h4>
<p>Catch general <code>Exception</code>:</p>
<pre><code class="language-csharp">try
{
    // code here
}
catch (Exception e)
{
    Console.WriteLine($&quot;An error occurred: {e.Message}&quot;);
}
</code></pre>
<h3>Complete Example with Error Handling</h3>
<pre><code class="language-csharp">try
{
    // GET all TodoItems
    var response = await client.GetAsync(baseUrl);
    if (response.IsSuccessStatusCode)
    {
        var responseString = await response.Content.ReadAsStringAsync();
        Console.WriteLine($&quot;All items: {responseString}&quot;);
    }
    else
    {
        Console.WriteLine($&quot;Server returned error: {response.StatusCode}&quot;);
    }
}
catch (HttpRequestException e)
{
    Console.WriteLine($&quot;Request error: {e.Message}&quot;);
}
catch (JsonSerializationException e)
{
    Console.WriteLine($&quot;Error deserializing response: {e.Message}&quot;);
}
catch (Exception e)
{
    Console.WriteLine($&quot;An error occurred: {e.Message}&quot;);
}
</code></pre>
<h3>Summary</h3>
<p>Proper error handling can greatly improve the reliability and user experience of your application. By anticipating and coding for various types of errors, you can make your application more robust and easier to maintain.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <div id="89414dde-68eb-4352-956b-aee7fd4bbc6b" class="task done">
                <div class="task_contents">&check;
                    <span class="task_contents">GraphQL について調べる</span>
                    <a href="#89414dde-68eb-4352-956b-aee7fd4bbc6b" class="permalink">&infin;</a>
                </div>
                <div class="notes">
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>GraphQL is a query language and runtime for APIs, developed by Facebook in 2012 and released as an open-source project in 2015. It provides a more flexible and efficient alternative to the traditional REST API. With GraphQL, you can specify exactly what data you need, which can reduce the amount of data transferred over the network. It also allows you to get all the data you need in a single request, rather than making multiple requests to different endpoints as is common in RESTful services.</p>
<h3>Key Concepts</h3>
<ol>
<li><p><strong>Query</strong>: The request for specific data, analogous to REST's GET method. You can specify exactly what data you want.</p>
<pre><code class="language-graphql">query {
  user(id: 1) {
    name
    age
  }
}
</code></pre>
</li>
<li><p><strong>Mutation</strong>: Operations that cause changes on the server, like creating, updating, or deleting data, analogous to REST's POST, PUT, DELETE methods.</p>
<pre><code class="language-graphql">mutation {
  createUser(name: &quot;John&quot;, age: 30) {
    id
    name
  }
}
</code></pre>
</li>
<li><p><strong>Schema</strong>: Defines the types, queries, and mutations in a GraphQL API. It's like a blueprint that describes what operations and data types are allowed.</p>
</li>
<li><p><strong>Resolver</strong>: A function on the server that specifies how to fetch the data for a particular field in the schema.</p>
</li>
<li><p><strong>Subscription</strong>: Allows real-time updates from the server to the client for particular events.</p>
</li>
</ol>
<h3>Advantages</h3>
<ol>
<li><strong>Reduced Overfetching</strong>: Only fetch the data that is actually needed.</li>
<li><strong>Strongly Typed Schema</strong>: API is described in a strongly typed schema language, providing a contract between the server and client.</li>
<li><strong>Introspection</strong>: You can query the schema for details about itself, which makes it self-documenting.</li>
<li><strong>Single Endpoint</strong>: Unlike REST, which often has multiple endpoints for different resources, GraphQL usually exposes a single endpoint for all interactions.</li>
</ol>
<h3>Disadvantages</h3>
<ol>
<li><strong>Complexity</strong>: For simple APIs, GraphQL can be overkill.</li>
<li><strong>Rate Limiting</strong>: Traditional REST APIs can easily limit the number of requests per endpoint but limiting the cost of a GraphQL query can be more complex.</li>
<li><strong>File Uploading</strong>: Built-in file upload functionality is not as straightforward as in REST.</li>
</ol>
<h3>Popular Libraries/Frameworks</h3>
<ol>
<li><strong>Server-side</strong>: Apollo Server (JavaScript), Sangria (Scala), GraphQL-Ruby, GraphQL .NET (C#)</li>
<li><strong>Client-side</strong>: Apollo Client (JavaScript), Relay (React-specific), urql (React-specific)</li>
</ol>
<h3>C# and GraphQL</h3>
<p>In the .NET ecosystem, the GraphQL .NET library can be used to build a GraphQL API. It allows you to define your schema in C# code and also provides utilities to easily connect with Entity Framework or other data sources.</p>
<h3>Basic Example in C#</h3>
<p>For a quick start in C#, you can use the NuGet package <code>GraphQL.Server.Ui.Playground</code> to set up a GraphQL playground and <code>GraphQL</code> for the core GraphQL implementation.</p>
<p>Define your types and schema, create resolvers, and then expose a single GraphQL endpoint. Clients can then query this endpoint to retrieve data or perform mutations.</p>
<p>Here is a simple example to set up a GraphQL API using ASP.NET Core:</p>
<ol>
<li><p>Install the GraphQL NuGet package:</p>
<pre><code class="language-bash">Install-Package GraphQL -Version 4.6.1
</code></pre>
</li>
<li><p>Create a simple model:</p>
<pre><code class="language-csharp">public class User
{
    public int Id { get; set; }
    public string Name { get; set; }
}
</code></pre>
</li>
<li><p>Create a query class:</p>
<pre><code class="language-csharp">public class UserQuery : ObjectGraphType
{
    public UserQuery()
    {
        Field&lt;UserType&gt;(&quot;user&quot;, arguments: new QueryArguments(new QueryArgument&lt;IntGraphType&gt; { Name = &quot;id&quot; }), resolve: context =&gt; {
            var id = context.GetArgument&lt;int&gt;(&quot;id&quot;);
            return new User { Id = id, Name = &quot;John Doe&quot; };
        });
    }
}
</code></pre>
</li>
<li><p>Create a user type:</p>
<pre><code class="language-csharp">public class UserType : ObjectGraphType&lt;User&gt;
{
    public UserType()
    {
        Field(x =&gt; x.Id, type: typeof(IntGraphType)).Description(&quot;User Id.&quot;);
        Field(x =&gt; x.Name).Description(&quot;User name.&quot;);
    }
}
</code></pre>
</li>
<li><p>Create a schema:</p>
<pre><code class="language-csharp">public class UserSchema : Schema
{
    public UserSchema(IDependencyResolver resolver) : base(resolver)
    {
        Query = resolver.Resolve&lt;UserQuery&gt;();
    }
}
</code></pre>
</li>
<li><p>Register the schema in <code>Startup.cs</code>:</p>
<pre><code class="language-csharp">services.AddScoped&lt;IDependencyResolver&gt;(x =&gt; new FuncDependencyResolver(type =&gt; (GraphType) serviceProvider.GetService(type)));
services.AddScoped&lt;UserSchema&gt;();
</code></pre>
</li>
<li><p>Expose a GraphQL endpoint:</p>
<pre><code class="language-csharp">app.UseGraphQL&lt;UserSchema&gt;(&quot;/graphql&quot;);
</code></pre>
</li>
</ol>
<p>This is just a basic introduction to GraphQL in C#, but hopefully, it gives you a good starting point. There are many more features and possibilities to explore, such as mutations, subscriptions, and integration with databases.</p>
                            </div>
                        </div>
                    </div>
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>Performing CRUD (Create, Read, Update, Delete) operations in GraphQL involves defining both Queries and Mutations in your GraphQL schema. Below is a simple example illustrating CRUD operations in a GraphQL API using C# with ASP.NET Core. This example uses the GraphQL.NET library.</p>
<h3>1. Install NuGet Packages</h3>
<p>First, you'll need to install a few NuGet packages:</p>
<pre><code class="language-bash">Install-Package GraphQL -Version 4.6.1
Install-Package GraphQL.Server.Transports.AspNetCore -Version 5.1.0
Install-Package GraphQL.Server.Ui.Playground -Version 5.1.0
</code></pre>
<h3>2. Create a Model</h3>
<p>Create a simple <code>Book</code> model:</p>
<pre><code class="language-csharp">public class Book
{
    public int Id { get; set; }
    public string Title { get; set; }
    public string Author { get; set; }
}
</code></pre>
<h3>3. Create GraphQL Types</h3>
<p>Create GraphQL object types for the Book model:</p>
<pre><code class="language-csharp">using GraphQL.Types;

public class BookType : ObjectGraphType&lt;Book&gt;
{
    public BookType()
    {
        Field(x =&gt; x.Id, type: typeof(IdGraphType)).Description(&quot;Book id.&quot;);
        Field(x =&gt; x.Title).Description(&quot;The title of the book.&quot;);
        Field(x =&gt; x.Author).Description(&quot;The author of the book.&quot;);
    }
}
</code></pre>
<h3>4. Create the Schema</h3>
<p>Now define the schema, queries, and mutations.</p>
<pre><code class="language-csharp">using GraphQL;
using GraphQL.Types;

public class BookSchema : Schema
{
    public BookSchema(IDependencyResolver resolver) : base(resolver)
    {
        Query = resolver.Resolve&lt;BookQuery&gt;();
        Mutation = resolver.Resolve&lt;BookMutation&gt;();
    }
}
</code></pre>
<h3>5. Implement Queries and Mutations</h3>
<p>Implement your CRUD operations in separate classes for Queries and Mutations:</p>
<h4>Queries (Read)</h4>
<pre><code class="language-csharp">public class BookQuery : ObjectGraphType
{
    public BookQuery()
    {
        Field&lt;BookType&gt;(
            &quot;book&quot;,
            arguments: new QueryArguments(new QueryArgument&lt;NonNullGraphType&lt;IdGraphType&gt;&gt; { Name = &quot;id&quot; }),
            resolve: context =&gt;
            {
                int id = context.GetArgument&lt;int&gt;(&quot;id&quot;);
                return new Book { Id = id, Title = &quot;Some Title&quot;, Author = &quot;Some Author&quot; }; // Fetch from database
            }
        );
    }
}
</code></pre>
<h4>Mutations (Create, Update, Delete)</h4>
<pre><code class="language-csharp">public class BookMutation : ObjectGraphType
{
    public BookMutation()
    {
        // Create
        Field&lt;BookType&gt;(
            &quot;createBook&quot;,
            arguments: new QueryArguments(
                new QueryArgument&lt;NonNullGraphType&lt;StringGraphType&gt;&gt; { Name = &quot;title&quot; },
                new QueryArgument&lt;NonNullGraphType&lt;StringGraphType&gt;&gt; { Name = &quot;author&quot; }
            ),
            resolve: context =&gt;
            {
                string title = context.GetArgument&lt;string&gt;(&quot;title&quot;);
                string author = context.GetArgument&lt;string&gt;(&quot;author&quot;);
                return new Book { Id = 1, Title = title, Author = author }; // Add to database and return new book
            }
        );

        // Update
        Field&lt;BookType&gt;(
            &quot;updateBook&quot;,
            arguments: new QueryArguments(
                new QueryArgument&lt;NonNullGraphType&lt;IdGraphType&gt;&gt; { Name = &quot;id&quot; },
                new QueryArgument&lt;NonNullGraphType&lt;StringGraphType&gt;&gt; { Name = &quot;title&quot; },
                new QueryArgument&lt;NonNullGraphType&lt;StringGraphType&gt;&gt; { Name = &quot;author&quot; }
            ),
            resolve: context =&gt;
            {
                int id = context.GetArgument&lt;int&gt;(&quot;id&quot;);
                string title = context.GetArgument&lt;string&gt;(&quot;title&quot;);
                string author = context.GetArgument&lt;string&gt;(&quot;author&quot;);
                return new Book { Id = id, Title = title, Author = author }; // Update in database and return updated book
            }
        );

        // Delete
        Field&lt;StringGraphType&gt;(
            &quot;deleteBook&quot;,
            arguments: new QueryArguments(new QueryArgument&lt;NonNullGraphType&lt;IdGraphType&gt;&gt; { Name = &quot;id&quot; }),
            resolve: context =&gt;
            {
                int id = context.GetArgument&lt;int&gt;(&quot;id&quot;);
                // Delete from database
                return $&quot;Deleted book with id {id}&quot;;
            }
        );
    }
}
</code></pre>
<h3>6. Register the Types and Schema</h3>
<p>In <code>Startup.cs</code>, add the following code to register the GraphQL schema and types:</p>
<pre><code class="language-csharp">public void ConfigureServices(IServiceCollection services)
{
    services.AddScoped&lt;IDependencyResolver&gt;(x =&gt; new FuncDependencyResolver(type =&gt; serviceProvider.GetService(type)));
    services.AddScoped&lt;BookSchema&gt;();
    services.AddGraphQL(x =&gt;
    {
        x.ExposeExceptions = true; // expose exceptions for debugging
    }).AddGraphTypes(ServiceLifetime.Scoped);
}

public void Configure(IApplicationBuilder app, IWebHostEnvironment env)
{
    app.UseGraphQL&lt;BookSchema&gt;();
    app.UseGraphQLPlayground(new GraphQLPlaygroundOptions()); // to explore API
}
</code></pre>
<p>After you complete these steps, you should be able to run your application and navigate to the GraphQL Playground (typically hosted at <code>http://localhost:5000/ui/playground</code>).</p>
<p>In the Playground, you can perform CRUD operations by executing GraphQL queries and mutations.</p>
<h4>Create a Book</h4>
<pre><code class="language-graphql">mutation {
  createBook(title: &quot;New Book&quot;, author: &quot;Some Author&quot;) {
    id
    title
    author
  }
}
</code></pre>
<h4>Read a Book</h4>
<pre><code class="language-graphql">query {
  book(id: 1) {
    id
    title
    author
  }
}
</code></pre>
<h4>Update a Book</h4>
<pre><code class="language-graphql">mutation {
  updateBook(id: 1, title: &quot;Updated Title&quot;, author: &quot;Updated Author&quot;) {
    id
    title
    author
  }
}
</code></pre>
<h4>Delete a Book</h4>
<pre><code class="language-graphql">mutation {
  deleteBook(id: 1)
}
</code></pre>
<p>Remember that in a real-world application, you'd be hooking up these queries and mutations to a database to perform the actual CRUD operations. This is just a simple example to get you started.</p>
                            </div>
                        </div>
                    </div>
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>File upload in GraphQL isn't a part of the official GraphQL specification. However, there are several community conventions for handling file uploads. One popular approach is using the <code>multipart/form-data</code> method to handle file uploads alongside GraphQL mutations.</p>
<h3>Server-Side: Setting up ASP.NET Core and GraphQL</h3>
<p>In the context of an ASP.NET Core application using GraphQL.NET, you can implement file upload functionality by using an additional NuGet package:</p>
<pre><code class="language-bash">Install-Package GraphQL.Upload.AspNetCore -Version 1.3.0
</code></pre>
<p>After installing the package, update your <code>Startup.cs</code> to register GraphQL Upload middleware:</p>
<pre><code class="language-csharp">public void Configure(IApplicationBuilder app, IWebHostEnvironment env)
{
    app.UseGraphQLUpload();
    app.UseGraphQL&lt;YourSchema&gt;();
    // ...
}
</code></pre>
<h3>Create a File Type and Mutation</h3>
<p>You can define a custom scalar to represent the uploaded file. The mutation would typically use the custom scalar as an argument.</p>
<p>First, create a new type to represent the uploaded file:</p>
<pre><code class="language-csharp">public class FileType : ScalarGraphType
{
    public FileType()
    {
        Name = &quot;Upload&quot;;
        Description = &quot;File upload scalar type.&quot;;
    }

    // Implement serialization and parsing logic here
}
</code></pre>
<p>Now, add a mutation that uses this type:</p>
<pre><code class="language-csharp">public class FileUploadMutation : ObjectGraphType
{
    public FileUploadMutation()
    {
        Field&lt;BooleanGraphType&gt;(
            &quot;uploadFile&quot;,
            arguments: new QueryArguments(
                new QueryArgument&lt;NonNullGraphType&lt;FileType&gt;&gt; { Name = &quot;file&quot; }
            ),
            resolve: context =&gt;
            {
                var file = context.GetArgument&lt;IFormFile&gt;(&quot;file&quot;);

                // Implement your file storage logic here

                return true;
            }
        );
    }
}
</code></pre>
<p>Don't forget to add this mutation type to your GraphQL schema.</p>
<h3>Client-Side: Uploading the File</h3>
<p>On the client-side, you'll need to make a <code>multipart/form-data</code> POST request that includes the mutation query and the file. This can be done in JavaScript with libraries like <code>apollo-client</code> or other HTTP clients capable of making multipart requests.</p>
<p>Here's how you could perform the upload using JavaScript's <code>fetch</code> API:</p>
<pre><code class="language-javascript">const query = `
  mutation ($file: Upload!) {
    uploadFile(file: $file)
  }
`;

const map = {
  &quot;0&quot;: [&quot;variables.file&quot;]
};

const body = new FormData();
body.append(&quot;operations&quot;, JSON.stringify({ query, variables: { file: null }}));
body.append(&quot;map&quot;, JSON.stringify(map));
body.append(&quot;0&quot;, fileInput.files[0]);

fetch('/graphql', {
  method: 'POST',
  body
})
.then(response =&gt; response.json())
.then(data =&gt; console.log(data))
.catch(error =&gt; console.error('Error:', error));
</code></pre>
<p>Replace <code>/graphql</code> with your GraphQL endpoint and <code>fileInput</code> with your file input element.</p>
<p>Remember that file uploads are typically complex operations that can involve additional concerns like validation, file type checks, and security precautions. Always ensure you handle these concerns in your implementation.</p>
                            </div>
                        </div>
                    </div>
                    <div class="note">
                        <div class="note_contents">
                            <p class="note_contents">かなりめんどくさい印象。ギリギリまで転送量を抑えないといけないシステムでないなら、REST でよさそう。</p>
                        </div>
                    </div>
                </div>
            </div>
            <div id="e5feee5c-f688-4611-ad34-38f36b7625ef" class="task done">
                <div class="task_contents">&check;
                    <span class="task_contents">REST でファイルをアップロードする方法を調べる</span>
                    <a href="#e5feee5c-f688-4611-ad34-38f36b7625ef" class="permalink">&infin;</a>
                </div>
                <div class="notes">
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>Uploading files in a RESTful API is relatively straightforward, especially when using ASP.NET Core. In this architecture, file upload is generally done using the <code>multipart/form-data</code> content type, which allows you to send files as part of the HTTP request.</p>
<p>Here's a simplified example that demonstrates how to implement file upload in a RESTful web service using ASP.NET Core:</p>
<h3>Server-Side: Setting up ASP.NET Core API for File Upload</h3>
<h4>1. Create a new ASP.NET Core API project or open an existing one.</h4>
<h4>2. Add an API Controller to handle file uploads.</h4>
<p>Here is how your API Controller might look:</p>
<pre><code class="language-csharp">using Microsoft.AspNetCore.Http;
using Microsoft.AspNetCore.Mvc;
using System.IO;
using System.Threading.Tasks;

namespace FileUploadDemo.Controllers
{
    [Route(&quot;api/[controller]&quot;)]
    [ApiController]
    public class FilesController : ControllerBase
    {
        [HttpPost(&quot;upload&quot;)]
        public async Task&lt;IActionResult&gt; Upload(IFormFile file)
        {
            // Validate the incoming file if needed (size, file type etc.)

            var path = Path.Combine(Directory.GetCurrentDirectory(), &quot;uploads&quot;, file.FileName);

            using (var stream = new FileStream(path, FileMode.Create))
            {
                await file.CopyToAsync(stream);
            }

            return Ok(new { Message = &quot;File uploaded successfully&quot; });
        }
    }
}
</code></pre>
<p>This code snippet includes a single API endpoint for POST requests to <code>/api/files/upload</code>. This endpoint expects a file as part of the request and saves it to an &quot;uploads&quot; directory in the application's root.</p>
<h3>Client-Side: Making the Upload Request</h3>
<p>You can upload a file to this RESTful API using various HTTP client libraries or tools like <code>curl</code>, Postman, or programmatically using languages and frameworks that can make HTTP requests.</p>
<p>Here is how you can use C# to upload a file to the API:</p>
<pre><code class="language-csharp">using System;
using System.Net.Http;
using System.Threading.Tasks;

class Program
{
    static async Task Main(string[] args)
    {
        using (var client = new HttpClient())
        {
            using (var content = new MultipartFormDataContent())
            {
                var filePath = &quot;C:\\path\\to\\your\\file.txt&quot;;

                // Load the file into a `StreamContent` and add it to the `MultipartFormDataContent`
                var fileContent = new StreamContent(System.IO.File.OpenRead(filePath));
                fileContent.Headers.ContentDisposition = new System.Net.Http.Headers.ContentDispositionHeaderValue(&quot;form-data&quot;)
                {
                    Name = &quot;\&quot;file\&quot;&quot;,
                    FileName = &quot;\&quot;file.txt\&quot;&quot;
                };
                content.Add(fileContent);

                // Make the HTTP request
                var response = await client.PostAsync(&quot;http://localhost:5000/api/files/upload&quot;, content);

                // Read and output the response
                var responseString = await response.Content.ReadAsStringAsync();
                Console.WriteLine(responseString);
            }
        }
    }
}
</code></pre>
<p>Replace <code>&quot;http://localhost:5000/api/files/upload&quot;</code> with the actual URL where your API is running, and <code>filePath</code> with the full path to the file you want to upload.</p>
<p>That's it! This is a basic example to demonstrate file uploading in a RESTful web service using ASP.NET Core. In real-world applications, you would typically add additional logic for error handling, file validation, and security measures.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <div class="note">
                <div class="note_contents">
                    <div class="note_ai_generated">
<p>The terms &quot;REST&quot; and &quot;RESTful&quot; are often used interchangeably, but they refer to slightly different concepts.</p>
<h3>REST (Representational State Transfer)</h3>
<p>REST is an architectural style for designing distributed systems. It was introduced in 2000 by Roy Fielding in his doctoral dissertation. REST is not tied to a specific technology or programming language; instead, it provides a set of architectural principles and constraints. These principles include statelessness, client-server architecture, cacheability, a uniform interface, and the use of standard HTTP methods (GET, POST, PUT, DELETE, etc.).</p>
<p>A system that adheres to these constraints can be considered a RESTful system. REST itself is not a standard or protocol, but it leverages standards like HTTP, URI, and MIME.</p>
<h3>RESTful</h3>
<p>The term &quot;RESTful&quot; is used to describe web services that adhere to the principles and constraints of REST. When a web service is designed and implemented in a manner consistent with the REST architecture, it's commonly referred to as a RESTful web service. These services use standard HTTP methods, are stateless between requests, and work with resources (objects or data) that have associated URI endpoints.</p>
<p>RESTful APIs are implemented using HTTP and can be accessed through standard HTTP methods. They often return data in JSON or XML format, although they can return data in other formats as well.</p>
<h3>Summary</h3>
<ul>
<li><strong>REST</strong> is an architectural style with a set of constraints for building distributed systems.</li>
<li><strong>RESTful</strong> describes any web service that follows the REST architectural principles.</li>
</ul>
<p>Both terms aim to promote a standard way of building web services, making them easier to understand and consume.</p>
                    </div>
                    <!-- Task: 8989a666-6422-4e62-8fee-00e5e5b340f0 -->
                </div>
            </div>
            <div id="91aeef14-cbdb-4dc1-b915-c630d38253f0" class="task done">
                <div class="task_contents">&check;
                    <span class="task_contents">SOAP について調べる</span>
                    <a href="#91aeef14-cbdb-4dc1-b915-c630d38253f0" class="permalink">&infin;</a>
                </div>
                <div class="notes">
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>SOAP (Simple Object Access Protocol) is a protocol for exchanging structured information in the implementation of web services in computer networks. Unlike REST, which is an architectural style, SOAP is a standardized protocol defined by the World Wide Web Consortium (W3C). It relies on XML for its message format and typically relies on other application layer protocols, most notably Hypertext Transfer Protocol (HTTP) and Simple Mail Transfer Protocol (SMTP), for message negotiation and transmission.</p>
<h3>Key Characteristics of SOAP:</h3>
<ol>
<li><p><strong>Standardized Protocol</strong>: SOAP has strict standards, which are defined by the W3C. This ensures that it integrates across a wide variety of languages and platforms.</p>
</li>
<li><p><strong>Built-in Features</strong>: SOAP includes built-in features like security (WS-Security), transactional messaging, message patterns, ACID-compliant transactions, and more.</p>
</li>
<li><p><strong>Strictly Typed</strong>: The SOAP messages are usually strictly typed, and the contract is defined via a Web Services Description Language (WSDL) document. This WSDL is an XML-based document that describes the operations the service offers, the input and output parameters, and the binding information.</p>
</li>
<li><p><strong>Fault Handling</strong>: SOAP messages have built-in error handling in the form of &quot;Fault Elements.&quot;</p>
</li>
<li><p><strong>Transport-Neutral</strong>: Although it's commonly used over HTTP/HTTPS, SOAP can be run over other transport protocols as well.</p>
</li>
<li><p><strong>Complexity</strong>: Due to its strict standards and numerous built-in features, SOAP is generally considered to be more complex and heavyweight compared to REST.</p>
</li>
</ol>
<h3>When to Use SOAP?</h3>
<ol>
<li><p><strong>Enterprise-Level Applications</strong>: When you need ACID-compliant transactions, SOAP can be a good fit.</p>
</li>
<li><p><strong>Strong Typed Contract</strong>: If you want the safety of a strongly typed contract between client and server, SOAP and its WSDL provide this out of the box.</p>
</li>
<li><p><strong>Built-in Features</strong>: If you need built-in security, reliability, or atomic transactions, SOAP has these features in-built (WS-Security, WS-Reliable Messaging, WS-Atomic Transaction).</p>
</li>
<li><p><strong>Interoperability</strong>: For older systems with strict and complex operations and transactions, SOAP can be a good choice due to its strict standards.</p>
</li>
</ol>
<h3>How SOAP Differs from REST:</h3>
<ol>
<li><p><strong>Standard vs. Architectural Style</strong>: SOAP is a protocol with a specific set of rules and specifications. REST is an architectural style that leverages standard practices within HTTP, but doesn't have a standard set of rules.</p>
</li>
<li><p><strong>Message Format</strong>: SOAP uses XML as its message format, which includes a lot of meta-information. REST typically uses JSON, which is easier to parse and generally more lightweight.</p>
</li>
<li><p><strong>Complexity</strong>: SOAP is generally considered to be more complex due to its strict rules and standards.</p>
</li>
<li><p><strong>Flexibility</strong>: REST is more flexible and allows for more freedom in the design of APIs. With SOAP, you're generally tied to a specific contract as defined in the WSDL.</p>
</li>
<li><p><strong>Transport</strong>: SOAP can use multiple transport protocols, while REST is generally tied to HTTP/HTTPS.</p>
</li>
<li><p><strong>Built-In Features</strong>: SOAP comes with a comprehensive specification that includes several built-in features for security, transactions, messaging patterns, etc., which are optional in REST.</p>
</li>
</ol>
<p>Both SOAP and REST have their place in the world of web services and can be the better choice depending on the specific requirements of a task.</p>
                            </div>
                        </div>
                    </div>
                    <div class="note">
                        <div class="note_contents">
                            <p class="note_contents">大昔にチョロッとさわり、誰も使っていないように感じてやめた WCF が SOAP だったようだ。</p>
                            <div class="note_ai_generated">
<p>Performing CRUD (Create, Read, Update, Delete) operations via a SOAP web service in C# is generally straightforward once you have added the service reference to your project. Here's how you can do CRUD operations using SOAP:</p>
<h3>Server-Side (WCF Service Example)</h3>
<p>Let's consider a simple WCF (Windows Communication Foundation) SOAP service that offers CRUD operations on a &quot;Product&quot; entity:</p>
<ol>
<li>Create a new WCF service application in Visual Studio.</li>
<li>Add a service contract and the corresponding methods.</li>
</ol>
<p>Here's a simple service contract in the <code>IService1.cs</code> file:</p>
<pre><code class="language-csharp">using System.ServiceModel;

[ServiceContract]
public interface IProductService
{
    [OperationContract]
    Product CreateProduct(Product product);

    [OperationContract]
    Product ReadProduct(int id);

    [OperationContract]
    Product UpdateProduct(Product product);

    [OperationContract]
    bool DeleteProduct(int id);
}

public class Product
{
    public int Id { get; set; }
    public string Name { get; set; }
}
</code></pre>
<p>In <code>Service1.svc.cs</code>, implement these methods:</p>
<pre><code class="language-csharp">public class ProductService : IProductService
{
    // Implementation with simple in-memory store for demonstration purposes
    private static Dictionary&lt;int, Product&gt; store = new Dictionary&lt;int, Product&gt;();
    private static int counter = 1;

    public Product CreateProduct(Product product)
    {
        product.Id = counter++;
        store[product.Id] = product;
        return product;
    }

    public Product ReadProduct(int id)
    {
        if (store.ContainsKey(id))
        {
            return store[id];
        }
        return null;
    }

    public Product UpdateProduct(Product product)
    {
        if (store.ContainsKey(product.Id))
        {
            store[product.Id] = product;
            return product;
        }
        return null;
    }

    public bool DeleteProduct(int id)
    {
        return store.Remove(id);
    }
}
</code></pre>
<h3>Client-Side (Console Application Example)</h3>
<p>After you have the service running, you can add a service reference to your client-side console application.</p>
<ol>
<li>Right-click on &quot;References&quot; and choose &quot;Add Service Reference.&quot;</li>
<li>Add the WSDL URL and proceed.</li>
</ol>
<p>Here is how you can perform CRUD operations from the client:</p>
<pre><code class="language-csharp">using System;

namespace SoapCrudClient
{
    class Program
    {
        static void Main(string[] args)
        {
            // Initialize the client
            ProductService.ProductServiceClient client = new ProductService.ProductServiceClient();

            // Create a new product
            ProductService.Product newProduct = new ProductService.Product
            {
                Name = &quot;Laptop&quot;
            };
            var createdProduct = client.CreateProduct(newProduct);
            Console.WriteLine($&quot;Created Product: {createdProduct.Id}, {createdProduct.Name}&quot;);

            // Read a product
            var readProduct = client.ReadProduct(createdProduct.Id);
            if (readProduct != null)
            {
                Console.WriteLine($&quot;Read Product: {readProduct.Id}, {readProduct.Name}&quot;);
            }

            // Update a product
            readProduct.Name = &quot;Desktop&quot;;
            var updatedProduct = client.UpdateProduct(readProduct);
            if (updatedProduct != null)
            {
                Console.WriteLine($&quot;Updated Product: {updatedProduct.Id}, {updatedProduct.Name}&quot;);
            }

            // Delete a product
            bool isDeleted = client.DeleteProduct(updatedProduct.Id);
            Console.WriteLine(isDeleted ? &quot;Product deleted successfully&quot; : &quot;Failed to delete the product&quot;);
        }
    }
}
</code></pre>
<p>In this example, I've used a simple in-memory store to keep track of the products. In a real-world application, you'd typically use a database for persistence.</p>
<p>Note: Before running the client, make sure the WCF service is up and running. Also, update the service endpoint in the client's app.config if necessary.</p>
<p>This is a very basic example but should give you a good starting point for performing CRUD operations using SOAP in C#.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <div id="6f9393a9-9041-4a6d-9185-8c2b82166b55" class="task done">
                <div class="task_contents">&check;
                    <span class="task_contents">ACID について調べる</span>
                    <a href="#6f9393a9-9041-4a6d-9185-8c2b82166b55" class="permalink">&infin;</a>
                </div>
                <div class="notes">
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>ACID is an acronym that stands for Atomicity, Consistency, Isolation, and Durability. These are the properties that ensure reliable database transactions. ACID properties are especially important in a distributed system to ensure that failures don't lead to inconsistencies or lost data. Here's a breakdown of what each of the properties represents:</p>
<h3>Atomicity</h3>
<p>Atomicity ensures that a transaction is treated as a single unit, which means either all of its operations are executed or none are. For example, in a money transfer operation from one bank account to another, atomicity ensures that if a debit operation fails, the subsequent credit operation is not executed.</p>
<h3>Consistency</h3>
<p>Consistency ensures that a transaction takes a database from one consistent state to another. This means that all data will meet all defined rules, including constraints, cascades, and triggers during the transaction.</p>
<h3>Isolation</h3>
<p>Isolation ensures that concurrent execution of transactions leaves the database in the same state that would have been obtained if the transactions were executed sequentially. This is crucial for correct processing in a multiuser system.</p>
<h3>Durability</h3>
<p>Durability means that once a transaction has been committed, it remains so even in the face of system failures. This often means that completed transactions are saved in a non-volatile memory.</p>
<h3>ACID in Context</h3>
<ol>
<li><p><strong>Databases</strong>: ACID properties are most commonly associated with relational database management systems. Many RDBMS software like MySQL, PostgreSQL, and SQL Server offer varying degrees of ACID compliance.</p>
</li>
<li><p><strong>Distributed Systems</strong>: Ensuring ACID properties in a distributed system, where a transaction might span multiple servers, is more complex but also crucial for maintaining data integrity.</p>
</li>
<li><p><strong>Transactions in Programming</strong>: ACID properties can also be relevant in the context of non-database transactions, like manipulating in-memory data structures.</p>
</li>
<li><p><strong>Web Services</strong>: SOAP-based web services can offer ACID-compliant transactions using protocols like WS-AtomicTransaction.</p>
</li>
</ol>
<p>Ensuring ACID compliance can have a performance impact, so some systems might allow you to configure the level of compliance depending on your needs. However, any system that requires reliability and data integrity should aim to be ACID-compliant.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <div id="93b3b03b-96ea-4f40-b696-3357cb2ac233" class="task done">
                <div class="task_contents">&check;
                    <span class="task_contents">WSDL について調べる</span>
                    <a href="#93b3b03b-96ea-4f40-b696-3357cb2ac233" class="permalink">&infin;</a>
                </div>
                <div class="notes">
                    <div class="note">
                        <div class="note_contents">
                            <p class="note_contents">自分がやるとは思えない。</p>
                            <div class="note_ai_generated">
<p>WSDL stands for Web Services Description Language. It's an XML-based language for describing web services and how to access them. WSDL serves as a contract between the service provider and the service consumers, specifying the operations that the web service offers. WSDL is commonly used in SOAP-based web services, though it's theoretically possible to use it with other web services as well.</p>
<h3>Components of a WSDL Document</h3>
<p>A WSDL document includes the following key elements:</p>
<ol>
<li><p><strong>Types</strong>: Defines the data types that are used by the web service.</p>
</li>
<li><p><strong>Message</strong>: An abstract, reusable definition of the data being communicated. Messages are associated with specific operations.</p>
</li>
<li><p><strong>Port Type</strong>: An abstract set of operations supported by one or more endpoints.</p>
</li>
<li><p><strong>Binding</strong>: Specifies concrete protocol and data format specifications for the operations and messages defined by a particular port type.</p>
</li>
<li><p><strong>Service</strong>: Defines a collection of related endpoints, where an endpoint refers to a specific address for a binding.</p>
</li>
<li><p><strong>Operations</strong>: Represents the functionality offered by the service. An operation is defined by a set of input and output message types.</p>
</li>
</ol>
<h3>Sample WSDL Example</h3>
<p>Here's an extremely simplified example of a WSDL document for a service that has a single operation, <code>Add</code>, which takes two integers as parameters and returns their sum.</p>
<pre><code class="language-xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;definitions xmlns=&quot;http://schemas.xmlsoap.org/wsdl/&quot;
             xmlns:tns=&quot;http://www.example.org/SimpleService/&quot;
             xmlns:xsd=&quot;http://www.w3.org/2001/XMLSchema&quot;
             xmlns:soap=&quot;http://schemas.xmlsoap.org/wsdl/soap/&quot;
             targetNamespace=&quot;http://www.example.org/SimpleService/&quot;&gt;

  &lt;types&gt;
    &lt;!-- Definition of types (usually would link to an XSD document) --&gt;
  &lt;/types&gt;

  &lt;message name=&quot;AddInput&quot;&gt;
    &lt;part name=&quot;a&quot; type=&quot;xsd:int&quot;/&gt;
    &lt;part name=&quot;b&quot; type=&quot;xsd:int&quot;/&gt;
  &lt;/message&gt;

  &lt;message name=&quot;AddOutput&quot;&gt;
    &lt;part name=&quot;result&quot; type=&quot;xsd:int&quot;/&gt;
  &lt;/message&gt;

  &lt;portType name=&quot;SimplePortType&quot;&gt;
    &lt;operation name=&quot;Add&quot;&gt;
      &lt;input message=&quot;tns:AddInput&quot;/&gt;
      &lt;output message=&quot;tns:AddOutput&quot;/&gt;
    &lt;/operation&gt;
  &lt;/portType&gt;

  &lt;binding name=&quot;SimpleBinding&quot; type=&quot;tns:SimplePortType&quot;&gt;
    &lt;soap:binding style=&quot;rpc&quot; transport=&quot;http://schemas.xmlsoap.org/soap/http&quot;/&gt;
    &lt;operation name=&quot;Add&quot;&gt;
      &lt;soap:operation soapAction=&quot;Add&quot;/&gt;
      &lt;input&gt;
        &lt;soap:body use=&quot;literal&quot;/&gt;
      &lt;/input&gt;
      &lt;output&gt;
        &lt;soap:body use=&quot;literal&quot;/&gt;
      &lt;/output&gt;
    &lt;/operation&gt;
  &lt;/binding&gt;

  &lt;service name=&quot;SimpleService&quot;&gt;
    &lt;port name=&quot;SimplePort&quot; binding=&quot;tns:SimpleBinding&quot;&gt;
      &lt;soap:address location=&quot;http://www.example.org/SimpleService/&quot; /&gt;
    &lt;/port&gt;
  &lt;/service&gt;

&lt;/definitions&gt;
</code></pre>
<h3>Using WSDL in C#</h3>
<p>When you're consuming a SOAP service in C#, you can add a service reference to your project by pointing it to the WSDL URL of the service you're interested in. The tooling in Visual Studio will generate proxy classes that allow you to easily call the service operations.</p>
<ol>
<li>Right-click the &quot;References&quot; folder in your project in the Solution Explorer.</li>
<li>Choose &quot;Add Service Reference...&quot;</li>
<li>In the dialog that appears, enter the URL to the WSDL in the &quot;Address&quot; field.</li>
<li>Click &quot;Go&quot; to fetch the WSDL and generate the proxy classes.</li>
<li>Once the service and its operations are listed, click &quot;OK&quot; to add the service reference to your project.</li>
</ol>
<p>After doing this, you can use the generated proxy classes in your code to interact with the web service.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <div id="e3466a22-1bf1-4334-9462-245ba25e7e93" class="task done">
                <div class="task_contents">&check;
                    <span class="task_contents">RabbitMQ について調べる</span>
                    <a href="#e3466a22-1bf1-4334-9462-245ba25e7e93" class="permalink">&infin;</a>
                </div>
                <div class="notes">
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>RabbitMQ is a message broker that allows applications to communicate with each other through asynchronous messaging patterns. It's commonly used for building real-time systems and works well for distributed systems where components need to be decoupled. RabbitMQ implements the Advanced Message Queuing Protocol (AMQP), but it also supports other messaging protocols like MQTT, STOMP, and more.</p>
<h3>Basic Concepts</h3>
<ol>
<li><strong>Producer</strong>: The part of the application that sends messages.</li>
<li><strong>Consumer</strong>: The part of the application that receives messages.</li>
<li><strong>Queue</strong>: The buffer that stores messages sent from the producer.</li>
<li><strong>Exchange</strong>: Receives messages from producers and routes them to appropriate queues.</li>
<li><strong>Binding</strong>: Defines the relationship between a queue and an exchange.</li>
</ol>
<h3>Advantages</h3>
<ul>
<li><strong>Reliability</strong>: RabbitMQ offers robust messaging for applications.</li>
<li><strong>Flexible Routing</strong>: Advanced message routing capabilities.</li>
<li><strong>Clustering</strong>: Easy to set up in a distributed and fault-tolerant configuration.</li>
<li><strong>Highly Scalable</strong>: Scales easily and can handle a high throughput of messages.</li>
<li><strong>Multiple Protocols</strong>: Supports AMQP, MQTT, STOMP, and more.</li>
<li><strong>Multi-platform</strong>: Client libraries are available for almost every language, including Java, .NET, Python, etc.</li>
</ul>
<h3>Using RabbitMQ with C#</h3>
<p>You can use the RabbitMQ .NET client to interact with RabbitMQ in a C# application. First, you'll need to install the RabbitMQ client package via NuGet:</p>
<pre><code>Install-Package RabbitMQ.Client
</code></pre>
<p>Here's a simple example that demonstrates sending (producing) and receiving (consuming) messages with RabbitMQ in C#.</p>
<h4>Producer (Send Message)</h4>
<pre><code class="language-csharp">using RabbitMQ.Client;
using System.Text;

class Program
{
    static void Main()
    {
        var factory = new ConnectionFactory() { HostName = &quot;localhost&quot; };
        using (var connection = factory.CreateConnection())
        using (var channel = connection.CreateModel())
        {
            channel.QueueDeclare(queue: &quot;hello&quot;,
                                 durable: false,
                                 exclusive: false,
                                 autoDelete: false,
                                 arguments: null);

            string message = &quot;Hello RabbitMQ!&quot;;
            var body = Encoding.UTF8.GetBytes(message);

            channel.BasicPublish(exchange: &quot;&quot;,
                                 routingKey: &quot;hello&quot;,
                                 basicProperties: null,
                                 body: body);
            Console.WriteLine(&quot; [x] Sent {0}&quot;, message);
        }
    }
}
</code></pre>
<h4>Consumer (Receive Message)</h4>
<pre><code class="language-csharp">using RabbitMQ.Client;
using RabbitMQ.Client.Events;
using System;
using System.Text;

class Program
{
    static void Main()
    {
        var factory = new ConnectionFactory() { HostName = &quot;localhost&quot; };
        using (var connection = factory.CreateConnection())
        using (var channel = connection.CreateModel())
        {
            channel.QueueDeclare(queue: &quot;hello&quot;,
                                 durable: false,
                                 exclusive: false,
                                 autoDelete: false,
                                 arguments: null);

            var consumer = new EventingBasicConsumer(channel);
            consumer.Received += (model, ea) =&gt;
            {
                var body = ea.Body.ToArray();
                var message = Encoding.UTF8.GetString(body);
                Console.WriteLine(&quot; [x] Received {0}&quot;, message);
            };
            channel.BasicConsume(queue: &quot;hello&quot;,
                                 autoAck: true,
                                 consumer: consumer);

            Console.WriteLine(&quot; Press [enter] to exit.&quot;);
            Console.ReadLine();
        }
    }
}
</code></pre>
<p>In this example, we have a producer that sends a &quot;Hello RabbitMQ!&quot; message to a queue named &quot;hello&quot;, and a consumer that receives messages from the same queue. Both producer and consumer connect to a RabbitMQ server running on <code>localhost</code>.</p>
<p>You'll need to have RabbitMQ server running locally (or change the <code>HostName</code> to point to a RabbitMQ server) to try out this example.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <div id="05eec944-702a-4932-b55f-d6caaa74ab8b" class="task done">
                <div class="task_contents">&check;
                    <span class="task_contents">Apache Kafka について調べる</span>
                    <a href="#05eec944-702a-4932-b55f-d6caaa74ab8b" class="permalink">&infin;</a>
                </div>
                <div class="notes">
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>Apache Kafka is a distributed streaming platform that allows you to build real-time streaming applications. It's designed to handle data feeds with high throughput and low latency. Kafka is often compared to message queues or other messaging systems like RabbitMQ, but it offers some unique features that make it well-suited for various kinds of distributed systems.</p>
<h3>Key Concepts</h3>
<ol>
<li><strong>Producer</strong>: Applications that publish data into Kafka topics.</li>
<li><strong>Consumer</strong>: Applications that subscribe to topics and process the feed of published messages.</li>
<li><strong>Broker</strong>: A single Kafka server instance.</li>
<li><strong>Topic</strong>: A category name to which records are stored and published.</li>
<li><strong>Partition</strong>: Kafka topics are split into partitions for parallelism, scalability, and fault-tolerance.</li>
<li><strong>ZooKeeper</strong>: Initially, Kafka used ZooKeeper to maintain distributed state. The new versions aim for KRaft mode to eliminate the ZooKeeper dependency.</li>
<li><strong>Stream Processor</strong>: Allows you to process and transform data as it arrives in real-time.</li>
</ol>
<h3>Advantages</h3>
<ul>
<li><strong>High Throughput</strong>: Kafka can handle a high volume of reads and writes per second.</li>
<li><strong>Scalability</strong>: Easily scales out in a distributed and fault-tolerant manner.</li>
<li><strong>Durability</strong>: Data is replicated across multiple brokers.</li>
<li><strong>Low Latency</strong>: Capable of real-time processing.</li>
<li><strong>Flexibility</strong>: More than a messaging queue and more than a log aggregator.</li>
</ul>
<h3>Typical Use-Cases</h3>
<ul>
<li><strong>Real-time analytics and monitoring</strong></li>
<li><strong>Aggregating data from different sources</strong></li>
<li><strong>Acting as a buffer to handle burst data loads</strong></li>
<li><strong>Real-time processing and transformation of streams</strong></li>
</ul>
<h3>Using Kafka with C#</h3>
<p>There are several libraries available to integrate Kafka with C#. One popular library is Confluent's .NET Client for Apache Kafka, which can be installed via NuGet.</p>
<p>Here's a basic example using Confluent's Kafka client to produce and consume messages.</p>
<h4>Producer Example in C#</h4>
<p>Install the package:</p>
<pre><code class="language-bash">Install-Package Confluent.Kafka
</code></pre>
<p>Code snippet:</p>
<pre><code class="language-csharp">using Confluent.Kafka;
using System;

class Program
{
    static void Main(string[] args)
    {
        var config = new ProducerConfig { BootstrapServers = &quot;localhost:9092&quot; };

        using (var producer = new ProducerBuilder&lt;Null, string&gt;(config).Build())
        {
            try
            {
                var dr = producer.ProduceAsync(&quot;test-topic&quot;, new Message&lt;Null, string&gt; { Value = &quot;test&quot; }).Result;
                Console.WriteLine($&quot;Delivered '{dr.Value}' to '{dr.TopicPartitionOffset}'&quot;);
            }
            catch (ProduceException&lt;Null, string&gt; e)
            {
                Console.WriteLine($&quot;Delivery failed: {e.Error.Reason}&quot;);
            }
        }
    }
}
</code></pre>
<h4>Consumer Example in C#</h4>
<p>Code snippet:</p>
<pre><code class="language-csharp">using Confluent.Kafka;
using System;

class Program
{
    static void Main(string[] args)
    {
        var conf = new ConsumerConfig
        {
            GroupId = &quot;test-consumer-group&quot;,
            BootstrapServers = &quot;localhost:9092&quot;,
            AutoOffsetReset = AutoOffsetReset.Earliest
        };

        using (var consumer = new ConsumerBuilder&lt;Ignore, string&gt;(conf).Build())
        {
            consumer.Subscribe(&quot;test-topic&quot;);

            while (true)
            {
                var consumeResult = consumer.Consume();

                Console.WriteLine($&quot;Consumed message '{consumeResult.Message.Value}' at: '{consumeResult.TopicPartitionOffset}'.&quot;);
            }
        }
    }
}
</code></pre>
<p>Both of these examples assume that you have a running Kafka instance on localhost at the default port (<code>9092</code>), and a topic named <code>test-topic</code>. Adjust the settings accordingly based on your actual setup.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <div id="dff594f4-dff4-4114-b9c0-8b16d3e6310e" class="task done">
                <div class="task_contents">&check;
                    <span class="task_contents">AWS SQS について調べる</span>
                    <a href="#dff594f4-dff4-4114-b9c0-8b16d3e6310e" class="permalink">&infin;</a>
                </div>
                <div class="notes">
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>Amazon Simple Queue Service (AWS SQS) is a fully managed message queuing service that enables the decoupling of components of a cloud application. AWS SQS offers two types of message queues:</p>
<ol>
<li><p><strong>Standard Queues</strong>: Offers maximum throughput, best-effort ordering, and at-least-once delivery.</p>
</li>
<li><p><strong>FIFO Queues</strong>: Designed to guarantee that messages are processed exactly once, in the exact order that they are sent.</p>
</li>
</ol>
<h3>Key Features</h3>
<ul>
<li><strong>Scalability</strong>: SQS scales dynamically so you can increase or decrease throughput quickly without manual intervention.</li>
<li><strong>Reliability</strong>: Messages are stored redundantly across multiple servers and data centers.</li>
<li><strong>Message Filtering</strong>: SQS supports message attributes, which allow you to filter the messages that a consumer receives.</li>
<li><strong>Security</strong>: Supports encryption at rest and in transit, and integrates with AWS IAM for access control.</li>
<li><strong>Event-Driven Computing</strong>: Works well with AWS Lambda for serverless architectures.</li>
</ul>
<h3>Use-Cases</h3>
<ul>
<li><strong>Decoupling Components</strong>: SQS can decouple different parts of a cloud application.</li>
<li><strong>Buffering</strong>: SQS can buffer requests from application components that are required to work at different speeds.</li>
<li><strong>Batch Operations</strong>: Collect messages for a batch operation.</li>
<li><strong>Real-Time Events</strong>: Handle real-time events in an asynchronous fashion.</li>
</ul>
<h3>Using AWS SQS with C#</h3>
<p>AWS provides the AWS SDK for .NET that makes it easy to interact with AWS services, including SQS. To get started, you first need to install the SDK via NuGet:</p>
<pre><code class="language-bash">Install-Package AWSSDK.SQS
</code></pre>
<p>Here's a simple C# example to send and receive messages using AWS SQS:</p>
<h4>Sending a Message</h4>
<pre><code class="language-csharp">using Amazon;
using Amazon.SQS;
using Amazon.SQS.Model;
using System;
using System.Threading.Tasks;

class Program
{
    static async Task Main(string[] args)
    {
        var sqsClient = new AmazonSQSClient(RegionEndpoint.USWest2);

        var request = new SendMessageRequest
        {
            QueueUrl = &quot;https://sqs.us-west-2.amazonaws.com/Your-Account-ID/Your-Queue-Name&quot;,
            MessageBody = &quot;Hello, world!&quot;
        };

        var response = await sqsClient.SendMessageAsync(request);

        if (response.HttpStatusCode == System.Net.HttpStatusCode.OK)
        {
            Console.WriteLine(&quot;Message sent successfully.&quot;);
        }
    }
}
</code></pre>
<h4>Receiving a Message</h4>
<pre><code class="language-csharp">using Amazon;
using Amazon.SQS;
using Amazon.SQS.Model;
using System;
using System.Threading.Tasks;

class Program
{
    static async Task Main(string[] args)
    {
        var sqsClient = new AmazonSQSClient(RegionEndpoint.USWest2);

        var request = new ReceiveMessageRequest
        {
            QueueUrl = &quot;https://sqs.us-west-2.amazonaws.com/Your-Account-ID/Your-Queue-Name&quot;,
            MaxNumberOfMessages = 1
        };

        var response = await sqsClient.ReceiveMessageAsync(request);

        if (response.Messages.Count &gt; 0)
        {
            Console.WriteLine(&quot;Received message: &quot; + response.Messages[0].Body);

            // After processing the message, it should be deleted from the queue
            var deleteRequest = new DeleteMessageRequest
            {
                QueueUrl = request.QueueUrl,
                ReceiptHandle = response.Messages[0].ReceiptHandle
            };

            var deleteResponse = await sqsClient.DeleteMessageAsync(deleteRequest);

            if (deleteResponse.HttpStatusCode == System.Net.HttpStatusCode.OK)
            {
                Console.WriteLine(&quot;Message deleted successfully.&quot;);
            }
        }
    }
}
</code></pre>
<p>Note that you'll need to replace <code>Your-Account-ID</code> and <code>Your-Queue-Name</code> with your own account ID and the name of the SQS queue you've created. Also, you'd typically use AWS IAM roles or profiles to handle authentication, which is not shown in these examples.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <div id="e7619de6-ecfc-45d4-9253-b8f06ec24aa6" class="task done">
                <div class="task_contents">&check;
                    <span class="task_contents">Apache Camel について調べる</span>
                    <a href="#e7619de6-ecfc-45d4-9253-b8f06ec24aa6" class="permalink">&infin;</a>
                </div>
                <div class="notes">
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>Apache Camel is an open-source integration framework that provides a wide variety of components and features to solve different integration problems using standard Enterprise Integration Patterns (EIPs). Camel aims to make it easier for you to connect various systems consuming or producing data. It can be used as a standalone application, within Spring/Spring Boot applications, or deployed in an application server.</p>
<h3>Key Concepts</h3>
<ol>
<li><p><strong>Endpoints</strong>: Components that can produce or consume messages from various sources, such as databases, messaging systems, APIs, and more.</p>
</li>
<li><p><strong>Routes</strong>: Definitions of how messages should move between endpoints.</p>
</li>
<li><p><strong>Processors</strong>: Elements that allow manipulation or transformation of messages within a route.</p>
</li>
<li><p><strong>Exchange</strong>: A container holding the received message (known as <code>In</code>) from the source and the corresponding response (known as <code>Out</code>) for the destination.</p>
</li>
<li><p><strong>Camel Context</strong>: The runtime system where routes and other Camel objects reside.</p>
</li>
<li><p><strong>DSL (Domain Specific Language)</strong>: Camel provides various DSLs (Java DSL, Spring XML DSL, etc.) to create routes and configure endpoints.</p>
</li>
</ol>
<h3>Advantages</h3>
<ul>
<li><p><strong>Flexibility</strong>: Offers over 300 pre-built components for interacting with various services and protocols.</p>
</li>
<li><p><strong>Pluggable</strong>: Easily extensible to add new components and data formats.</p>
</li>
<li><p><strong>Cross-platform</strong>: Supports multiple languages and frameworks, including Java and Spring.</p>
</li>
<li><p><strong>Modular</strong>: You can pick and choose the components and libraries that are required for your application.</p>
</li>
<li><p><strong>Enterprise-Ready</strong>: Provides robust and scalable solutions for large-scale enterprise applications.</p>
</li>
</ul>
<h3>Typical Use-Cases</h3>
<ul>
<li><p><strong>Data Integration</strong>: Integrating different data sources and providing a unified interface.</p>
</li>
<li><p><strong>Message Routing</strong>: Route messages based on content, headers, or other attributes.</p>
</li>
<li><p><strong>Data Transformation</strong>: Transforming messages from one format to another, like XML to JSON and vice versa.</p>
</li>
<li><p><strong>Workflow and Business Process Automation</strong>: Automate workflows and business processes between different systems.</p>
</li>
<li><p><strong>API Gateway</strong>: Consolidating multiple services behind a unified API interface.</p>
</li>
</ul>
<h3>Example</h3>
<p>Here's a simple Java DSL example to demonstrate a Camel route that consumes messages from a JMS queue, processes them, and sends them to another JMS queue:</p>
<pre><code class="language-java">import org.apache.camel.CamelContext;
import org.apache.camel.builder.RouteBuilder;
import org.apache.camel.impl.DefaultCamelContext;

public class SimpleRoute {

    public static void main(String[] args) throws Exception {
        CamelContext context = new DefaultCamelContext();

        context.addRoutes(new RouteBuilder() {
            @Override
            public void configure() {
                from(&quot;jms:queue:TEST.SOURCE.QUEUE&quot;)
                    .process(exchange -&gt; {
                        String message = exchange.getIn().getBody(String.class);
                        // Perform some processing on the message
                    })
                    .to(&quot;jms:queue:TEST.DESTINATION.QUEUE&quot;);
            }
        });

        context.start();
        Thread.sleep(2000); // Keep it running for a while
        context.stop();
    }
}
</code></pre>
<p>Note: This example is just for illustration and doesn't include error handling, dependencies, or configuration settings like JMS connection factories.</p>
<p>Camel is very rich in features, and this is a very basic introduction. It supports many more EIPs, DSLs, and has a large, active community.</p>
                            </div>
                        </div>
                    </div>
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>As of my last update in January 2022, Apache Camel itself is a Java-based framework and doesn't have a direct equivalent in C#. However, there are other frameworks and libraries in the C# ecosystem that serve similar purposes of message routing, transformation, and system integration, even though they might not offer the exact same features or breadth of components that Camel does.</p>
<p>Here are some alternative options in C# for similar integration tasks:</p>
<h3>MassTransit</h3>
<p>MassTransit is a free, open-source distributed application framework for .NET. It supports RabbitMQ and Azure Service Bus as transports and provides a wide range of features like scheduling, sagas, and more.</p>
<h3>NServiceBus</h3>
<p>NServiceBus is a messaging and workflow framework that helps you create distributed systems. It has a variety of built-in features like retries, message auditing, and more. Note that NServiceBus is not free but offers a lot of powerful features for enterprise applications.</p>
<h3>Azure Logic Apps</h3>
<p>If you're working in the Azure ecosystem, Azure Logic Apps offers a serverless way to create workflows and integrate services, apps, and data across cloud and on-premises.</p>
<h3>Hangfire</h3>
<p>Hangfire is more of a background job framework but can be used for message processing tasks. It can run inside ASP.NET applications.</p>
<h3>Reactive Extensions (Rx.NET)</h3>
<p>Rx.NET can be used for event stream processing, which includes features for filtering, grouping, and transforming streams of events.</p>
<h3>Custom Solutions</h3>
<p>You can always roll out your own minimalistic solution using basic .NET libraries. For example, you could use <code>HttpClient</code> for RESTful operations, <code>RabbitMQ.Client</code> for RabbitMQ messaging, etc.</p>
<p>While none of these are direct replacements for Apache Camel, they serve similar purposes and can be used to build robust and scalable distributed systems in C#.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <div id="3c940dad-ab53-40bd-8edb-bdf74e672231" class="task done">
                <div class="task_contents">&check;
                    <span class="task_contents">MassTransit について調べる</span>
                    <a href="#3c940dad-ab53-40bd-8edb-bdf74e672231" class="permalink">&infin;</a>
                </div>
                <div class="notes">
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>MassTransit is an open-source distributed application framework for .NET. It is designed to simplify the development of message-based distributed systems and provides a set of services for building such applications. MassTransit helps decouple the components of an application, thereby making it easier to develop, test, and maintain. It can be particularly useful in microservices architectures and is widely used with message brokers like RabbitMQ and Azure Service Bus.</p>
<h3>Key Features</h3>
<ol>
<li><p><strong>Message Routing</strong>: Automatically routes messages to the correct message consumers.</p>
</li>
<li><p><strong>Fault Tolerance</strong>: Built-in support for message retry policies.</p>
</li>
<li><p><strong>Sagas</strong>: Long-running workflows can be easily modeled using the Saga feature.</p>
</li>
<li><p><strong>Monitoring and Introspection</strong>: Observability through logging, monitoring, and visualization.</p>
</li>
<li><p><strong>Concurrency and Parallelism</strong>: Supports concurrent message processing and multiple message consumers.</p>
</li>
<li><p><strong>Modular and Extensible</strong>: You can easily extend its capabilities or integrate it with other services.</p>
</li>
<li><p><strong>Supports Multiple Transports</strong>: RabbitMQ and Azure Service Bus are natively supported, and it's possible to extend it for other message brokers.</p>
</li>
</ol>
<h3>Typical Use-Cases</h3>
<ul>
<li><p><strong>Event-Driven Architectures</strong>: Decoupling services and performing asynchronous operations.</p>
</li>
<li><p><strong>Workflow Coordination</strong>: Coordinating long-running workflows using Sagas.</p>
</li>
<li><p><strong>Microservices Communication</strong>: Facilitating communication between microservices in a decoupled fashion.</p>
</li>
<li><p><strong>Real-Time Data Processing</strong>: Consuming and processing data in real-time from various producers.</p>
</li>
</ul>
<h3>Basic Example in C#</h3>
<p>Here's a simple example that demonstrates how to set up a MassTransit consumer and producer using RabbitMQ as the transport:</p>
<h4>Installing NuGet Packages</h4>
<p>You can install the MassTransit package via NuGet:</p>
<pre><code class="language-bash">Install-Package MassTransit
Install-Package MassTransit.RabbitMq
</code></pre>
<h4>Producer</h4>
<pre><code class="language-csharp">using MassTransit;
using System;

public class Program
{
    public static async Task Main()
    {
        var busControl = Bus.Factory.CreateUsingRabbitMq(cfg =&gt;
        {
            cfg.Host(new Uri(&quot;rabbitmq://localhost&quot;), h =&gt; { });
        });

        await busControl.StartAsync();

        await busControl.Publish(new { Text = &quot;Hello, world!&quot; });

        await busControl.StopAsync();
    }
}
</code></pre>
<h4>Consumer</h4>
<pre><code class="language-csharp">using MassTransit;
using System;

public class MessageConsumer : IConsumer&lt;Message&gt;
{
    public async Task Consume(ConsumeContext&lt;Message&gt; context)
    {
        Console.WriteLine($&quot;Received: {context.Message.Text}&quot;);
    }
}

public interface Message
{
    string Text { get; }
}

public class Program
{
    public static async Task Main()
    {
        var busControl = Bus.Factory.CreateUsingRabbitMq(cfg =&gt;
        {
            cfg.Host(new Uri(&quot;rabbitmq://localhost&quot;), h =&gt; { });

            cfg.ReceiveEndpoint(&quot;message_queue&quot;, e =&gt;
            {
                e.Consumer&lt;MessageConsumer&gt;();
            });
        });

        await busControl.StartAsync();

        Console.WriteLine(&quot;Press enter to exit&quot;);
        Console.ReadLine();

        await busControl.StopAsync();
    }
}
</code></pre>
<p>In this example, the producer sends a message with the text &quot;Hello, world!&quot;, and the consumer receives this message and prints it to the console.</p>
<p>Note that these are simplified examples and don't cover important topics like error handling, message retry, etc. Before running the examples, ensure that RabbitMQ is running and accessible at <code>rabbitmq://localhost</code>.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <div id="d088b2ea-da00-450c-b443-403a7ad9b3fc" class="task done">
                <div class="task_contents">&check;
                    <span class="task_contents">NServiceBus について調べる</span>
                    <a href="#d088b2ea-da00-450c-b443-403a7ad9b3fc" class="permalink">&infin;</a>
                </div>
                <div class="notes">
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>NServiceBus is a messaging framework for building distributed .NET systems. Developed by Particular Software, NServiceBus provides abstractions over different message queuing services and aims to make systems easier to scale, maintain, and understand. While it is not open-source, it is well-supported and offers enterprise-level features out-of-the-box.</p>
<h3>Key Features</h3>
<ol>
<li><p><strong>Reliability</strong>: Offers built-in retry logic, durable messaging, and more to make your system robust and fault-tolerant.</p>
</li>
<li><p><strong>Sagas</strong>: Supports long-running business processes, providing an infrastructure for stateful and time-bound conversations.</p>
</li>
<li><p><strong>Publish-Subscribe</strong>: Native support for the publish-subscribe pattern for event-driven architecture.</p>
</li>
<li><p><strong>Extensibility</strong>: Customizable and extensible with behaviors, message mutators, and custom serialization.</p>
</li>
<li><p><strong>Monitoring and Visualization</strong>: Offers monitoring tools and visualization capabilities to provide insight into your running system.</p>
</li>
<li><p><strong>Various Transports and Persistence</strong>: Supports various transports (RabbitMQ, Azure Service Bus, etc.) and persistence options (RavenDB, SQL Server, etc.).</p>
</li>
</ol>
<h3>Typical Use-Cases</h3>
<ul>
<li><p><strong>Microservices</strong>: Provides a robust infrastructure for microservices communication.</p>
</li>
<li><p><strong>Complex Business Processes</strong>: Ideal for workflow-based business rules and long-running processes.</p>
</li>
<li><p><strong>Event-Driven Architectures</strong>: Enables event-driven designs, thanks to its publish-subscribe capabilities.</p>
</li>
<li><p><strong>Integration</strong>: Connects different systems, both internal and external.</p>
</li>
</ul>
<h3>Basic Example in C#</h3>
<p>To start using NServiceBus, you'll need to install the NuGet package:</p>
<pre><code class="language-bash">Install-Package NServiceBus
</code></pre>
<p>Here's a simple example illustrating a message sender and receiver using NServiceBus.</p>
<h4>Message Definition</h4>
<pre><code class="language-csharp">public class MyMessage : IMessage
{
    public string Content { get; set; }
}
</code></pre>
<h4>Sender (or &quot;Endpoint&quot;)</h4>
<pre><code class="language-csharp">static async Task Main()
{
    var endpointConfiguration = new EndpointConfiguration(&quot;SenderEndpoint&quot;);
    var transport = endpointConfiguration.UseTransport&lt;LearningTransport&gt;();

    var endpointInstance = await Endpoint.Start(endpointConfiguration)
        .ConfigureAwait(false);

    var message = new MyMessage
    {
        Content = &quot;Hello, NServiceBus!&quot;
    };

    await endpointInstance.Send(&quot;ReceiverEndpoint&quot;, message)
        .ConfigureAwait(false);

    await endpointInstance.Stop()
        .ConfigureAwait(false);
}
</code></pre>
<h4>Receiver (or &quot;Endpoint&quot;)</h4>
<pre><code class="language-csharp">public class MyMessageHandler : IHandleMessages&lt;MyMessage&gt;
{
    public Task Handle(MyMessage message, IMessageHandlerContext context)
    {
        Console.WriteLine($&quot;Received message: {message.Content}&quot;);
        return Task.CompletedTask;
    }
}

static async Task Main()
{
    var endpointConfiguration = new EndpointConfiguration(&quot;ReceiverEndpoint&quot;);
    var transport = endpointConfiguration.UseTransport&lt;LearningTransport&gt;();

    endpointConfiguration.RegisterComponents(components =&gt;
    {
        components.ConfigureComponent&lt;MyMessageHandler&gt;(DependencyLifecycle.InstancePerCall);
    });

    var endpointInstance = await Endpoint.Start(endpointConfiguration)
        .ConfigureAwait(false);

    Console.WriteLine(&quot;Press any key to exit&quot;);
    Console.ReadKey();

    await endpointInstance.Stop()
        .ConfigureAwait(false);
}
</code></pre>
<p>In this example, the <code>SenderEndpoint</code> sends a <code>MyMessage</code> object to the <code>ReceiverEndpoint</code>, which is set up to handle <code>MyMessage</code> objects using the <code>MyMessageHandler</code> class.</p>
<p>Note that this example uses the Learning Transport (<code>LearningTransport</code>), which is meant for learning and development purposes. For production, you would configure a different transport such as RabbitMQ, Azure Service Bus, or others.</p>
<p>This is a very basic illustration, and NServiceBus offers a lot more features like message retries, error handling, sagas for long-running workflows, and more.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <div id="38bc7cfb-8f26-48b9-89b7-d9389ed9a29d" class="task done">
                <div class="task_contents">&check;
                    <span class="task_contents">Azure Logic Apps について調べる</span>
                    <a href="#38bc7cfb-8f26-48b9-89b7-d9389ed9a29d" class="permalink">&infin;</a>
                </div>
                <div class="notes">
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>Azure Logic Apps is a cloud service from Microsoft that helps you automate workflows and integrate services, apps, and data across clouds or on-premises. It provides a visual designer to model and automate your process as a series of steps known as a workflow. Azure Logic Apps is often used for tasks like data integration, ETL processes, automating business processes, and building SaaS (Software as a Service) integrations.</p>
<h3>Key Features</h3>
<ol>
<li><p><strong>Visual Designer</strong>: Provides a user-friendly interface for designing workflows.</p>
</li>
<li><p><strong>Connectors</strong>: Offers a large library of pre-built connectors for common services and protocols, such as SQL Server, SharePoint, Salesforce, and many others.</p>
</li>
<li><p><strong>Scalability</strong>: Highly scalable and capable of handling a large number of workflow instances and high throughput.</p>
</li>
<li><p><strong>Triggers and Actions</strong>: Workflows can be triggered manually or by events/data changes in other services, and can perform actions in response.</p>
</li>
<li><p><strong>Conditional Logic</strong>: Supports condition-based logic, loops, and branching in workflows.</p>
</li>
<li><p><strong>Serverless</strong>: Fully managed service which doesn't require you to manage any infrastructure.</p>
</li>
<li><p><strong>Monitoring and Diagnostics</strong>: Offers integrated tools for monitoring, logging, and diagnosing issues with your apps.</p>
</li>
</ol>
<h3>Typical Use-Cases</h3>
<ul>
<li><p><strong>Data Integration</strong>: ETL tasks, moving data between different services, and data transformation.</p>
</li>
<li><p><strong>SaaS Product Integration</strong>: Integrating various SaaS products like CRM, marketing services, data analytics, etc.</p>
</li>
<li><p><strong>Business Process Automation</strong>: Automating complex business processes and workflows.</p>
</li>
<li><p><strong>Real-Time Workflows</strong>: Responding to events in real-time, like sending notifications or alerts.</p>
</li>
</ul>
<h3>Basic Example</h3>
<p>To give you an idea of how it works, here’s a simple use-case:</p>
<p><strong>Scenario</strong>: Every time a new record is inserted into a SQL Database, you want to send an email.</p>
<ol>
<li><p><strong>Trigger</strong>: Set up a Logic App to be triggered whenever a new record is inserted into the SQL Database.</p>
</li>
<li><p><strong>Action 1</strong>: The Logic App reads the new record.</p>
</li>
<li><p><strong>Action 2</strong>: Use an Email connector (like Outlook or Gmail) to send an email based on the new record.</p>
</li>
</ol>
<p>You would configure all this visually in the Azure Portal:</p>
<ol>
<li>Choose the SQL Database connector and set it up to trigger on a new record.</li>
<li>Add an action to read the new SQL record.</li>
<li>Add another action to send an email and populate it with the data from the SQL record.</li>
</ol>
<p>You can set up all these steps visually through drag-and-drop in the Azure Portal, without having to write any code.</p>
<p>Azure Logic Apps is a powerful tool for automating workflows and integrating services with the added benefits of being easy to use and not requiring you to manage any servers.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <div id="d007e820-2e47-4927-b67a-d084babf5b9e" class="task done">
                <div class="task_contents">&check;
                    <span class="task_contents">Hangfire について調べる</span>
                    <a href="#d007e820-2e47-4927-b67a-d084babf5b9e" class="permalink">&infin;</a>
                </div>
                <div class="notes">
                    <div class="note">
                        <div class="note_contents">
                            <p class="note_contents">便利そう。</p>
                            <div class="note_ai_generated">
<p>Hangfire is an open-source framework that enables you to run background tasks in .NET and .NET Core applications. It is typically used for tasks like sending emails, processing uploaded files, and other operations that you don't want to perform during a regular web request. Hangfire provides a way to perform fire-and-forget, delayed, and recurring tasks with reliability features like automatic retries and job persistence.</p>
<h3>Key Features</h3>
<ol>
<li><p><strong>Persistent Jobs</strong>: Hangfire uses persistent storage to keep track of background jobs. This means that tasks will survive application restarts and server failures.</p>
</li>
<li><p><strong>Job Types</strong>: Supports multiple types of background jobs: fire-and-forget, delayed, recurring, and continuations.</p>
</li>
<li><p><strong>Dashboard</strong>: Comes with a built-in dashboard for monitoring job progress, history, and other statistics. The dashboard also allows you to manage jobs, such as retrying failed jobs.</p>
</li>
<li><p><strong>Scalability</strong>: Easy to scale out by running multiple Hangfire servers.</p>
</li>
<li><p><strong>Extensibility</strong>: Provides several extension points for customizing its behavior, including custom job filters, background processes, and more.</p>
</li>
<li><p><strong>Integration</strong>: Easily integrates with various .NET project types, including ASP.NET Core, ASP.NET MVC, and others.</p>
</li>
</ol>
<h3>Typical Use-Cases</h3>
<ul>
<li><p><strong>Email Sending</strong>: Queueing and sending emails in the background.</p>
</li>
<li><p><strong>Batch Processing</strong>: Executing large data manipulation tasks like import/export of data.</p>
</li>
<li><p><strong>Scheduled Reports</strong>: Generating and sending reports at scheduled intervals.</p>
</li>
<li><p><strong>Data Cleanup</strong>: Periodic deletion or archiving of old data from databases.</p>
</li>
</ul>
<h3>Basic Example in C#</h3>
<p>To install Hangfire, you can use NuGet:</p>
<pre><code class="language-bash">Install-Package Hangfire
</code></pre>
<p>Here is a simple example that adds Hangfire to an ASP.NET Core application and queues a background job.</p>
<p>In your <code>Startup.cs</code>:</p>
<pre><code class="language-csharp">public void ConfigureServices(IServiceCollection services)
{
    // ... other services

    // Use SQL Server as storage; you can use other storage backends like Redis, etc.
    services.AddHangfire(x =&gt; x.UseSqlServerStorage(&quot;&lt;connection_string_here&gt;&quot;));
}

public void Configure(IApplicationBuilder app, IWebHostEnvironment env)
{
    // ... other middleware

    app.UseHangfireDashboard(); // This enables the dashboard at &quot;/hangfire&quot;
    app.UseHangfireServer(); // This starts the background job processing

    // Enqueue a simple background job
    BackgroundJob.Enqueue(() =&gt; Console.WriteLine(&quot;Hello, Hangfire!&quot;));
}
</code></pre>
<p>In this example, Hangfire is set to use SQL Server as its storage backend, and it enqueues a simple job to write a message to the console. Once the application is running, you can navigate to <code>/hangfire</code> to see the Hangfire Dashboard, which provides information about job status, retries, and more.</p>
<p>Hangfire is a powerful tool for background job processing in .NET applications, offering a lot of features to make task scheduling, execution, and monitoring straightforward.</p>
                            </div>
                        </div>
                    </div>
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>Certainly! To send an email at a scheduled time using Hangfire, you'd usually combine Hangfire with a .NET mail library, such as <code>System.Net.Mail</code>. Here's a basic example that demonstrates how to schedule an email to be sent at a specific time in an ASP.NET Core application:</p>
<h3>Step 1: Install Hangfire and configure it in Startup.cs</h3>
<p>Install the Hangfire NuGet package if you haven't already.</p>
<pre><code class="language-bash">Install-Package Hangfire
</code></pre>
<p>Add the following code to your <code>Startup.cs</code>:</p>
<pre><code class="language-csharp">using Hangfire;

public void ConfigureServices(IServiceCollection services)
{
    // ...other services
    services.AddHangfire(x =&gt; x.UseSqlServerStorage(&quot;Your_Connection_String_Here&quot;));
}

public void Configure(IApplicationBuilder app, IWebHostEnvironment env)
{
    // ...other middlewares

    app.UseHangfireDashboard();
    app.UseHangfireServer();

    // You can schedule the job here or inside another part of your application
    RecurringJob.AddOrUpdate(&quot;send-email-job&quot;, () =&gt; SendScheduledEmail(), Cron.Daily);
}
</code></pre>
<h3>Step 2: Create the email sending function</h3>
<p>You'll need to implement the actual email sending logic. You can use <code>System.Net.Mail</code> for this. Here's a simple example:</p>
<pre><code class="language-csharp">using System.Net;
using System.Net.Mail;

public void SendScheduledEmail()
{
    var fromAddress = new MailAddress(&quot;from@example.com&quot;, &quot;From Name&quot;);
    var toAddress = new MailAddress(&quot;to@example.com&quot;, &quot;To Name&quot;);
    const string fromPassword = &quot;fromPassword&quot;;
    const string subject = &quot;Scheduled Email&quot;;
    const string body = &quot;This is a scheduled email from Hangfire.&quot;;

    var smtp = new SmtpClient
    {
        Host = &quot;smtp.example.com&quot;,
        Port = 587,
        EnableSsl = true,
        DeliveryMethod = SmtpDeliveryMethod.Network,
        UseDefaultCredentials = false,
        Credentials = new NetworkCredential(fromAddress.Address, fromPassword)
    };

    using (var message = new MailMessage(fromAddress, toAddress)
    {
        Subject = subject,
        Body = body
    })
    {
        smtp.Send(message);
    }
}
</code></pre>
<p>Replace the placeholders like <code>&quot;Your_Connection_String_Here&quot;</code>, <code>from@example.com</code>, <code>to@example.com</code>, etc., with your actual values.</p>
<h3>Step 3: Schedule the email job using Hangfire</h3>
<p>If you've added Hangfire in <code>Startup.cs</code>, you can schedule your email sending job to run at a specific time. In the example above, <code>RecurringJob.AddOrUpdate()</code> is used to schedule <code>SendScheduledEmail()</code> to run daily. You can set different time intervals using Cron expressions.</p>
<p>Now, Hangfire will handle invoking <code>SendScheduledEmail()</code> based on the Cron expression, taking care of retries and persistence.</p>
<p>This is just a simple example to get you started. In a production scenario, you might want to include more sophisticated email templates, error handling, and logging.</p>
                            </div>
                        </div>
                    </div>
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>If you need to schedule emails at very specific times that can't be covered by a recurring Cron schedule, Hangfire's <code>BackgroundJob.Schedule</code> method could be used to schedule a job for a specific time in the future. This allows you to specify exactly when the email should be sent, down to the minute.</p>
<p>Here's a quick example:</p>
<pre><code class="language-csharp">using System;
using Hangfire;

public void Configure(IApplicationBuilder app, IWebHostEnvironment env)
{
    // ... other middlewares
    app.UseHangfireDashboard();
    app.UseHangfireServer();

    // Schedule email to be sent on a specific date and time
    DateTime sendEmailDateTime = new DateTime(2023, 10, 1, 15, 30, 0); // 3:30 PM on October 1, 2023
    TimeSpan delayUntilSend = sendEmailDateTime - DateTime.Now;

    // The SendScheduledEmail method will be executed at the specific DateTime
    BackgroundJob.Schedule(() =&gt; SendScheduledEmail(), delayUntilSend);
}
</code></pre>
<p>In this example, <code>BackgroundJob.Schedule</code> will queue <code>SendScheduledEmail()</code> to be run at 3:30 PM on October 1, 2023. The <code>TimeSpan</code> <code>delayUntilSend</code> is calculated based on the current time (<code>DateTime.Now</code>).</p>
<p>If you have varying times for different emails, you could dynamically create these jobs based on your application's needs. For example, you could read from a database table containing scheduled times and recipient email addresses, and then loop through those records to schedule each email:</p>
<pre><code class="language-csharp">public void Configure(IApplicationBuilder app, IWebHostEnvironment env)
{
    // ... other middlewares
    app.UseHangfireDashboard();
    app.UseHangfireServer();

    // Example: pretend we get these from a database
    List&lt;DateTime&gt; emailTimes = new List&lt;DateTime&gt;
    {
        new DateTime(2023, 10, 1, 15, 30, 0),
        new DateTime(2023, 10, 2, 14, 0, 0),
        // ... add as many as you need
    };

    foreach (DateTime emailTime in emailTimes)
    {
        TimeSpan delayUntilSend = emailTime - DateTime.Now;
        BackgroundJob.Schedule(() =&gt; SendScheduledEmail(), delayUntilSend);
    }
}
</code></pre>
<p>In this example, a list of <code>DateTime</code> objects represents the exact times at which you want to send emails. Hangfire schedules each one separately. Normally, you'd get this data from a database or other data source.</p>
<p>This way, you can set the exact times you'd like each email to be sent.</p>
                            </div>
                        </div>
                    </div>
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>Scheduling a large number of jobs like 10,000 emails won't necessarily &quot;burden&quot; Hangfire itself, as it's designed to handle many jobs efficiently. However, there are several considerations and potential bottlenecks to think about:</p>
<h3>Database Load</h3>
<p>Hangfire uses a persistent storage to manage jobs. If you schedule 10,000 jobs at once, it could put a strain on the database. It's crucial to ensure that the database can handle this load, especially if it's also used for other parts of the application.</p>
<h3>Mail Server</h3>
<p>The mail server you use to send emails might have rate limits or other restrictions. You'll need to make sure that your mail server can handle sending this many emails in a short period.</p>
<h3>Application Server Resources</h3>
<p>The resources on your application server (CPU, memory, etc.) can also be a limiting factor, depending on the complexity of the jobs you're running.</p>
<h3>Throttling</h3>
<p>If you're using a third-party service for sending emails, make sure to adhere to their sending limits to avoid being throttled or blocked.</p>
<h3>Best Practices</h3>
<ol>
<li><p><strong>Batching</strong>: If possible, instead of scheduling 10,000 separate jobs, you might want to batch emails together and send them in chunks. Hangfire supports batch job processing.</p>
</li>
<li><p><strong>Rate Limiting</strong>: Implement some form of rate limiting so you're not hitting your email service too hard, too quickly.</p>
</li>
<li><p><strong>Error Handling and Retries</strong>: Make sure your job processing includes proper error handling. Hangfire has built-in support for retries, but you'll want to ensure you're not constantly retrying failed email sends that will never succeed (e.g., due to an invalid email address).</p>
</li>
<li><p><strong>Monitoring and Alerts</strong>: Keep an eye on job successes and failures, queue lengths, and processing times. Hangfire's dashboard can help, but for production scenarios, you might want more robust monitoring and alerting.</p>
</li>
<li><p><strong>Testing</strong>: Before going live, test your setup under conditions that simulate your expected load to identify any bottlenecks or issues.</p>
</li>
<li><p><strong>Horizontal Scaling</strong>: If a single instance can't handle the load, Hangfire is designed to work well in a distributed setting. You can run multiple instances of the Hangfire server, either on the same machine or across a cluster of machines.</p>
</li>
</ol>
<p>While Hangfire can handle a high volume of jobs, the rest of your infrastructure also needs to be up to the task. Always test thoroughly before deploying such a solution into production.</p>
                            </div>
                        </div>
                    </div>
                    <div class="note">
                        <div class="note_contents">
                            <p class="note_contents">かなり役立ちそうだが、ほかにもありそうなので、必要になったときにググり直す。</p>
                        </div>
                    </div>
                </div>
            </div>
            <div id="a3ef6b36-7269-4eb3-b4f6-34f434f1f0d2" class="task done">
                <div class="task_contents">&check;
                    <span class="task_contents">Reactive Extensions について調べる</span>
                    <a href="#a3ef6b36-7269-4eb3-b4f6-34f434f1f0d2" class="permalink">&infin;</a>
                </div>
                <div class="notes">
                    <div class="note">
                        <div class="note_contents">
                            <p class="note_contents">これも便利そう。</p>
                            <div class="note_ai_generated">
<p>Reactive Extensions (Rx) is a library for composing asynchronous and event-based programs by using observable sequences. Essentially, it extends the observer pattern to support sequences of data and/or events and adds operators that allow you to compose sequences together declaratively while abstracting away concerns about things like low-level threading, synchronization, thread-safety, concurrent data structures, and non-blocking I/O.</p>
<p>Rx is available in various languages, including Rx.NET for C# (also called Rx.NET or Reactive Extensions for .NET).</p>
<h3>Installing Reactive Extensions</h3>
<p>To start using Reactive Extensions in a C# project, you can install it via NuGet:</p>
<pre><code class="language-bash">Install-Package System.Reactive
</code></pre>
<h3>Basic Example in C#</h3>
<p>Here's a simple example of how to create an observable sequence and subscribe to it using Rx.NET:</p>
<pre><code class="language-csharp">using System;
using System.Reactive.Linq;

class Program
{
    static void Main()
    {
        // Create an observable sequence of integers from 1 to 5.
        var observable = Observable.Range(1, 5);

        // Subscribe to the observable sequence, printing each element to the console.
        var subscription = observable.Subscribe(
            x =&gt; Console.WriteLine($&quot;OnNext: {x}&quot;), // OnNext handler
            ex =&gt; Console.WriteLine($&quot;OnError: {ex.Message}&quot;), // OnError handler
            () =&gt; Console.WriteLine(&quot;OnCompleted&quot;) // OnCompleted handler
        );

        // Dispose of the subscription when you're done to unsubscribe and free resources.
        subscription.Dispose();
    }
}
</code></pre>
<p>When you run this program, you'll see this output:</p>
<pre><code>OnNext: 1
OnNext: 2
OnNext: 3
OnNext: 4
OnNext: 5
OnCompleted
</code></pre>
<h3>Using Reactive Extensions in Real-world Scenarios</h3>
<p>Reactive Extensions can be incredibly useful in real-world scenarios such as:</p>
<ol>
<li><strong>UI Events</strong>: Easily handle UI events in desktop or mobile applications.</li>
<li><strong>Real-time Data Streams</strong>: Process streams of real-time data.</li>
<li><strong>Asynchronous Operations</strong>: Compose asynchronous operations together more easily.</li>
<li><strong>Resource Management</strong>: Automatically manage resources like network connections, file handles, etc.</li>
</ol>
<p>Here is a simple example where Rx is used to throttle search-as-you-type scenarios:</p>
<pre><code class="language-csharp">using System;
using System.Reactive.Linq;

class Program
{
    static void Main()
    {
        var searchTextChangedObservable = Observable.FromEventPattern&lt;SearchTextChangedArgs&gt;(
            h =&gt; SearchTextChanged += h,
            h =&gt; SearchTextChanged -= h);

        var throttledSearchTextChanged = searchTextChangedObservable
            .Throttle(TimeSpan.FromMilliseconds(300))
            .Select(evt =&gt; evt.EventArgs.Text);

        throttledSearchTextChanged.Subscribe(searchText =&gt;
        {
            // Perform search operation
            Console.WriteLine($&quot;Searching for: {searchText}&quot;);
        });
    }

    public static event EventHandler&lt;SearchTextChangedArgs&gt; SearchTextChanged;
}

public class SearchTextChangedArgs : EventArgs
{
    public string Text { get; set; }
}
</code></pre>
<p>In this example, the <code>Throttle</code> operator ensures that you don't perform the search operation until the user has stopped typing for 300 milliseconds, reducing the number of search operations.</p>
<p>Rx allows you to write complex asynchronous code in a more straightforward and maintainable way.</p>
                            </div>
                        </div>
                    </div>
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>Updating the elapsed time in a WPF application can be done elegantly using Reactive Extensions (Rx). Here's a simple example that demonstrates how you can do this. This example uses a <code>Label</code> control on a WPF window to display elapsed time, updated every second.</p>
<p>Firstly, install the Rx package:</p>
<pre><code class="language-bash">Install-Package System.Reactive
</code></pre>
<p>Now let's go through the C# code.</p>
<pre><code class="language-csharp">using System;
using System.Reactive.Linq;
using System.Windows;

namespace WpfAppWithRx
{
    public partial class MainWindow : Window
    {
        public MainWindow()
        {
            InitializeComponent();

            // Create an observable sequence that fires every second.
            var timer = Observable.Interval(TimeSpan.FromSeconds(1));

            // Subscribe to the timer observable to update elapsed time.
            timer.ObserveOnDispatcher() // Observe on the UI thread
                .Subscribe(elapsed =&gt;
                {
                    // Update the label content with the elapsed time.
                    ElapsedTimeLabel.Content = $&quot;Elapsed Time: {elapsed} seconds&quot;;
                });
        }
    }
}
</code></pre>
<p>Here we make use of <code>Observable.Interval</code> which creates an observable sequence that generates a value every second. Then, we subscribe to that observable and update the <code>Label</code> content accordingly. The <code>ObserveOnDispatcher()</code> ensures that the action runs on the UI thread, which is necessary for updating the UI in WPF.</p>
<p>Here's the XAML for the <code>MainWindow</code>:</p>
<pre><code class="language-xml">&lt;Window x:Class=&quot;WpfAppWithRx.MainWindow&quot;
        xmlns=&quot;http://schemas.microsoft.com/winfx/2006/xaml/presentation&quot;
        xmlns:x=&quot;http://schemas.microsoft.com/winfx/2006/xaml&quot;
        Title=&quot;MainWindow&quot; Height=&quot;200&quot; Width=&quot;400&quot;&gt;
    &lt;Grid&gt;
        &lt;Label x:Name=&quot;ElapsedTimeLabel&quot; Content=&quot;Elapsed Time:&quot; HorizontalAlignment=&quot;Center&quot; VerticalAlignment=&quot;Center&quot;/&gt;
    &lt;/Grid&gt;
&lt;/Window&gt;
</code></pre>
<p>In this example, a <code>Label</code> control named <code>ElapsedTimeLabel</code> is used to display the elapsed time. This label gets updated every second with the elapsed time since the program started.</p>
<p>This is a very basic example, but it should give you an idea of how to use Rx with WPF to update elapsed time. You can extend this further to fit into more complex scenarios or user interfaces.</p>
                            </div>
                        </div>
                    </div>
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>In the example provided, there is indeed no explicit disposal of the subscription or the underlying thread resources. In a real-world application, particularly one that has resource-sensitive constraints or that requires very careful management of subscriptions, you'd want to manage the lifecycle of your subscriptions more carefully.</p>
<p>The <code>Subscribe</code> method returns an <code>IDisposable</code> which can be disposed of to clean up the subscription. Here's a slight modification of the previous example:</p>
<pre><code class="language-csharp">using System;
using System.Reactive.Linq;
using System.Windows;

namespace WpfAppWithRx
{
    public partial class MainWindow : Window, IDisposable
    {
        private IDisposable _subscription;

        public MainWindow()
        {
            InitializeComponent();

            // Create an observable sequence that fires every second.
            var timer = Observable.Interval(TimeSpan.FromSeconds(1));

            // Subscribe to the timer observable to update elapsed time.
            _subscription = timer.ObserveOnDispatcher() // Observe on the UI thread
                .Subscribe(elapsed =&gt;
                {
                    // Update the label content with the elapsed time.
                    ElapsedTimeLabel.Content = $&quot;Elapsed Time: {elapsed} seconds&quot;;
                });
        }

        public void Dispose()
        {
            _subscription?.Dispose();
        }

        // ... (Other methods and event handlers)
    }
}
</code></pre>
<p>In this modified example, we keep track of the <code>IDisposable</code> returned by <code>Subscribe</code> in a field <code>_subscription</code>. We've also made the <code>MainWindow</code> class implement <code>IDisposable</code>. When you're sure that you're done with the timer (perhaps when the window is closed), you can call <code>Dispose()</code> to clean up the subscription.</p>
<p>Here's how you could tie it into the WPF lifecycle:</p>
<pre><code class="language-csharp">protected override void OnClosed(EventArgs e)
{
    Dispose();
    base.OnClosed(e);
}
</code></pre>
<p>This way, when the WPF window is closed, the subscription is also disposed of, effectively stopping the timer and releasing resources.</p>
<p>Note: The call to <code>ObserveOnDispatcher()</code> ensures that the subscription action is marshalled back onto the UI thread, so there's no need to manage that particular thread yourself; it's managed by the WPF Dispatcher.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <div id="331b6940-328c-4f9b-b7dc-54ab45203914" class="task done">
                <div class="task_contents">&check;
                    <span class="task_contents">MuleSoft について調べる</span>
                    <a href="#331b6940-328c-4f9b-b7dc-54ab45203914" class="permalink">&infin;</a>
                </div>
                <div class="notes">
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>MuleSoft is an integration platform for connecting applications, data sources, and APIs, both in the cloud and on-premises. MuleSoft's Anypoint Platform offers a variety of tools and services, including Anypoint Studio, CloudHub, and Mule ESB (Enterprise Service Bus) to help organizations build application networks.</p>
<p>Here are some key features and capabilities:</p>
<h3>Features</h3>
<ol>
<li><p><strong>API-Led Connectivity</strong>: MuleSoft emphasizes a design approach where reusable APIs serve as the building blocks of your integration solutions. This can lead to more maintainable, scalable, and reusable code.</p>
</li>
<li><p><strong>Wide Range of Connectors</strong>: Connect to databases, SaaS platforms, IoT devices, and virtually any other endpoint you can think of. You can also build your custom connectors.</p>
</li>
<li><p><strong>Data Transformation</strong>: MuleSoft provides powerful data transformation capabilities using DataWeave, its expression language.</p>
</li>
<li><p><strong>Scalability</strong>: Deploy Mule applications on-premises, in the cloud, or in a hybrid configuration. You can scale your integrations horizontally to process a large number of transactions.</p>
</li>
<li><p><strong>Real-time Monitoring and Analytics</strong>: Anypoint Monitoring and Analytics provide insights into how your APIs and integrations are performing.</p>
</li>
<li><p><strong>Security</strong>: Built-in support for tokenization, encryption, and secure property management, among other security features.</p>
</li>
</ol>
<h3>How It Works</h3>
<ul>
<li><p><strong>Design</strong>: Start by designing your API in a web-based design environment or Anypoint Studio, MuleSoft's Eclipse-based IDE.</p>
</li>
<li><p><strong>Develop</strong>: Implement your API and integration flows by dragging and dropping connectors, transformers, and components onto a canvas in Anypoint Studio.</p>
</li>
<li><p><strong>Deploy</strong>: Deploy your application onto a Mule Server or a CloudHub, MuleSoft's iPaaS (integration Platform as a Service) offering.</p>
</li>
<li><p><strong>Manage and Analyze</strong>: Once deployed, manage, monitor, and analyze your API through the Anypoint Management Center.</p>
</li>
</ul>
<h3>Use Cases</h3>
<p>MuleSoft is commonly used in scenarios such as:</p>
<ul>
<li>Connecting legacy systems to modern front-ends</li>
<li>Syncing data between disparate systems</li>
<li>Orchestrating microservices</li>
<li>Creating APIs for exposing data and services</li>
</ul>
<p>MuleSoft competes with other integration platforms and ESB products like Apache Camel, WSO2, IBM Integration Bus, and others, but it provides a good mix of ease-of-use, scalability, and extensive capabilities out of the box.</p>
                            </div>
                        </div>
                    </div>
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>MuleSoft itself is primarily a Java-based platform; it doesn't run on C# or the .NET runtime. However, you can certainly interact with MuleSoft applications from C# code in several ways:</p>
<ol>
<li><p><strong>RESTful APIs</strong>: If a MuleSoft application exposes RESTful APIs, you can use the <code>HttpClient</code> class in C# to interact with it. This is one of the most common ways to integrate C# applications with MuleSoft.</p>
</li>
<li><p><strong>SOAP Web Services</strong>: If the MuleSoft application exposes a SOAP-based web service, you can use C# libraries like <code>System.ServiceModel</code> to create a SOAP client.</p>
</li>
<li><p><strong>Messaging Services</strong>: If the MuleSoft application uses a messaging service like RabbitMQ, Kafka, or JMS, you can use respective C# client libraries to interact with those services.</p>
</li>
<li><p><strong>Database</strong>: If both the C# application and the MuleSoft application read from/write to a common database, that's another way they can interact.</p>
</li>
<li><p><strong>File Systems, FTP, SFTP</strong>: If either system writes files that the other reads, you could use filesystem or FTP/SFTP as an integration point.</p>
</li>
</ol>
<h3>Example of calling a MuleSoft REST API from C#</h3>
<p>Here's a simple example using <code>HttpClient</code> to GET data from a MuleSoft API:</p>
<pre><code class="language-csharp">using System;
using System.Net.Http;
using System.Threading.Tasks;

class Program
{
    static async Task Main()
    {
        HttpClient httpClient = new HttpClient();

        try
        {
            HttpResponseMessage response = await httpClient.GetAsync(&quot;http://mulesoft-api-endpoint/resource&quot;);
            response.EnsureSuccessStatusCode();
            string responseBody = await response.Content.ReadAsStringAsync();

            Console.WriteLine($&quot;Received data: {responseBody}&quot;);
        }
        catch (HttpRequestException e)
        {
            Console.WriteLine($&quot;Request error: {e.Message}&quot;);
        }
    }
}
</code></pre>
<h3>Example of using a SOAP service</h3>
<p>If MuleSoft exposes a SOAP service, you can use <code>SvcUtil.exe</code> to generate a C# proxy and then use it in your code to call the MuleSoft SOAP service.</p>
<pre><code class="language-csharp">using System;
using YourNamespace.YourServiceReference; // Generated using SvcUtil.exe

class Program
{
    static void Main()
    {
        YourServiceClient client = new YourServiceClient();

        try
        {
            var result = client.YourMethod(parameters);
            Console.WriteLine($&quot;Received data: {result}&quot;);
        }
        catch (Exception e)
        {
            Console.WriteLine($&quot;Service error: {e.Message}&quot;);
        }
    }
}
</code></pre>
<p>In all these ways, while you can't run MuleSoft on the .NET runtime, you can absolutely make your C#/.NET applications interoperate smoothly with MuleSoft.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <div id="d3e2ecb0-a3d2-4f4f-9b4c-6070f1d6c152" class="task done">
                <div class="task_contents">&check;
                    <span class="task_contents">Dell Boomi について調べる</span>
                    <a href="#d3e2ecb0-a3d2-4f4f-9b4c-6070f1d6c152" class="permalink">&infin;</a>
                </div>
                <div class="notes">
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>Dell Boomi is an Integration Platform as a Service (iPaaS) that facilitates the integration of various cloud-based and on-premises applications and data. It's a low-code/no-code platform, which means it is designed to allow business users as well as developers to create integrations without writing code (or with minimal coding).</p>
<h3>Key Features</h3>
<ol>
<li><p><strong>Drag-and-Drop Interface</strong>: Boomi provides a visual interface for designing integrations, making it relatively easy for non-developers to work with.</p>
</li>
<li><p><strong>Pre-built Connectors</strong>: Boomi comes with a large number of pre-built connectors for various SaaS applications, databases, and protocols.</p>
</li>
<li><p><strong>Data Mapping and Transformation</strong>: The platform offers strong capabilities for mapping and transforming data as it flows between systems.</p>
</li>
<li><p><strong>Process Automation</strong>: Beyond mere data integration, Boomi can also automate complex business processes.</p>
</li>
<li><p><strong>Scalability</strong>: Boomi is designed to scale both vertically and horizontally, catering to varying business requirements.</p>
</li>
<li><p><strong>Community and Reusable Components</strong>: Boomi has a strong community and allows you to use pre-built templates and components, shared by other users, to speed up the development process.</p>
</li>
<li><p><strong>API Management</strong>: Boomi also has API management capabilities, allowing you to both consume existing APIs and expose your services as APIs.</p>
</li>
<li><p><strong>Monitoring and Analytics</strong>: Boomi provides robust monitoring and analytics tools for managing your integrations and understanding data flows.</p>
</li>
</ol>
<h3>Typical Use Cases</h3>
<ol>
<li><p><strong>Application Integration</strong>: Linking disparate business applications to function as a cohesive unit.</p>
</li>
<li><p><strong>Data Integration and ETL</strong>: Extract, transform, and load data between different databases, either for reporting or to sync data.</p>
</li>
<li><p><strong>B2B/EDI Integration</strong>: Automate and streamline transactions and data transfer with business partners.</p>
</li>
<li><p><strong>API Design and Management</strong>: Develop, deploy, and manage APIs.</p>
</li>
<li><p><strong>IoT Integration</strong>: Integrate with various IoT platforms and devices to collect and analyze data.</p>
</li>
</ol>
<h3>Interacting with Dell Boomi from Other Platforms</h3>
<p>Dell Boomi primarily provides integration solutions, but you can interact with Boomi processes through various means, such as:</p>
<ul>
<li><p><strong>Web Services</strong>: Boomi can expose processes as web services that can be consumed from other applications, including those written in languages like Java, C#, Python, etc.</p>
</li>
<li><p><strong>HTTP Requests</strong>: Boomi allows you to trigger processes using simple HTTP requests, providing a straightforward way to integrate with other platforms.</p>
</li>
<li><p><strong>Messaging Services</strong>: You can use popular messaging protocols like JMS, AMQP, or MQTT to trigger or interact with Boomi processes.</p>
</li>
<li><p><strong>Direct Database Access</strong>: Processes in Boomi can read from or write to databases, which can also be accessed from other applications.</p>
</li>
</ul>
<p>While Dell Boomi and platforms like it offer a great deal of flexibility and ease of use, they are not usually the right fit for every single use case. They excel for certain types of integration scenarios but might not offer the same level of fine-grained control that a fully coded solution could provide.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <div id="83890f8a-7843-4ea2-9fde-8f986202c069" class="task done">
                <div class="task_contents">&check;
                    <span class="task_contents">Web Components について調べる → Razor Components</span>
                    <a href="#83890f8a-7843-4ea2-9fde-8f986202c069" class="permalink">&infin;</a>
                </div>
                <div class="notes">
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>In the context of web development, the term &quot;web component&quot; typically refers to a reusable encapsulated component built using web technologies like HTML, JavaScript, and CSS. Web components are not natively related to C# or any other server-side language; they are primarily client-side constructs.</p>
<p>However, if you are developing a web application using ASP.NET Core, you can build reusable &quot;components&quot; using Razor Components, which are part of the Blazor framework. Razor Components can encapsulate rendering logic, data, and event handling in a reusable unit, somewhat similar to how web components work in front-end frameworks.</p>
<h3>Simple Razor Component Example</h3>
<p>Here's a simple example of a Razor Component that provides a form to send an email:</p>
<pre><code class="language-razor">@code {
    private string To { get; set; }
    private string Subject { get; set; }
    private string Body { get; set; }

    private void SendEmail()
    {
        // Logic to send email using C#
        // This can call an API endpoint, or use SmtpClient to send email directly
        // For example, call HttpClient to POST data to &quot;/api/Email&quot;
    }
}

&lt;div&gt;
    &lt;h3&gt;Send Email&lt;/h3&gt;
    &lt;form&gt;
        &lt;div&gt;
            &lt;label for=&quot;to&quot;&gt;To:&lt;/label&gt;
            &lt;input type=&quot;email&quot; id=&quot;to&quot; @bind=&quot;To&quot; /&gt;
        &lt;/div&gt;
        &lt;div&gt;
            &lt;label for=&quot;subject&quot;&gt;Subject:&lt;/label&gt;
            &lt;input type=&quot;text&quot; id=&quot;subject&quot; @bind=&quot;Subject&quot; /&gt;
        &lt;/div&gt;
        &lt;div&gt;
            &lt;label for=&quot;body&quot;&gt;Body:&lt;/label&gt;
            &lt;textarea id=&quot;body&quot; @bind=&quot;Body&quot;&gt;&lt;/textarea&gt;
        &lt;/div&gt;
        &lt;button type=&quot;button&quot; @onclick=&quot;SendEmail&quot;&gt;Send Email&lt;/button&gt;
    &lt;/form&gt;
&lt;/div&gt;
</code></pre>
<p>This component uses Razor syntax to define a simple form. When the &quot;Send Email&quot; button is clicked, the <code>SendEmail</code> method will be invoked, where you can put your logic to actually send the email. Note the <code>@bind</code> attribute used for two-way data binding between the input elements and C# properties.</p>
<h3>Using the Razor Component</h3>
<p>To use this component in another Razor component or page, you simply include it using its tag name, which by default is the file name:</p>
<pre><code class="language-razor">@page &quot;/some-page&quot;

&lt;h1&gt;Some Page&lt;/h1&gt;

&lt;YourComponentName /&gt;
</code></pre>
<p>So, while you can't directly create &quot;web components&quot; in C# (since web components are a browser technology), you can create something very similar using Razor Components in a Blazor application. These components are rendered either on the server and sent to the client, or rendered directly in the client browser using WebAssembly, depending on how you've configured your Blazor app.</p>
                            </div>
                        </div>
                    </div>
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>Yes, it's entirely possible to implement a wiki engine as a Razor component (or a set of Razor components) that can be reused across multiple ASP.NET Core Blazor applications. The component could encapsulate all the wiki-related functionality such as creating, editing, and viewing articles, tracking history, etc.</p>
<h3>Considerations for Reusability:</h3>
<ol>
<li><p><strong>Modularity</strong>: Design the wiki engine as a modular component or a set of components. This allows the primary functions of a wiki—like rendering markdown or other markup, editing text, and keeping track of page history—to be encapsulated in reusable units.</p>
</li>
<li><p><strong>Dependency Injection</strong>: Use dependency injection to supply services that the wiki engine will need, such as data storage and retrieval, user authentication, etc. This will allow you to easily swap out these services in different applications.</p>
</li>
<li><p><strong>Configuration</strong>: Allow key aspects of the component to be easily configured so that it can adapt to different environments, data sources, etc. This could be done through parameters or a configuration object.</p>
</li>
<li><p><strong>Styling</strong>: Use scoped or parameterized CSS so that the component can adapt to the styling of the host application.</p>
</li>
<li><p><strong>Documentation</strong>: Thoroughly document how to integrate the component into a new project, including any dependencies, necessary services, etc.</p>
</li>
</ol>
<h3>Example Architecture:</h3>
<p>Here's a simplified example to illustrate what the architecture of such a Razor component might look like:</p>
<pre><code class="language-csharp">// WikiEngine.razor

@inject IWikiStorageService WikiStorageService

@code {
    private string CurrentArticle { get; set; }

    private void LoadArticle(string articleName)
    {
        CurrentArticle = WikiStorageService.GetArticle(articleName);
    }

    private void SaveArticle(string articleName, string content)
    {
        WikiStorageService.SaveArticle(articleName, content);
    }
}

&lt;!-- Markup and logic for displaying/editing an article --&gt;

&lt;div&gt;
    &lt;button @onclick=&quot;() =&gt; LoadArticle('SomeArticle')&quot;&gt;Load Some Article&lt;/button&gt;
    &lt;button @onclick=&quot;() =&gt; SaveArticle('SomeArticle', CurrentArticle)&quot;&gt;Save Some Article&lt;/button&gt;

    &lt;div&gt;
        @CurrentArticle
    &lt;/div&gt;
&lt;/div&gt;
</code></pre>
<p>Here <code>IWikiStorageService</code> would be an interface defining the operations required to load and save wiki articles. An implementation of this interface would handle the actual storage, whether that's a database, a cloud storage service, or something else.</p>
<h3>Reusing the Component:</h3>
<p>Once the component (or components) is developed and tested, you could package it as a Razor Class Library. This would make it easy to reuse the component across multiple Blazor applications by simply referencing the library.</p>
<p>By carefully planning out the architecture and dependencies, and by making good use of Blazor's built-in features for modularity and reusability, you can definitely build a wiki engine as a Razor component that can be reused across multiple systems.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <div class="note">
                <div class="note_contents">
                    <div class="note_ai_generated">
<p>Yes, a Razor Class Library (RCL) is different from a standard C# Class Library, although both are types of class libraries in .NET. Here are the main differences:</p>
<h3>Razor Class Library (RCL):</h3>
<ol>
<li><p><strong>Content</strong>: In addition to C# code, a Razor Class Library can contain Razor files (<code>*.razor</code>), HTML, CSS, and JavaScript files, enabling the development of reusable web UI components.</p>
</li>
<li><p><strong>Purpose</strong>: It is designed for the purpose of sharing Razor components and static assets across different web applications.</p>
</li>
<li><p><strong>Framework-specific</strong>: RCLs are intended to be used with ASP.NET Core projects, specifically with Blazor and MVC applications that can consume Razor components or views.</p>
</li>
<li><p><strong>Reusable UI Components</strong>: You can bundle both the logic (in C#) and the view (in Razor/HTML/CSS) as a single reusable unit.</p>
</li>
<li><p><strong>Static Web Assets</strong>: RCLs can also include static web assets like images, JS, and CSS files that get bundled when the library is packed.</p>
</li>
</ol>
<h3>C# Class Library:</h3>
<ol>
<li><p><strong>Content</strong>: Contains only C# code files (<code>*.cs</code>) and other supporting files but not Razor, HTML, CSS, or JavaScript files.</p>
</li>
<li><p><strong>Purpose</strong>: Designed to be a reusable set of classes and methods and can be used in any kind of C# application - web, desktop, services, etc.</p>
</li>
<li><p><strong>Framework-agnostic</strong>: Can be used across different types of .NET projects, not limited to ASP.NET Core or any specific framework.</p>
</li>
<li><p><strong>No UI Components</strong>: Typically contains only logic and data models; no user interface components.</p>
</li>
<li><p><strong>No Static Web Assets</strong>: Does not include a mechanism for bundling static web assets.</p>
</li>
</ol>
<h3>How to Create:</h3>
<ul>
<li>A Razor Class Library can be created in Visual Studio by selecting &quot;Razor Class Library&quot; as the project type.</li>
<li>A standard C# Class Library can be created by selecting &quot;Class Library&quot; as the project type.</li>
</ul>
<p>In summary, while both are class libraries that allow you to encapsulate and reuse code, a Razor Class Library allows you to encapsulate and share UI components and static assets for web applications, whereas a standard C# Class Library is geared towards encapsulating business logic or data models that can be reused across different types of .NET applications.</p>
                    </div>
                    <!-- Task: 8989a666-6422-4e62-8fee-00e5e5b340f0 -->
                </div>
            </div>
            <div id="a12b4fe9-78cd-428d-9371-ee6fac5e15d8" class="task done">
                <div class="task_contents">&check;
                    <span class="task_contents">XML の読み書きのライブラリーを探す</span>
                    <a href="#a12b4fe9-78cd-428d-9371-ee6fac5e15d8" class="permalink">&infin;</a>
                </div>
                <div class="notes">
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>In C#, there are several libraries and classes available for reading and writing XML data:</p>
<h3>XmlReader and XmlWriter</h3>
<p>These are low-level classes that provide a fast, forward-only, read-only (in the case of <code>XmlReader</code>) or write-only (in the case of <code>XmlWriter</code>) cursor-based API.</p>
<h4>Reading with XmlReader</h4>
<pre><code class="language-csharp">using System.Xml;

class Program
{
    static void Main()
    {
        using (XmlReader reader = XmlReader.Create(&quot;file.xml&quot;))
        {
            while (reader.Read())
            {
                if (reader.NodeType == XmlNodeType.Element &amp;&amp; reader.Name == &quot;tag&quot;)
                {
                    string content = reader.ReadElementContentAsString();
                    // Process content
                }
            }
        }
    }
}
</code></pre>
<h4>Writing with XmlWriter</h4>
<pre><code class="language-csharp">using System.Xml;

class Program
{
    static void Main()
    {
        using (XmlWriter writer = XmlWriter.Create(&quot;file.xml&quot;))
        {
            writer.WriteStartDocument();
            writer.WriteStartElement(&quot;root&quot;);
            writer.WriteElementString(&quot;tag&quot;, &quot;content&quot;);
            writer.WriteEndElement();
            writer.WriteEndDocument();
        }
    }
}
</code></pre>
<h3>XmlDocument</h3>
<p>This provides a more traditional Document Object Model (DOM) style API, where you can navigate back and forth through the XML tree and manipulate elements.</p>
<h4>Reading with XmlDocument</h4>
<pre><code class="language-csharp">using System.Xml;

class Program
{
    static void Main()
    {
        XmlDocument doc = new XmlDocument();
        doc.Load(&quot;file.xml&quot;);
        XmlNode node = doc.SelectSingleNode(&quot;/root/tag&quot;);
        if (node != null)
        {
            string content = node.InnerText;
            // Process content
        }
    }
}
</code></pre>
<h4>Writing with XmlDocument</h4>
<pre><code class="language-csharp">using System.Xml;

class Program
{
    static void Main()
    {
        XmlDocument doc = new XmlDocument();
        XmlElement root = doc.CreateElement(&quot;root&quot;);
        doc.AppendChild(root);
        XmlElement tag = doc.CreateElement(&quot;tag&quot;);
        tag.InnerText = &quot;content&quot;;
        root.AppendChild(tag);
        doc.Save(&quot;file.xml&quot;);
    }
}
</code></pre>
<h3>XDocument and XElement (LINQ to XML)</h3>
<p>These are part of the LINQ to XML API and provide a more modern and convenient way of working with XML data.</p>
<h4>Reading with XDocument</h4>
<pre><code class="language-csharp">using System.Xml.Linq;

class Program
{
    static void Main()
    {
        XDocument doc = XDocument.Load(&quot;file.xml&quot;);
        XElement tag = doc.Root.Element(&quot;tag&quot;);
        if (tag != null)
        {
            string content = tag.Value;
            // Process content
        }
    }
}
</code></pre>
<h4>Writing with XDocument</h4>
<pre><code class="language-csharp">using System.Xml.Linq;

class Program
{
    static void Main()
    {
        XDocument doc = new XDocument(
            new XElement(&quot;root&quot;,
                new XElement(&quot;tag&quot;, &quot;content&quot;)
            )
        );
        doc.Save(&quot;file.xml&quot;);
    }
}
</code></pre>
<p>These are just the basics. Each of these approaches has its own set of features and capabilities, and you can choose the one that best suits your needs.</p>
                            </div>
                        </div>
                    </div>
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>In addition to the built-in .NET libraries for handling XML, there are several external libraries and packages that offer more features or different approaches for XML processing. Here are some popular ones:</p>
<h3>XmlSerializer</h3>
<p>Although it's a part of .NET Standard, the <code>XmlSerializer</code> class is worth mentioning because it simplifies the process of converting objects to XML and vice versa.</p>
<pre><code class="language-csharp">using System.IO;
using System.Xml.Serialization;

public class Person
{
    public string Name { get; set; }
    public int Age { get; set; }
}

// Serialize
var serializer = new XmlSerializer(typeof(Person));
using (var writer = new StringWriter())
{
    serializer.Serialize(writer, new Person { Name = &quot;John&quot;, Age = 30 });
    string xml = writer.ToString();
}

// Deserialize
using (var reader = new StringReader(xml))
{
    var person = (Person)serializer.Deserialize(reader);
}
</code></pre>
<h3>Newtonsoft.Json</h3>
<p>Even though Newtonsoft.Json is primarily known for JSON serialization, it also provides some utilities for converting JSON to XML and vice versa.</p>
<h3>System.Xml.XPath and System.Xml.Xsl</h3>
<p>These are also part of .NET but offer XPath and XSLT support, which can be quite powerful for querying and transforming XML documents.</p>
<h3>XmlDoc2CmdletDoc</h3>
<p>This tool is more specific to PowerShell but can be useful for generating documentation from XML comments in C# code, particularly for cmdlets.</p>
<h3>SgmlReader</h3>
<p>This library is used for parsing SGML files and can also be used to convert SGML to XML. This could be useful if you're dealing with older systems or document formats like HTML that aren't necessarily well-formed XML.</p>
<h3>RestSharp</h3>
<p>Though not an XML-specific library, RestSharp simplifies the process of making HTTP requests and can automatically serialize/deserialize XML for you. It's very handy when interacting with web services that return XML.</p>
<h3>External XML Parsers</h3>
<p>There are also some third-party libraries not specific to .NET that provide XML parsing capabilities and that you can use in a .NET project by creating a wrapper or using P/Invoke for C libraries, like libxml2.</p>
<p>Before choosing an external library, make sure to evaluate whether its features align with your specific requirements and whether the library is actively maintained and well-documented.</p>
                            </div>
                        </div>
                    </div>
                    <div class="note">
                        <div class="note_contents">
                            <p class="note_contents">ビルトインのもので足りるようだ。</p>
                        </div>
                    </div>
                </div>
            </div>
            <div id="15c1e19c-edf9-4036-a76b-4c8d905bf069" class="task done">
                <div class="task_contents">&check;
                    <span class="task_contents">JSON Web Token について調べる</span>
                    <a href="#15c1e19c-edf9-4036-a76b-4c8d905bf069" class="permalink">&infin;</a>
                </div>
                <div class="notes">
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>JSON Web Token (JWT) is a compact and self-contained way for securely transmitting information between parties as a JSON object. JWT is commonly used for authorization and information exchange. The token is encoded and potentially signed and/or encrypted, making it a safe way to pass claims between two parties.</p>
<h3>Structure of JWT</h3>
<p>A JWT typically consists of three parts:</p>
<ol>
<li><p><strong>Header</strong>: The header usually consists of two parts: the type of token, which is JWT, and the signing algorithm being used, such as HMAC SHA256 or RSA.</p>
</li>
<li><p><strong>Payload</strong>: The payload contains the claims. Claims are statements about an entity (typically, the user) and additional data.</p>
</li>
<li><p><strong>Signature</strong>: The signature is used to verify that the sender of the JWT is who it says it is and to ensure that the message wasn't changed along the way.</p>
</li>
</ol>
<p>These three parts are encoded individually and concatenated with periods (<code>.</code>).</p>
<h3>Using JWT in C#</h3>
<p>In C#, the <code>System.IdentityModel.Tokens.Jwt</code> namespace provides classes to work with JWT. You'll likely need to install the <code>System.IdentityModel.Tokens.Jwt</code> NuGet package to get started.</p>
<p>Here's a simple example to create a JWT:</p>
<pre><code class="language-csharp">using System;
using System.Collections.Generic;
using Microsoft.IdentityModel.Tokens;
using System.IdentityModel.Tokens.Jwt;
using System.Security.Claims;

class Program
{
    static void Main()
    {
        var secretKey = &quot;some_very_long_and_secure_key_here&quot;;
        var securityKey = new SymmetricSecurityKey(System.Text.Encoding.UTF8.GetBytes(secretKey));
        var credentials = new SigningCredentials(securityKey, SecurityAlgorithms.HmacSha256);

        var claims = new List&lt;Claim&gt;
        {
            new Claim(JwtRegisteredClaimNames.Sub, &quot;some_username&quot;),
            new Claim(&quot;role&quot;, &quot;admin&quot;)
        };

        var token = new JwtSecurityToken(
            issuer: &quot;your_domain.com&quot;,
            audience: &quot;your_audience&quot;,
            claims: claims,
            expires: DateTime.Now.AddMinutes(30),
            signingCredentials: credentials
        );

        var tokenHandler = new JwtSecurityTokenHandler();
        var jwtToken = tokenHandler.WriteToken(token);

        Console.WriteLine(jwtToken);
    }
}
</code></pre>
<p>And to validate a JWT:</p>
<pre><code class="language-csharp">using System;
using Microsoft.IdentityModel.Tokens;
using System.IdentityModel.Tokens.Jwt;

class Program
{
    static void Main()
    {
        var tokenHandler = new JwtSecurityTokenHandler();
        var jwtToken = &quot;your_jwt_token_here&quot;;
        var secretKey = &quot;some_very_long_and_secure_key_here&quot;;
        var securityKey = new SymmetricSecurityKey(System.Text.Encoding.UTF8.GetBytes(secretKey));

        var validationParameters = new TokenValidationParameters
        {
            ValidateIssuerSigningKey = true,
            IssuerSigningKey = securityKey,
            ValidateIssuer = false,
            ValidateAudience = false
        };

        try
        {
            tokenHandler.ValidateToken(jwtToken, validationParameters, out SecurityToken validatedToken);
            Console.WriteLine(&quot;Token is valid&quot;);
        }
        catch
        {
            Console.WriteLine(&quot;Token is invalid&quot;);
        }
    }
}
</code></pre>
<p>In a real-world scenario, you'll also likely want to validate the <code>issuer</code> and <code>audience</code> claims and possibly others to ensure the token is valid for its intended use.</p>
<p>Keep in mind that JWTs are not encrypted by default, so sensitive data should not be stored directly in a JWT unless it is encrypted.</p>
                            </div>
                        </div>
                    </div>
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>ASP.NET Core Identity and JSON Web Tokens (JWT) are two separate but often complementary technologies used in the context of web applications. They serve different purposes and can be used independently or together, depending on your needs.</p>
<h3>ASP.NET Core Identity</h3>
<p>ASP.NET Core Identity is a membership system that allows you to add login functionality to your application. It helps you manage users, passwords, profile data, roles, claims, tokens, email confirmation, and more.</p>
<p>Out of the box, ASP.NET Core Identity uses cookie-based authentication. When a user logs in, a cookie is set in their browser, and this cookie is used to authenticate subsequent requests.</p>
<h3>JSON Web Tokens (JWT)</h3>
<p>JWT is a standard for encoding claims between two parties. The claims in a JWT are encoded as a JSON object that is digitally signed using JSON Web Signature (JWS). JWTs are often used for authentication and information exchange in web development.</p>
<h3>Relationship</h3>
<p>ASP.NET Core Identity doesn't depend on JWT, nor does JWT depend on ASP.NET Core Identity. However, you can configure ASP.NET Core Identity to use JWT as its token format, particularly when you are building stateless APIs that don't maintain session state between requests.</p>
<p>In such a case, when a user logs in, instead of (or in addition to) setting a cookie, the server would return a JWT that encapsulates the user's identity information. This token would then be included in the <code>Authorization</code> header in subsequent API requests, allowing the server to identify the user.</p>
<p>In summary, while ASP.NET Core Identity and JWT can be used independently, they are often used together in modern web applications to provide a comprehensive authentication and authorization solution.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <div id="fce9f10e-8536-4082-89ba-a3d874983925" class="task done">
                <div class="task_contents">&check;
                    <span class="task_contents">Azure Cognitive Search について調べる</span>
                    <a href="#fce9f10e-8536-4082-89ba-a3d874983925" class="permalink">&infin;</a>
                </div>
                <div class="notes">
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>Azure Cognitive Search is a cloud-based search service from Microsoft's Azure platform that provides developers with the tools to build robust search functionality into their applications, websites, and services. It provides capabilities such as text-based search, filtering, and complex types, allowing developers to build search experiences similar to those offered by large commercial websites. The service is designed to be highly customizable and to scale easily with your needs.</p>
<h3>Key Features:</h3>
<ol>
<li><p><strong>Text Search</strong>: Full-text search capabilities, offering features like fuzzy search, wildcards, and Boolean operators.</p>
</li>
<li><p><strong>Faceted Navigation</strong>: Allows users to filter and refine search results based on various attributes or facets.</p>
</li>
<li><p><strong>Ranking and Scoring</strong>: Supports customized ranking of search results to promote the most relevant items.</p>
</li>
<li><p><strong>Multi-language Support</strong>: Comes with built-in language analyzers for text from multiple languages.</p>
</li>
<li><p><strong>Typeahead and Autocomplete</strong>: Features for suggesting queries as the user types.</p>
</li>
<li><p><strong>Synonyms</strong>: Supports synonym mapping to ensure that search results capture different phrasings or terminology.</p>
</li>
<li><p><strong>Cognitive Services Integration</strong>: Ability to integrate with other Azure Cognitive Services like Computer Vision and Text Analytics to extract insights from images and text.</p>
</li>
<li><p><strong>Security</strong>: Offers built-in security measures, including encryption at rest and in transit, as well as integration with Azure Active Directory for identity and access management.</p>
</li>
<li><p><strong>Indexers</strong>: Can automatically populate search indexes from a variety of data sources like Azure SQL Database, Azure Cosmos DB, and more.</p>
</li>
<li><p><strong>Scalability</strong>: Designed to scale easily to accommodate growing data volumes and query loads.</p>
</li>
</ol>
<h3>Typical Use Cases:</h3>
<ol>
<li><p><strong>E-commerce Websites</strong>: Enhance product search and improve customer experience.</p>
</li>
<li><p><strong>Content Discovery</strong>: Make articles, blogs, and other content easily discoverable on content-heavy websites.</p>
</li>
<li><p><strong>Enterprise Search</strong>: Allow employees to search through internal documents, databases, and other resources.</p>
</li>
<li><p><strong>Media Libraries</strong>: Facilitate the search of large volumes of images, videos, and other media types.</p>
</li>
<li><p><strong>Real Estate Platforms</strong>: Enable users to search for properties based on multiple attributes such as location, price, and features.</p>
</li>
<li><p><strong>Customer Service</strong>: Help users quickly find answers to their questions by searching through FAQs, support documents, and more.</p>
</li>
</ol>
<h3>Development Languages and SDKs</h3>
<p>Azure Cognitive Search supports a range of programming languages through SDKs and APIs, including .NET, Python, Java, and JavaScript. You can also make direct REST API calls to interact with the service.</p>
<p>Overall, Azure Cognitive Search offers a powerful and scalable solution for adding sophisticated search capabilities to various types of applications and services.</p>
                            </div>
                        </div>
                    </div>
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>In a real-world application, you might want to integrate natural language processing (NLP) capabilities or machine learning algorithms to find an answer to a natural language question in a large set of text data. However, a simplified example using Azure Cognitive Search and C# could look something like the following:</p>
<p>Firstly, you'll need to install the Azure.Search.Documents NuGet package. You can install it via Package Manager Console with this command:</p>
<pre><code class="language-bash">Install-Package Azure.Search.Documents -Version 11.4.1
</code></pre>
<p>Here's some sample code for performing a search query:</p>
<pre><code class="language-csharp">using Azure;
using Azure.Search.Documents;
using Azure.Search.Documents.Models;
using System;
using System.Threading.Tasks;

class Program
{
    private static readonly Uri serviceEndpoint = new Uri(&quot;https://&lt;Your-Service-Name&gt;.search.windows.net/&quot;);
    private static readonly AzureKeyCredential credential = new AzureKeyCredential(&quot;&lt;Your-Search-Service-API-Key&gt;&quot;);

    static async Task Main(string[] args)
    {
        SearchClient client = new SearchClient(serviceEndpoint, &quot;your-index-name&quot;, credential);

        // Assume the question is &quot;What is the capital of France?&quot;
        string userQuestion = &quot;What is the capital of France?&quot;;

        // Perform a search query
        SearchResults&lt;SearchDocument&gt; response = await client.SearchAsync&lt;SearchDocument&gt;(userQuestion);

        // Process the search result
        foreach (SearchResult&lt;SearchDocument&gt; result in response.GetResults())
        {
            // Here you might have more complex logic to determine the best answer
            // For the sake of this example, just print the result
            Console.WriteLine($&quot;Document Key: {result.Document[&quot;id&quot;]}, Score: {result.Score}&quot;);
            Console.WriteLine($&quot;Answer: {result.Document[&quot;answerField&quot;]}&quot;);
        }
    }
}
</code></pre>
<p>In this example, replace <code>&lt;Your-Service-Name&gt;</code> and <code>&lt;Your-Search-Service-API-Key&gt;</code> with your Azure Cognitive Search service name and API key, respectively. Also, replace <code>&quot;your-index-name&quot;</code> and <code>&quot;answerField&quot;</code> with the actual names you have used in your Azure Cognitive Search service.</p>
<p>This is a highly simplified example. Real-world applications often require more advanced features like scoring profiles, filtering, facets, and integration with NLP services to interpret and find answers to natural language questions effectively.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <div id="cff6b0d1-c0e3-453a-9244-6c249b8dc77d" class="task done">
                <div class="task_contents">&check;
                    <span class="task_contents">Azure OpenAI Service On Your Data について調べる</span>
                    <a href="#cff6b0d1-c0e3-453a-9244-6c249b8dc77d" class="permalink">&infin;</a>
                </div>
                <div class="notes">
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>As of my last update in January 2022, Azure OpenAI Service on Your Data is a service that allows businesses and developers to run OpenAI's models, such as GPT-3, on their own premises or within their own Azure environment. This enables organizations to maintain data privacy and compliance while still taking advantage of OpenAI's state-of-the-art models.</p>
<h3>Key Features:</h3>
<ol>
<li><p><strong>Data Privacy</strong>: By running the OpenAI model on your data in your own environment, you can meet strict data privacy and sovereignty requirements.</p>
</li>
<li><p><strong>Low Latency</strong>: Running the model locally can also offer performance benefits, including reduced latency.</p>
</li>
<li><p><strong>Flexibility and Customization</strong>: You can tailor the model to suit your specific requirements, such as integrating it into your existing workflows or pairing it with other Azure services for more advanced use-cases.</p>
</li>
<li><p><strong>Scalability</strong>: The service is designed to scale with your needs, allowing you to handle large volumes of data as necessary.</p>
</li>
<li><p><strong>Azure Integration</strong>: Given that it's an Azure service, it naturally integrates well with other Azure offerings like Azure Cognitive Services, Azure Machine Learning, Azure Databases, etc.</p>
</li>
</ol>
<h3>Typical Use Cases:</h3>
<ol>
<li><p><strong>Natural Language Understanding</strong>: Process and analyze large sets of textual data for applications such as sentiment analysis, summarization, and more.</p>
</li>
<li><p><strong>Automated Customer Service</strong>: Implement conversational agents that can handle customer inquiries without storing sensitive data outside your controlled environment.</p>
</li>
<li><p><strong>Data Extraction</strong>: Automatically pull relevant information or perform transformations on structured and unstructured data.</p>
</li>
<li><p><strong>Content Creation and Enhancement</strong>: Generate textual content or enhance existing content in a more controlled environment.</p>
</li>
<li><p><strong>Custom Solutions</strong>: Develop specialized solutions that require deep understanding or manipulation of natural language, all while keeping the data on your premises or within your Azure environment.</p>
</li>
</ol>
<h3>Development and Deployment:</h3>
<p>Just like with other Azure services, you would generally use SDKs or APIs to integrate OpenAI models into your application. You'll need to configure the Azure environment to meet the resource requirements of the OpenAI model, which can be resource-intensive.</p>
<p>Remember that specific details, such as pricing and availability, would depend on your agreement with Azure and OpenAI. Always refer to the most current documentation for the most up-to-date information.</p>
<p>Please note that the landscape of cloud services and machine learning is continually evolving, and new features or changes could have been introduced after my training data was last updated in January 2022.</p>
                            </div>
                        </div>
                    </div>
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>Azure OpenAI Service on Your Data and Azure Cognitive Search serve different purposes and are used for different types of tasks, even though both can deal with large volumes of data and have applications in text-based operations.</p>
<h3>Azure Cognitive Search:</h3>
<ol>
<li><p><strong>Purpose</strong>: Specifically designed to add sophisticated search capabilities to applications and services, including full-text search, faceted navigation, and filtering.</p>
</li>
<li><p><strong>Data Types</strong>: Primarily aimed at structured or semi-structured data like databases, JSON files, etc.</p>
</li>
<li><p><strong>Scalability</strong>: Indexing and searching are highly scalable, optimized for fast retrieval of relevant information.</p>
</li>
<li><p><strong>Customization</strong>: Allows for some customization in scoring profiles, ranking, etc., but generally does not interpret or understand the text.</p>
</li>
<li><p><strong>Text Analysis</strong>: Can perform simple text-based operations like tokenization, stemming, and linguistic analysis, but does not have capabilities like natural language understanding or generation.</p>
</li>
<li><p><strong>Where It Runs</strong>: A fully managed cloud service, but as of my last update, it doesn't offer an on-premises solution.</p>
</li>
<li><p><strong>Security</strong>: Provides features for data encryption and integrates with Azure Active Directory for identity and access management.</p>
</li>
<li><p><strong>Use Cases</strong>: Ideal for creating search experiences in e-commerce websites, content discovery platforms, enterprise search, etc.</p>
</li>
</ol>
<h3>Azure OpenAI Service on Your Data:</h3>
<ol>
<li><p><strong>Purpose</strong>: Designed to run OpenAI models like GPT-3 in your own Azure environment for tasks that require natural language understanding or text generation.</p>
</li>
<li><p><strong>Data Types</strong>: Generally aimed at any form of text data, be it structured or unstructured.</p>
</li>
<li><p><strong>Scalability</strong>: Designed to scale, but depending on the model, might require considerable computational resources.</p>
</li>
<li><p><strong>Customization</strong>: Highly customizable because you are leveraging machine learning models that can be fine-tuned (if permitted by OpenAI) or adapted to specific tasks.</p>
</li>
<li><p><strong>Text Analysis</strong>: Capable of complex operations like natural language understanding, question answering, summarization, translation, and text generation.</p>
</li>
<li><p><strong>Where It Runs</strong>: In your own Azure environment, enabling you to meet strict data privacy and sovereignty requirements.</p>
</li>
<li><p><strong>Security</strong>: Since it runs in your own Azure environment, you have complete control over data security and compliance.</p>
</li>
<li><p><strong>Use Cases</strong>: Ideal for tasks requiring a deep understanding of natural language, like automated customer service, content generation, data extraction, etc.</p>
</li>
</ol>
<h3>Summary:</h3>
<ul>
<li><strong>Azure Cognitive Search</strong> is specialized for building search experiences and works well for retrieving and ranking information based on queries.</li>
<li><strong>Azure OpenAI Service on Your Data</strong> is more generalized and versatile for tasks requiring an understanding or generation of natural language text and allows for on-premises or in-Azure-environment deployments for better data control.</li>
</ul>
<p>Both services can be used complementarily. For example, you could use Azure OpenAI Service on Your Data to interpret a natural language query and then pass a transformed query to Azure Cognitive Search for retrieving the most relevant documents.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <div id="d15f5ef4-6df2-43ba-ad3a-1e845710c027" class="task done">
                <div class="task_contents">&check;
                    <span class="task_contents">Docker について調べる</span>
                    <a href="#d15f5ef4-6df2-43ba-ad3a-1e845710c027" class="permalink">&infin;</a>
                </div>
                <div class="notes">
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>Docker is a platform that enables developers to build, package, and distribute applications as lightweight, portable containers. A container is a standalone executable package that includes everything needed to run a piece of software, including the code, runtime, libraries, and system tools. Docker containers are built from images that can be pulled from Docker Hub or other container registries.</p>
<h3>Key Concepts:</h3>
<ol>
<li><p><strong>Container</strong>: An isolated environment in which an application and its dependencies run. Containers are created from images.</p>
</li>
<li><p><strong>Image</strong>: A snapshot of a container, containing the application and all its dependencies. You can pull images from a registry or create your own.</p>
</li>
<li><p><strong>Dockerfile</strong>: A text document that contains the instructions for building a Docker image.</p>
</li>
<li><p><strong>Docker Hub</strong>: A cloud-based registry where Docker users and partners publish and share containers.</p>
</li>
<li><p><strong>Docker Engine</strong>: The core component that runs containers. It can run on your local machine, or it can be a cloud-based service.</p>
</li>
<li><p><strong>Docker Compose</strong>: A tool for defining and running multi-container applications.</p>
</li>
</ol>
<h3>Basic Commands:</h3>
<ul>
<li><code>docker pull &lt;image&gt;</code>: Pull an image from a registry.</li>
<li><code>docker build -t &lt;image-name&gt; .</code>: Build an image from a Dockerfile in the current directory.</li>
<li><code>docker run &lt;image&gt;</code>: Run a container from an image.</li>
<li><code>docker ps</code>: List running containers.</li>
<li><code>docker stop &lt;container-id&gt;</code>: Stop a running container.</li>
<li><code>docker rm &lt;container-id&gt;</code>: Remove a stopped container.</li>
<li><code>docker rmi &lt;image-id&gt;</code>: Remove an image.</li>
</ul>
<h3>Advantages:</h3>
<ol>
<li><p><strong>Portability</strong>: Applications in Docker containers can run anywhere that Docker is installed, regardless of the underlying infrastructure.</p>
</li>
<li><p><strong>Consistency</strong>: Docker containers ensure that applications run the same way everywhere, which simplifies development, testing, and deployment.</p>
</li>
<li><p><strong>Isolation</strong>: Containers isolate applications from each other and from the host system, improving security.</p>
</li>
<li><p><strong>Scalability</strong>: Docker containers can be easily scaled up or down, making it easier to build distributed, microservices-based applications.</p>
</li>
<li><p><strong>Speed</strong>: Containers are lightweight and start quickly, making it easier to build and deploy applications rapidly.</p>
</li>
</ol>
<h3>Use Cases:</h3>
<ul>
<li><strong>Development and Testing</strong>: Create a consistent environment for building and testing applications.</li>
<li><strong>Microservices Architecture</strong>: Deploy each microservice in its own container for better scalability and maintainability.</li>
<li><strong>Continuous Integration/Continuous Deployment (CI/CD)</strong>: Automate the building, testing, and deployment of applications in containers.</li>
<li><strong>Application Isolation</strong>: Run different versions or instances of an application in isolation from each other.</li>
</ul>
<h3>Integration with Azure:</h3>
<p>Docker can be integrated with Azure services like Azure Container Instances (ACI), Azure Kubernetes Service (AKS), and Azure App Service to manage and deploy containers at scale in the cloud.</p>
<p>Whether you're a developer looking to streamline your workflow or an organization seeking to scale applications with cloud services, Docker offers a range of solutions to make managing containers easier, more efficient, and more secure.</p>
                            </div>
                        </div>
                    </div>
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>Yes, you can use Docker to manage versions of your ASP.NET Core application and ensure that the running copy is the latest version. There are several ways to achieve this, depending on your deployment environment and strategy. Below are some methods to consider:</p>
<h3>Docker Compose</h3>
<p>You can use Docker Compose to manage your application and its services. With Docker Compose, you can define your ASP.NET Core service along with any dependencies in a <code>docker-compose.yml</code> file. To ensure you're running the latest version, you can pull the latest image and restart the service:</p>
<ol>
<li>Pull the latest image: <code>docker-compose pull myservice</code></li>
<li>Restart the service: <code>docker-compose up -d myservice</code></li>
</ol>
<h3>Watchtower</h3>
<p><a href="https://github.com/containrrr/watchtower">Watchtower</a> is a third-party tool that automatically updates running Docker containers to the latest image. You can run Watchtower as a separate container, and it will monitor your running containers and their images. If it finds an outdated image, it will update the container with the latest version.</p>
<p>To run Watchtower:</p>
<pre><code class="language-bash">docker run -d --name watchtower -v /var/run/docker.sock:/var/run/docker.sock containrrr/watchtower
</code></pre>
<p>Note that Watchtower will automatically update all running containers by default. You can limit this behavior to specific containers with additional flags.</p>
<h3>Shell Script</h3>
<p>You can create a shell script that performs the following steps:</p>
<ol>
<li>Pulls the latest image for your ASP.NET Core application.</li>
<li>Compares the image ID of the running container to the latest image.</li>
<li>If the IDs are different, it stops the current container, removes it, and starts a new container using the latest image.</li>
</ol>
<p>Here's a simplified example:</p>
<pre><code class="language-bash"># Pull the latest image
docker pull my-aspnetcore-app:latest

# Get the image ID of the running container
CURRENT_IMAGE_ID=$(docker inspect my-aspnetcore-container --format '{{.Image}}')

# Get the image ID of the latest pulled image
LATEST_IMAGE_ID=$(docker images --filter=reference='my-aspnetcore-app:latest' --format '{{.ID}}')

# Compare IDs and update if needed
if [ &quot;$CURRENT_IMAGE_ID&quot; != &quot;$LATEST_IMAGE_ID&quot; ]; then
  docker stop my-aspnetcore-container
  docker rm my-aspnetcore-container
  docker run --name my-aspnetcore-container -d my-aspnetcore-app:latest
fi
</code></pre>
<p>This script can be scheduled to run at intervals using cron jobs (Linux) or Task Scheduler (Windows).</p>
<h3>Azure DevOps or GitHub Actions (CI/CD)</h3>
<p>If your application is hosted in a cloud environment like Azure, you can also set up a CI/CD pipeline to build, test, and deploy the latest version of your ASP.NET Core app automatically. Azure DevOps or GitHub Actions can be configured to update the Docker container whenever changes are pushed to your repository.</p>
<p>These are just some of the ways to ensure you're running the latest version of your ASP.NET Core application in a Docker container. Your choice may vary based on your specific requirements and deployment environment.</p>
                            </div>
                        </div>
                    </div>
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>Yes, Docker does support Blazor applications. You can containerize both Blazor Server and Blazor WebAssembly applications using Docker. Below are some general steps on how to do so for each type.</p>
<h3>For Blazor Server Applications</h3>
<ol>
<li><strong>Create a Dockerfile</strong>: In the root directory of your Blazor Server application, create a file named <code>Dockerfile</code> with the following content:</li>
</ol>
<pre><code class="language-Dockerfile"># Use the SDK image to build the app
FROM mcr.microsoft.com/dotnet/aspnet:5.0 AS base
WORKDIR /app
EXPOSE 80

FROM mcr.microsoft.com/dotnet/sdk:5.0 AS build
WORKDIR /src
COPY [&quot;MyBlazorApp/MyBlazorApp.csproj&quot;, &quot;MyBlazorApp/&quot;]
RUN dotnet restore &quot;MyBlazorApp/MyBlazorApp.csproj&quot;
COPY . .
WORKDIR &quot;/src/MyBlazorApp&quot;
RUN dotnet build &quot;MyBlazorApp.csproj&quot; -c Release -o /app/build

FROM build AS publish
RUN dotnet publish &quot;MyBlazorApp.csproj&quot; -c Release -o /app/publish

# Copy the build app to the base image and define entrypoint
FROM base AS final
WORKDIR /app
COPY --from=publish /app/publish .
ENTRYPOINT [&quot;dotnet&quot;, &quot;MyBlazorApp.dll&quot;]
</code></pre>
<p>Replace <code>MyBlazorApp</code> with the name of your Blazor Server application project.</p>
<ol start="2">
<li><strong>Build the Docker Image</strong>: In your terminal, navigate to the root directory where your <code>Dockerfile</code> is located and run:</li>
</ol>
<pre><code class="language-bash">docker build -t my-blazor-server-app .
</code></pre>
<ol start="3">
<li><strong>Run the Docker Container</strong>: Once the image is built, you can run it:</li>
</ol>
<pre><code class="language-bash">docker run -p 8080:80 my-blazor-server-app
</code></pre>
<p>Open your web browser and navigate to <code>http://localhost:8080</code> to access your Blazor Server app running in a Docker container.</p>
<h3>For Blazor WebAssembly Applications</h3>
<p>For Blazor WebAssembly, the hosting model is different, as the application runs directly in the browser. Still, you can use Docker to serve the application files using a web server.</p>
<p>Here's an example using an Nginx image to serve a Blazor WebAssembly app:</p>
<ol>
<li><strong>Create a Dockerfile</strong>: In the root directory of your Blazor WebAssembly application, create a <code>Dockerfile</code> with the following content:</li>
</ol>
<pre><code class="language-Dockerfile"># Build the app
FROM mcr.microsoft.com/dotnet/sdk:5.0 AS build
WORKDIR /src
COPY [&quot;MyBlazorWasmApp/MyBlazorWasmApp.csproj&quot;, &quot;MyBlazorWasmApp/&quot;]
RUN dotnet restore &quot;MyBlazorWasmApp/MyBlazorWasmApp.csproj&quot;
COPY . .
WORKDIR &quot;/src/MyBlazorWasmApp&quot;
RUN dotnet publish &quot;MyBlazorWasmApp.csproj&quot; -c Release -o /app/publish

# Use Nginx to serve the app
FROM nginx:alpine AS final
WORKDIR /var/www/web
COPY --from=build /app/publish/wwwroot .
COPY nginx.conf /etc/nginx/nginx.conf
</code></pre>
<p>Replace <code>MyBlazorWasmApp</code> with the name of your Blazor WebAssembly application project. Create an <code>nginx.conf</code> file with suitable settings for serving a Blazor WebAssembly application.</p>
<ol start="2">
<li><p><strong>Build the Docker Image</strong>: Build your Docker image as you would for the Blazor Server.</p>
</li>
<li><p><strong>Run the Docker Container</strong>: Run your Docker container as you would for the Blazor Server.</p>
</li>
</ol>
<p>Both Blazor Server and Blazor WebAssembly applications can be effectively containerized using Docker, making it easier to deploy, scale, and manage them.</p>
                            </div>
                        </div>
                    </div>
                    <div class="note">
                        <div class="note_contents">
                            <p class="note_contents">やってみないと分からないが、概要は理解したと思う。</p>
                        </div>
                    </div>
                </div>
            </div>
            <div id="0b3dd2c5-8a02-4bd3-b1de-320a1d14eb15" class="task done">
                <div class="task_contents">&check;
                    <span class="task_contents">Watchtower について調べる</span>
                    <a href="#0b3dd2c5-8a02-4bd3-b1de-320a1d14eb15" class="permalink">&infin;</a>
                </div>
                <div class="notes">
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>Watchtower is an open-source tool that automatically updates running Docker containers with any new images that are available. It periodically checks a registry for updated images and replaces the running container with the new one, preserving the options that were used when initially running the container.</p>
<h3>Key Features:</h3>
<ol>
<li><p><strong>Automatic Updates</strong>: Watchtower periodically checks for updates to Docker images and replaces running containers with the updated version if available.</p>
</li>
<li><p><strong>Notifications</strong>: You can configure Watchtower to send notifications via email, Slack, and other platforms when it updates a container.</p>
</li>
<li><p><strong>Scheduling</strong>: Watchtower supports cron-like scheduling for updates so that you can specify when you'd like your containers to be updated.</p>
</li>
<li><p><strong>Selective Monitoring</strong>: By default, Watchtower will update all running containers. However, you can specify which containers should be updated automatically.</p>
</li>
<li><p><strong>Rolling Updates</strong>: Watchtower can be configured to perform rolling updates to minimize downtime during an upgrade.</p>
</li>
</ol>
<h3>Basic Usage:</h3>
<p>To run Watchtower, you can execute the following Docker command:</p>
<pre><code class="language-bash">docker run -d --name watchtower -v /var/run/docker.sock:/var/run/docker.sock containrrr/watchtower
</code></pre>
<p>This command will start a new container named &quot;watchtower&quot; that has access to the Docker socket, enabling it to interact with Docker on your host.</p>
<h3>More Advanced Options:</h3>
<ul>
<li><p><strong>Check Interval</strong>: You can specify an interval (in seconds) for Watchtower to check for updates. For example, for a 1-hour interval:</p>
<pre><code class="language-bash">docker run -d --name watchtower -v /var/run/docker.sock:/var/run/docker.sock containrrr/watchtower --interval 3600
</code></pre>
</li>
<li><p><strong>Specifying Containers</strong>: To monitor specific containers, you can list their names at the end of the Watchtower command:</p>
<pre><code class="language-bash">docker run -d --name watchtower -v /var/run/docker.sock:/var/run/docker.sock containrrr/watchtower container1 container2
</code></pre>
</li>
<li><p><strong>Notifications</strong>: You can set up notifications to, for example, a Slack channel by setting environment variables when running the Watchtower container:</p>
<pre><code class="language-bash">docker run -d --name watchtower -v /var/run/docker.sock:/var/run/docker.sock -e WATCHTOWER_NOTIFICATIONS=slack -e WATCHTOWER_NOTIFICATION_SLACK_HOOK_URL=https://hooks.slack.com/your-webhook-url containrrr/watchtower
</code></pre>
</li>
</ul>
<h3>Considerations:</h3>
<ol>
<li><p><strong>Downtime</strong>: Depending on how you configure Watchtower, there might be a brief period of downtime for your application when Watchtower pulls the new image and restarts the container.</p>
</li>
<li><p><strong>Breaking Changes</strong>: Automatic updates might introduce breaking changes if the updated image is not fully compatible with the previous version.</p>
</li>
<li><p><strong>Custom Configuration</strong>: Ensure that any custom configurations or data are persisted outside the container, as Watchtower replaces containers.</p>
</li>
<li><p><strong>Security</strong>: Granting a container access to the Docker daemon (as shown in the basic usage example) can be a security risk if misconfigured. Make sure you understand the implications of doing this in your environment.</p>
</li>
<li><p><strong>Database Migrations</strong>: Watchtower is not designed to handle complex workflows like database migrations. For such cases, manual intervention or a more advanced deployment strategy is recommended.</p>
</li>
</ol>
<p>Watchtower can be a handy tool for small to medium-sized deployments where the benefits of automatic updates outweigh the risks and challenges mentioned above. For larger or more complex systems, a more sophisticated update mechanism might be appropriate.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <div id="a1183d61-1f79-41e8-837b-92be78926d17" class="task done">
                <div class="task_contents">&check;
                    <span class="task_contents">Azure DevOps について調べる</span>
                    <a href="#a1183d61-1f79-41e8-837b-92be78926d17" class="permalink">&infin;</a>
                </div>
                <div class="notes">
                    <div class="note">
                        <div class="note_contents">
                            <p class="note_contents">便利そう。ハンズオンのときに復習する。</p>
                            <div class="note_ai_generated">
<p>Azure DevOps is a set of cloud-based collaboration tools provided by Microsoft that facilitate software development and delivery. It offers various features to support DevOps practices, such as planning, tracking, code version control, automated builds, testing, and releases.</p>
<h3>Core Features:</h3>
<ol>
<li><p><strong>Azure Boards</strong>: A work tracking system that uses Kanban boards, backlogs, custom reporting, and dashboards to visualize work and promote collaboration among team members.</p>
</li>
<li><p><strong>Azure Repos</strong>: Provides Git-based source code repositories, pull requests, and code reviews to facilitate version control and code sharing.</p>
</li>
<li><p><strong>Azure Pipelines</strong>: A CI/CD (Continuous Integration/Continuous Deployment) tool that automates the building, testing, and deployment of applications to different platforms like Windows, Linux, macOS, and containers like Docker.</p>
</li>
<li><p><strong>Azure Test Plans</strong>: Allows you to create, manage, and run tests in order to ensure the quality of your application.</p>
</li>
<li><p><strong>Azure Artifacts</strong>: Provides package management for Maven, npm, NuGet, and Python packages. You can share code across your organization.</p>
</li>
</ol>
<h3>How to Use Azure DevOps with Docker</h3>
<p>Azure DevOps can be integrated with Docker to automate building, testing, and deploying containerized applications.</p>
<ol>
<li><p><strong>Build Docker Images</strong>: Azure Pipelines can be configured to automatically build Docker images whenever there is a code change.</p>
</li>
<li><p><strong>Push to Registry</strong>: After building the Docker image, you can push it to a Docker registry like Docker Hub, Azure Container Registry, or any other registry of your choice.</p>
</li>
<li><p><strong>Run Tests</strong>: Use Azure Test Plans to run automated tests against your Docker containers to ensure they meet your quality criteria.</p>
</li>
<li><p><strong>Deploy Containers</strong>: Once the image is built and tests pass, Azure Pipelines can deploy the Docker container to various platforms including Azure Kubernetes Service (AKS), Azure Container Instances (ACI), or any other container orchestration service.</p>
</li>
</ol>
<h3>Advantages:</h3>
<ol>
<li><p><strong>Integrated Environment</strong>: Azure DevOps provides a one-stop-shop for planning, coding, testing, and deploying applications, reducing the need for disparate, standalone tools.</p>
</li>
<li><p><strong>Extensibility</strong>: Azure DevOps can be extended with a wide range of plugins and extensions from the Azure DevOps Marketplace to customize your workflow.</p>
</li>
<li><p><strong>Scalability</strong>: Easily scalable to accommodate both small and large projects.</p>
</li>
<li><p><strong>Security</strong>: Azure DevOps offers features like access control, audit trails, and secure connections to ensure your code and data are secure.</p>
</li>
<li><p><strong>Cross-Platform and Language Agnostic</strong>: Azure DevOps can be used for projects written in any language and deployed to any platform.</p>
</li>
</ol>
<p>Whether you are a developer working on a small project or an organization with complex delivery pipelines, Azure DevOps offers a comprehensive suite of tools that can cater to your needs.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <div id="f9a7dcfb-5e80-466d-b241-50b3633f982f" class="task done">
                <div class="task_contents">&check;
                    <span class="task_contents">GitHub Actions について調べる</span>
                    <a href="#f9a7dcfb-5e80-466d-b241-50b3633f982f" class="permalink">&infin;</a>
                </div>
                <div class="notes">
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>GitHub Actions is a CI/CD (Continuous Integration/Continuous Deployment) and automation service provided by GitHub. It allows you to define workflows that can build, test, package, release, and deploy your code directly from your GitHub repository. GitHub Actions can be used with any programming language and integrate with any platform, including Windows, macOS, and Linux.</p>
<h3>Core Features:</h3>
<ol>
<li><p><strong>Event-Driven</strong>: GitHub Actions can be triggered by a wide range of events on the GitHub platform, such as pushing code to a repository, creating pull requests, releasing new versions, and more.</p>
</li>
<li><p><strong>Workflow Configuration</strong>: Workflows are defined using YAML files that live within your repository. This makes it easy to version-control and collaborate on your CI/CD pipelines.</p>
</li>
<li><p><strong>Matrix Builds</strong>: You can run your build and test jobs on multiple versions of a language or various OS types simultaneously.</p>
</li>
<li><p><strong>Hosted Runners</strong>: GitHub provides hosted runners for multiple operating systems, including Ubuntu, Windows, and macOS. You can also host your own runners if you need specialized hardware or software.</p>
</li>
<li><p><strong>Extensible</strong>: GitHub Actions can be easily extended with actions from the GitHub Marketplace, where you can find a wide variety of pre-built actions created by the community and vendors.</p>
</li>
<li><p><strong>Environment Variables and Secrets</strong>: You can define environment variables and secrets that are available to your workflow, making it easy to manage configuration and sensitive information.</p>
</li>
</ol>
<h3>How to Use GitHub Actions with Docker</h3>
<p>GitHub Actions can be used to build, test, and deploy Docker containers as part of your CI/CD workflow. Here's a simple example of a workflow YAML file that builds and pushes a Docker image:</p>
<pre><code class="language-yaml">name: Build and Push Docker Image

on:
  push:
    branches: [ main ]

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v2

    - name: Login to Docker Hub
      uses: docker/login-action@v1
      with:
        username: ${{ secrets.DOCKER_HUB_USERNAME }}
        password: ${{ secrets.DOCKER_HUB_ACCESS_TOKEN }}

    - name: Build and push Docker image
      uses: docker/build-push-action@v2
      with:
        context: .
        push: true
        tags: username/repository:tag
</code></pre>
<p>In this example:</p>
<ul>
<li>The workflow runs whenever code is pushed to the <code>main</code> branch.</li>
<li>It checks out the code from the repository.</li>
<li>It logs in to Docker Hub using credentials stored as GitHub secrets.</li>
<li>It builds the Docker image and pushes it to a Docker Hub repository.</li>
</ul>
<h3>Advantages:</h3>
<ol>
<li><p><strong>Integrated with GitHub</strong>: Being built into GitHub means you can set up CI/CD without leaving the platform, making it convenient to manage everything related to your code.</p>
</li>
<li><p><strong>Flexible and Customizable</strong>: You can define complex workflows using YAML, and you can run virtually any script or command as part of your workflow.</p>
</li>
<li><p><strong>Rich Ecosystem</strong>: The GitHub Marketplace offers a large selection of pre-built actions that can do everything from deploying code to AWS, Azure, or other cloud providers, to sending SMS notifications.</p>
</li>
<li><p><strong>Free Tier Available</strong>: For public repositories, GitHub Actions is free. For private repositories, a number of free minutes are available, and additional minutes are affordably priced.</p>
</li>
</ol>
<p>GitHub Actions provides a powerful, flexible way to automate all your software workflows, from CI/CD to issue triaging and beyond, directly within the GitHub platform.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <div id="55e84fc0-5f83-4e46-9bc2-0e538827f2ba" class="task done">
                <div class="task_contents">&check;
                    <span class="task_contents">一つのアカウントで複数のサービスにアクセスできる仕組みについて調べる</span>
                    <a href="#55e84fc0-5f83-4e46-9bc2-0e538827f2ba" class="permalink">&infin;</a>
                </div>
                <div class="notes">
                    <div class="note">
                        <div class="note_contents">
                            <p class="note_contents">Google や Microsoft のアカウントでそういうことができる。いずれも OAuth なのか、実装上の違いはないのか、どのくらいのことができるのかといったことを調べる。</p>
                        </div>
                    </div>
                    <div class="note">
                        <div class="note_contents">
                            <p class="note_contents">たとえば、アクセス権を階層的に定義する場合の実装はどうなるか。</p>
                            <p class="note_contents">Google アカウントなどでログインしてもらい、最初は一番下のアクセス権を与え、システム管理者が気づき次第、手作業でアクセス権を高める？ それだとユーザーが多いときの管理コストが大きい。</p>
                        </div>
                    </div>
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>Implementing a single authentication mechanism that works across multiple web systems is often achieved through a single sign-on (SSO) system. SSO allows a user to log in once and gain access to multiple services without being required to log in again for each one. Here are some strategies and steps to implement such a mechanism:</p>
<h3>Strategies for SSO</h3>
<ol>
<li><p><strong>OAuth 2.0/OpenID Connect</strong>: A well-known standard for authorization that allows you to use a single identity provider (like Google, Facebook, or a custom identity provider) to handle authentication for multiple services.</p>
</li>
<li><p><strong>SAML</strong>: Security Assertion Markup Language is an XML-based standard for exchanging authentication and authorization data between parties, particularly between an identity provider and a service provider.</p>
</li>
<li><p><strong>JWT (JSON Web Tokens)</strong>: A compact, URL-safe means of representing claims between two parties. JWTs can be used in SSO implementations.</p>
</li>
<li><p><strong>LDAP</strong>: Lightweight Directory Access Protocol can be used in enterprise settings for centralized authentication.</p>
</li>
<li><p><strong>Kerberos</strong>: A network authentication protocol designed to provide strong authentication for users and services.</p>
</li>
<li><p><strong>Custom SSO</strong>: You can also build a custom SSO system that fits the specific needs of your applications. However, adhering to a standard is often safer and more scalable.</p>
</li>
</ol>
<h3>General Steps to Implement SSO</h3>
<ol>
<li><p><strong>Choose an Identity Provider (IdP)</strong>: This is the service that will authenticate user credentials and send the information to the service providers (the individual web systems).</p>
</li>
<li><p><strong>Configure Service Providers (SPs)</strong>: Each web system must be configured to accept the SSO tokens generated by the identity provider.</p>
</li>
<li><p><strong>User Authentication</strong>: The IdP needs to verify the user's identity, typically through a username and password, although multi-factor authentication can also be used.</p>
</li>
<li><p><strong>Token Generation</strong>: After successful authentication, the IdP generates a secure token that can be understood and validated by the service providers.</p>
</li>
<li><p><strong>Token Validation</strong>: When a user tries to access a service, the service provider requests a token from the identity provider. The service provider validates this token and, if valid, grants access to the user.</p>
</li>
<li><p><strong>User Session</strong>: The user’s session can be managed either centrally by the IdP or locally by each service provider, with the latter periodically checking the validity of the SSO token.</p>
</li>
<li><p><strong>Security Measures</strong>: Always use HTTPS and consider other security measures like token encryption, periodic key rotation, and logout mechanisms.</p>
</li>
</ol>
<h3>Coding Aspects</h3>
<ul>
<li><p>Each web system (service provider) needs to include a library or SDK that can communicate with the chosen identity provider. Many SSO systems offer SDKs for popular languages and frameworks.</p>
</li>
<li><p>The identity provider often has its own setup code, whether it's a dedicated server or a cloud service.</p>
</li>
</ul>
<h3>Example Libraries and Services</h3>
<ul>
<li>Microsoft's Azure Active Directory</li>
<li>Google Identity Platform</li>
<li>Auth0</li>
<li>Okta</li>
<li>Open-source libraries like Spring Security for Java</li>
</ul>
<p>Implementing SSO is non-trivial but offers a much better user experience and typically provides better security than managing multiple separate login systems. The choice of technology and approach will depend on your specific requirements, including the programming languages you are using, the level of security you need, and whether you are building for an enterprise or consumer audience.</p>
                            </div>
                        </div>
                    </div>
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>Services that implement OAuth specifications are generally categorized into two roles:</p>
<ol>
<li><p><strong>OAuth Providers (Identity Providers)</strong>: These are services that authenticate users and issue OAuth tokens. Users have accounts with these services, and they authenticate against them during the OAuth flow. Examples include:</p>
<ul>
<li><p><strong>Google Sign-In</strong>: Google's identity platform that uses OAuth 2.0 and OpenID Connect.</p>
</li>
<li><p><strong>Facebook Login</strong>: Facebook's authentication system also utilizes OAuth 2.0.</p>
</li>
<li><p><strong>GitHub</strong>: Primarily for developer-focused services, GitHub offers OAuth 2.0 based authentication.</p>
</li>
<li><p><strong>Twitter</strong>: Offers OAuth 1.0a and OAuth 2.0 for different types of applications.</p>
</li>
<li><p><strong>Microsoft Identity Platform</strong>: Provides OAuth 2.0 authentication, previously known as Azure Active Directory (Azure AD) v2.0.</p>
</li>
<li><p><strong>Apple Sign In</strong>: Apple's own OAuth 2.0 based single sign-on service.</p>
</li>
<li><p><strong>LinkedIn</strong>: Provides OAuth 2.0 based authentication for LinkedIn profiles.</p>
</li>
<li><p><strong>Spotify</strong>: Uses OAuth 2.0 for authorizing third-party applications.</p>
</li>
<li><p><strong>Dropbox</strong>: Uses OAuth 2.0 for their API.</p>
</li>
<li><p><strong>Reddit</strong>: Offers OAuth 2.0 based authentication for its API.</p>
</li>
<li><p><strong>Salesforce</strong>: Utilizes OAuth 2.0 for their various cloud services.</p>
</li>
<li><p><strong>PayPal</strong>: For API permissions and authentication, PayPal also uses OAuth 2.0.</p>
</li>
<li><p><strong>Okta</strong>: An identity management service that also provides OAuth 2.0 and OpenID Connect services.</p>
</li>
<li><p><strong>Auth0</strong>: Offers both OAuth 2.0 and OpenID Connect services.</p>
</li>
</ul>
</li>
<li><p><strong>OAuth Consumers (Service Providers)</strong>: These are services that allow users to authenticate via an OAuth provider, generally to gain some access to data or services without needing a separate username and password for each site. Examples include:</p>
<ul>
<li><p><strong>Third-party Mail Clients</strong>: Like Thunderbird or Microsoft Outlook may use OAuth for Gmail access.</p>
</li>
<li><p><strong>Social Media Management Tools</strong>: Like Buffer or Hootsuite that require access to social media accounts.</p>
</li>
<li><p><strong>Collaboration Tools</strong>: Such as Slack, which can integrate with Google Drive or GitHub via OAuth.</p>
</li>
<li><p><strong>Online Stores</strong>: That allow you to create an account and log in using credentials from Google, Facebook, etc.</p>
</li>
<li><p><strong>Cloud-based Software Solutions</strong>: Such as CRM or ERP systems that integrate with other platforms via OAuth.</p>
</li>
<li><p><strong>Developer Tools</strong>: Like IDEs or CI/CD platforms that integrate with GitHub or GitLab using OAuth.</p>
</li>
</ul>
</li>
</ol>
<p>Both of these categories have become integral to modern web architecture, and OAuth is widely adopted as the standard for secure delegated access.</p>
                            </div>
                        </div>
                    </div>
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>Hierarchical authentication using OAuth refers to the implementation of an OAuth-based authentication system where access control is structured in a hierarchical manner. This concept isn't native to OAuth itself, but OAuth can be combined with other authorization mechanisms to achieve a hierarchical structure.</p>
<p>In the context of OAuth, &quot;hierarchical&quot; typically pertains to the scope or granularity of access rights. Instead of just &quot;read&quot; or &quot;write&quot; permissions, you might have several nested levels of permissions.</p>
<h3>How Hierarchical Authentication Using OAuth Might Look:</h3>
<ol>
<li><p><strong>Top-Level (Admin Rights)</strong>:</p>
<ul>
<li>Full control over an application, its data, and its settings.</li>
<li>Can add or remove other administrators or manage lower-level users.</li>
</ul>
</li>
<li><p><strong>Mid-Level (Managerial Rights)</strong>:</p>
<ul>
<li>Can perform most actions but might not be allowed to alter fundamental settings.</li>
<li>Can manage users within their purview, but not other managers or admins.</li>
</ul>
</li>
<li><p><strong>Entry-Level (User Rights)</strong>:</p>
<ul>
<li>Access to use the application and its features.</li>
<li>No control over settings or other users.</li>
</ul>
</li>
<li><p><strong>Guest-Level</strong>:</p>
<ul>
<li>Very limited, read-only access.</li>
</ul>
</li>
</ol>
<h3>Implementation Steps:</h3>
<ol>
<li><p><strong>Define Scopes</strong>: In OAuth, the permissions granted to a token are defined by its &quot;scopes&quot;. Scopes can be used to represent hierarchical levels of access. For instance, scopes can be defined as <code>admin</code>, <code>manager</code>, <code>user</code>, and <code>guest</code>.</p>
</li>
<li><p><strong>Token Request</strong>: When an application requests an OAuth token, it specifies the required scopes. The OAuth provider then checks whether the user has the necessary permissions to grant those scopes.</p>
</li>
<li><p><strong>Token Inspection</strong>: When an OAuth token is presented to a service, the service can inspect the token's scopes to determine the user's level of access.</p>
</li>
<li><p><strong>Fine-Grained Authorization</strong>: Beyond just inspecting the token's scopes, the service might combine the OAuth token with its own authorization logic. For instance, a user might have a <code>manager</code> scope in their token, but an internal database dictates which specific users or departments they can manage.</p>
</li>
<li><p><strong>Revocation</strong>: Hierarchical structures often come with the need to revoke or downgrade permissions. Ensure you have mechanisms in place for this. If an admin removes managerial rights from a user, you'll need to invalidate any tokens that were issued with the <code>manager</code> scope for that user.</p>
</li>
<li><p><strong>Refresh Tokens</strong>: Utilize refresh tokens to allow the client to fetch a new access token when the previous one expires. This way, even if a token's permissions are long-lived, you can re-check and re-assert the user's permissions at regular intervals.</p>
</li>
</ol>
<h3>Challenges:</h3>
<ul>
<li><p><strong>Complexity</strong>: Hierarchical authentication structures can be complex to design, implement, and maintain. Ensuring that permissions are always correctly assigned and enforced can be challenging.</p>
</li>
<li><p><strong>User Experience</strong>: From a user's perspective, it might be confusing if they are suddenly denied access to a resource without understanding the hierarchical structure or any changes made to their level in the hierarchy.</p>
</li>
<li><p><strong>Scalability</strong>: As more hierarchical levels and nuances are introduced, managing and validating these permissions at scale can become a concern.</p>
</li>
</ul>
<h3>Conclusion:</h3>
<p>OAuth itself doesn't inherently support or reject hierarchical authentication. Instead, OAuth provides mechanisms (like scopes) that can be utilized to build hierarchical (or any other structured) authentication and authorization systems. When designing such a system, be clear about the requirements, maintain a robust mechanism for token validation and revocation, and ensure that the user experience remains intuitive.</p>
                            </div>
                        </div>
                    </div>
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>OAuth scopes are a mechanism in OAuth 2.0 and OpenID Connect that allows you to specify the access level that you are requesting from the user. In essence, they define the permissions that are granted to the OAuth access token. By using scopes, the application can request only the permissions it needs to accomplish the task at hand, which is good for both security and user experience.</p>
<h3>Common Scopes in OAuth 2.0</h3>
<ol>
<li><p><strong>Read/Write Access</strong>: For example, an application that interacts with a user's Google Drive may request read-only (<code>https://www.googleapis.com/auth/drive.readonly</code>) or read/write (<code>https://www.googleapis.com/auth/drive</code>) access.</p>
</li>
<li><p><strong>Profile Information</strong>: Some OAuth providers offer a <code>profile</code> scope to access the basic profile information of a user.</p>
</li>
<li><p><strong>Email</strong>: An <code>email</code> scope could grant access to the user's email address.</p>
</li>
<li><p><strong>OpenID</strong>: In OpenID Connect, the <code>openid</code> scope is used to request an ID token.</p>
</li>
<li><p><strong>Offline Access</strong>: The <code>offline_access</code> scope can be used to request a refresh token that allows the application to maintain access when the user is not present.</p>
</li>
<li><p><strong>Custom Scopes</strong>: You're not limited to predefined scopes; many services allow you to define your own custom scopes.</p>
</li>
</ol>
<h3>Using Scopes in OAuth 2.0 Flow</h3>
<ol>
<li><p><strong>Requesting Scopes</strong>: When directing the user to the authorization endpoint, the application includes the scopes it needs in the <code>scope</code> query parameter, often space-delimited.</p>
<ul>
<li>Example: <code>https://auth.example.com/authorize?response_type=code&amp;client_id=CLIENT_ID&amp;redirect_uri=REDIRECT_URI&amp;scope=read write</code></li>
</ul>
</li>
<li><p><strong>User Consent</strong>: The OAuth service will typically show the user a consent screen that lists the scopes the application is requesting. The user can then allow or deny the request.</p>
</li>
<li><p><strong>Access Token</strong>: If the user consents, the access token that is issued will be associated with the scopes that were approved.</p>
</li>
<li><p><strong>Token Inspection</strong>: The service that receives the access token can check its associated scopes to see what access should be granted.</p>
</li>
</ol>
<h3>Security Implications</h3>
<ol>
<li><p><strong>Least Privilege</strong>: By requesting only the scopes your application needs, you adhere to the principle of least privilege, reducing the potential damage from a compromised access token.</p>
</li>
<li><p><strong>User Trust</strong>: Being transparent about the permissions you're requesting can help gain user trust.</p>
</li>
<li><p><strong>Revocable</strong>: Users can generally revoke access tokens, either by revoking individual scopes or by revoking access to the entire application.</p>
</li>
</ol>
<h3>Scope Validation</h3>
<p>When an access token is presented to a service, it's crucial to validate that the token has the required scopes for the requested operation. This is generally done on the server that receives the token, either by introspecting the token or by examining self-contained claims in a JWT (JSON Web Token).</p>
<h3>Examples:</h3>
<ul>
<li><p>GitHub OAuth Scopes: GitHub allows granular scopes like <code>read:user</code>, <code>user:email</code>, <code>repo</code>, etc.</p>
</li>
<li><p>Google OAuth Scopes: Google's API offers various scopes for accessing different services like Google Drive, Google Calendar, Google Photos, etc.</p>
</li>
</ul>
<h3>Summary</h3>
<p>OAuth scopes offer a flexible way to request and grant varying levels of access. This provides a good balance between functionality and security, allowing users to maintain control over their data while enabling powerful integrations by third-party applications.</p>
                            </div>
                        </div>
                    </div>
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>Here's a simple example to demonstrate OAuth 2.0 authentication using Razor Pages in an ASP.NET Core application. In this sample, I'll use Google as the OAuth provider. This is just for demonstration and educational purposes; you should always consult official documentation and tailor the example to your needs.</p>
<h3>Prerequisites</h3>
<ul>
<li>Install the ASP.NET Core SDK from <a href="https://dotnet.microsoft.com/download">here</a>.</li>
<li>Create an OAuth 2.0 client ID from Google Cloud Console.</li>
</ul>
<h3>Steps</h3>
<ol>
<li><p><strong>Create a new Razor Pages project</strong></p>
<pre><code class="language-bash">dotnet new webapp -o RazorPagesOAuthExample
</code></pre>
<p>Navigate to the newly created folder:</p>
<pre><code class="language-bash">cd RazorPagesOAuthExample
</code></pre>
</li>
<li><p><strong>Edit <code>Startup.cs</code></strong></p>
<p>Open the <code>Startup.cs</code> file and add the following in the <code>ConfigureServices</code> method to configure the OAuth 2.0 authentication:</p>
<pre><code class="language-csharp">public void ConfigureServices(IServiceCollection services)
{
    services.AddAuthentication(options =&gt;
    {
        options.DefaultScheme = CookieAuthenticationDefaults.AuthenticationScheme;
    })
    .AddCookie(options =&gt; { options.LoginPath = &quot;/Login&quot;; })
    .AddGoogle(options =&gt;
    {
        options.ClientId = &quot;YOUR_GOOGLE_CLIENT_ID&quot;;
        options.ClientSecret = &quot;YOUR_GOOGLE_CLIENT_SECRET&quot;;
    });

    services.AddRazorPages();
}
</code></pre>
<p>And also, add these lines in the <code>Configure</code> method:</p>
<pre><code class="language-csharp">public void Configure(IApplicationBuilder app, IWebHostEnvironment env)
{
    // ... existing code ...

    app.UseAuthentication();
    app.UseAuthorization();

    // ... existing code ...
}
</code></pre>
<p>Don't forget to add necessary <code>using</code> directives:</p>
<pre><code class="language-csharp">using Microsoft.AspNetCore.Authentication.Cookies;
using Microsoft.AspNetCore.Authentication.Google;
</code></pre>
</li>
<li><p><strong>Add Login and Logout functionality</strong></p>
<p>Create a new Razor Page named &quot;Login&quot; using the following command:</p>
<pre><code class="language-bash">dotnet new page --name Login --namespace RazorPagesOAuthExample.Pages
</code></pre>
<p>Replace <code>Login.cshtml.cs</code> content with:</p>
<pre><code class="language-csharp">using System.Threading.Tasks;
using Microsoft.AspNetCore.Authentication;
using Microsoft.AspNetCore.Mvc;
using Microsoft.AspNetCore.Mvc.RazorPages;

namespace RazorPagesOAuthExample.Pages
{
    public class LoginModel : PageModel
    {
        public IActionResult OnGet()
        {
            return Challenge(new AuthenticationProperties { RedirectUri = &quot;/&quot; }, &quot;Google&quot;);
        }
    }
}
</code></pre>
<p>Create a <code>Logout.cshtml</code> file with a code-behind <code>Logout.cshtml.cs</code>:</p>
<pre><code class="language-csharp">using Microsoft.AspNetCore.Authentication;
using Microsoft.AspNetCore.Authentication.Cookies;
using Microsoft.AspNetCore.Mvc;
using Microsoft.AspNetCore.Mvc.RazorPages;

namespace RazorPagesOAuthExample.Pages
{
    public class LogoutModel : PageModel
    {
        public async Task&lt;IActionResult&gt; OnGetAsync()
        {
            await HttpContext.SignOutAsync(CookieAuthenticationDefaults.AuthenticationScheme);
            return RedirectToPage(&quot;/Index&quot;);
        }
    }
}
</code></pre>
</li>
<li><p><strong>Run Your App</strong></p>
<p>Finally, run your app:</p>
<pre><code class="language-bash">dotnet run
</code></pre>
</li>
</ol>
<p>Now, when you navigate to <code>/Login</code>, you should be redirected to Google's OAuth 2.0 login page. After successful authentication, you should be redirected back to the home page. To log out, you can navigate to <code>/Logout</code>.</p>
<p>Note: Replace <code>YOUR_GOOGLE_CLIENT_ID</code> and <code>YOUR_GOOGLE_CLIENT_SECRET</code> with the credentials you got from the Google Cloud Console.</p>
<p>This is a simplified example to get you started. There's much more you can do, including handling user information, dealing with tokens for API calls, integrating with other OAuth providers, and so on.</p>
                            </div>
                        </div>
                    </div>
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>Yes, you can attach additional information to the authentication data for sharing across multiple systems. However, there are different approaches to achieving this depending on your specific use case and architectural considerations.</p>
<h3>JWT Claims (Token-based approach)</h3>
<p>If you are using a JWT (JSON Web Token) as your authentication mechanism, you can include additional custom claims in the payload section of the token. Custom claims can hold arbitrary information like user settings or display language. All systems that validate the JWT will then have access to this additional information.</p>
<p>For example:</p>
<pre><code class="language-json">{
  &quot;sub&quot;: &quot;123&quot;,
  &quot;name&quot;: &quot;John Doe&quot;,
  &quot;displayLanguage&quot;: &quot;en-US&quot;,
  &quot;userSettings&quot;: {
    &quot;theme&quot;: &quot;dark&quot;
  }
}
</code></pre>
<h3>Shared Database or Cache</h3>
<p>Another approach is to use a shared database or caching layer that can be accessed by all the systems that need the additional information. In this case, the authentication token could include a unique identifier for the user. This identifier can be used to look up additional information stored in the shared database.</p>
<h3>Identity Provider (OAuth, OpenID Connect)</h3>
<p>If you are using an Identity Provider that supports OAuth 2.0 or OpenID Connect, you can usually include additional claims when requesting tokens. The provider can then include this additional information either in the token itself or in the accompanying user information payload.</p>
<h3>.NET Core Identity Claims</h3>
<p>In ASP.NET Core Identity, you can use claims to add additional user information. Once a user is authenticated, you can add extra claims to the <code>ClaimsIdentity</code> object.</p>
<p>Example:</p>
<pre><code class="language-csharp">var identity = new ClaimsIdentity(CookieAuthenticationDefaults.AuthenticationScheme);
identity.AddClaim(new Claim(ClaimTypes.Name, username));
identity.AddClaim(new Claim(&quot;DisplayLanguage&quot;, &quot;en-US&quot;));
identity.AddClaim(new Claim(&quot;UserSetting:Theme&quot;, &quot;dark&quot;));

var principal = new ClaimsPrincipal(identity);
await HttpContext.SignInAsync(CookieAuthenticationDefaults.AuthenticationScheme, principal);
</code></pre>
<h3>Sharing Data Between Systems</h3>
<p>If multiple systems need to share this information, the choice of mechanism would depend on the architecture:</p>
<ol>
<li><strong>APIs</strong>: Systems could call an API to fetch this additional data.</li>
<li><strong>Shared Database</strong>: Systems could look up a shared database to fetch the additional user information.</li>
<li><strong>Token Claims</strong>: If using a standard like OAuth, all the needed information could be packaged into the token itself as claims.</li>
</ol>
<h3>Considerations</h3>
<ol>
<li><strong>Security</strong>: Make sure that the additional information is secured both in transit and at rest.</li>
<li><strong>Consistency</strong>: Ensure that the data is kept consistent across all systems.</li>
<li><strong>Data Size</strong>: Be mindful of the size of the additional data, especially if you are using it in tokens or cookies, as there are size limits to consider.</li>
</ol>
<p>To sum it up, there are various ways to attach and share additional information for authentication across multiple systems. The approach you choose would depend on your specific needs and architecture.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <div id="fa6b4f56-fb53-451d-b117-1f28c5b1627d" class="task done">
                <div class="task_contents">&check;
                    <span class="task_contents">JSON の読み書きのライブラリーを探す</span>
                    <a href="#fa6b4f56-fb53-451d-b117-1f28c5b1627d" class="permalink">&infin;</a>
                </div>
                <div class="notes">
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>JSON (JavaScript Object Notation) manipulation and parsing is a common task in modern programming. C# provides several libraries for handling JSON data. Here are some popular ones:</p>
<h3>System.Text.Json (Built-in)</h3>
<p>Starting from .NET Core 3.0, Microsoft introduced <code>System.Text.Json</code> as a high-performance JSON framework. It's integrated into the .NET Core runtime and doesn't require any additional packages.</p>
<pre><code class="language-csharp">using System.Text.Json;

var jsonString = &quot;{ \&quot;name\&quot;: \&quot;John\&quot;, \&quot;age\&quot;: 30 }&quot;;
var obj = JsonSerializer.Deserialize&lt;Dictionary&lt;string, object&gt;&gt;(jsonString);
var newJsonString = JsonSerializer.Serialize(obj);
</code></pre>
<h3>Newtonsoft.Json (Json.NET)</h3>
<p>Json.NET, created by James Newton-King, is one of the most popular and versatile JSON libraries for .NET. While it's not built into the runtime like <code>System.Text.Json</code>, its rich feature set and flexibility have made it a de facto standard for many years.</p>
<p>To install:</p>
<pre><code class="language-bash">Install-Package Newtonsoft.Json
</code></pre>
<p>Usage example:</p>
<pre><code class="language-csharp">using Newtonsoft.Json;

var jsonString = &quot;{ \&quot;name\&quot;: \&quot;John\&quot;, \&quot;age\&quot;: 30 }&quot;;
var obj = JsonConvert.DeserializeObject&lt;Dictionary&lt;string, object&gt;&gt;(jsonString);
var newJsonString = JsonConvert.SerializeObject(obj);
</code></pre>
<h3>Jil</h3>
<p>Jil is a fast JSON (de)serializer, built on top of <code>Sigil</code> with a number of somewhat crazy optimization tricks.</p>
<p>To install:</p>
<pre><code class="language-bash">Install-Package Jil
</code></pre>
<p>Usage example:</p>
<pre><code class="language-csharp">using Jil;

var jsonString = &quot;{ \&quot;name\&quot;: \&quot;John\&quot;, \&quot;age\&quot;: 30 }&quot;;
var obj = JSON.Deserialize&lt;Dictionary&lt;string, object&gt;&gt;(jsonString);
var newJsonString = JSON.Serialize(obj);
</code></pre>
<h3>Utf8Json</h3>
<p>Utf8Json claims to be a fast JSON serializer for C#. It focuses on performance by directly encoding to UTF-8.</p>
<p>To install:</p>
<pre><code class="language-bash">Install-Package Utf8Json
</code></pre>
<p>Usage example:</p>
<pre><code class="language-csharp">using Utf8Json;

var jsonString = &quot;{ \&quot;name\&quot;: \&quot;John\&quot;, \&quot;age\&quot;: 30 }&quot;;
var obj = JsonSerializer.Deserialize&lt;Dictionary&lt;string, object&gt;&gt;(Encoding.UTF8.GetBytes(jsonString));
var newJsonString = Encoding.UTF8.GetString(JsonSerializer.Serialize(obj));
</code></pre>
<h3>SimpleJson</h3>
<p>SimpleJson is a simple and easy-to-use JSON library that can be incorporated as a single file.</p>
<p>Usage example:</p>
<pre><code class="language-csharp">var jsonString = &quot;{ \&quot;name\&quot;: \&quot;John\&quot;, \&quot;age\&quot;: 30 }&quot;;
var obj = SimpleJson.DeserializeObject&lt;Dictionary&lt;string, object&gt;&gt;(jsonString);
var newJsonString = SimpleJson.SerializeObject(obj);
</code></pre>
<h3>ServiceStack.Text</h3>
<p>ServiceStack's libraries include a JSON serializer among other text-based formats. It is known for its speed and clean API.</p>
<p>To install:</p>
<pre><code class="language-bash">Install-Package ServiceStack.Text
</code></pre>
<p>Usage example:</p>
<pre><code class="language-csharp">using ServiceStack.Text;

var jsonString = &quot;{ \&quot;name\&quot;: \&quot;John\&quot;, \&quot;age\&quot;: 30 }&quot;;
var obj = JsonSerializer.DeserializeFromString&lt;Dictionary&lt;string, object&gt;&gt;(jsonString);
var newJsonString = JsonSerializer.SerializeToString(obj);
</code></pre>
<p>Each of these libraries has its own set of features, optimizations, and use-cases where they excel. You can choose the one that fits your requirements the best.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <div id="056bdbcf-6cf7-477b-94fc-3396cc672d67" class="task done">
                <div class="task_contents">&check;
                    <span class="task_contents">翻訳 API を探す</span>
                    <a href="#056bdbcf-6cf7-477b-94fc-3396cc672d67" class="permalink">&infin;</a>
                </div>
                <div class="notes">
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>Translation APIs allow developers to integrate machine translation into their applications, services, or websites. These APIs support multiple languages and are generally easy to use. Here are some popular translation APIs:</p>
<h3>Google Cloud Translation API</h3>
<p>Google offers a robust and feature-rich translation API that supports a wide variety of languages. The API provides simple text translation as well as language detection.</p>
<ul>
<li><strong>Documentation</strong>: <a href="https://cloud.google.com/translate/docs">Google Cloud Translation API Docs</a></li>
<li><strong>Features</strong>: Text translation, language detection, glossary, and more.</li>
</ul>
<h3>Microsoft Translator Text API</h3>
<p>Part of Azure Cognitive Services, Microsoft's Translator Text API is another feature-rich offering that supports text translation, transliteration, and language detection.</p>
<ul>
<li><strong>Documentation</strong>: <a href="https://docs.microsoft.com/en-us/azure/cognitive-services/translator/">Microsoft Translator Text API Docs</a></li>
<li><strong>Features</strong>: Text translation, transliteration, dictionary lookup, language detection, and more.</li>
</ul>
<h3>IBM Language Translator</h3>
<p>IBM also offers a Language Translator service that supports translation and language identification.</p>
<ul>
<li><strong>Documentation</strong>: <a href="https://cloud.ibm.com/docs/language-translator?topic=language-translator-about">IBM Language Translator Docs</a></li>
<li><strong>Features</strong>: Text translation, identification, and more.</li>
</ul>
<h3>DeepL API</h3>
<p>DeepL offers high-quality translations and claims to outperform other services in terms of translation quality.</p>
<ul>
<li><strong>Documentation</strong>: <a href="https://www.deepl.com/docs-api/">DeepL API Docs</a></li>
<li><strong>Features</strong>: Text translation.</li>
</ul>
<h3>Amazon Translate</h3>
<p>Amazon Translate is a neural machine translation service that is part of AWS. It aims for high-quality, affordable translations.</p>
<ul>
<li><strong>Documentation</strong>: <a href="https://docs.aws.amazon.com/translate/index.html">Amazon Translate Docs</a></li>
<li><strong>Features</strong>: Text translation, batch translation, custom terminology, and more.</li>
</ul>
<h3>Yandex.Translate</h3>
<p>Yandex also offers a translation API, although it's not as feature-rich as some of the other major offerings.</p>
<ul>
<li><strong>Documentation</strong>: <a href="https://tech.yandex.com/translate/">Yandex.Translate API Docs</a></li>
<li><strong>Features</strong>: Text translation, language detection.</li>
</ul>
<h3>MyMemory</h3>
<p>MyMemory is an interesting option as it combines machine translation with a database of human translations, aiming to provide more contextually accurate translations.</p>
<ul>
<li><strong>Documentation</strong>: <a href="https://mymemory.translated.net/doc/spec.php">MyMemory API Docs</a></li>
<li><strong>Features</strong>: Text translation, language detection.</li>
</ul>
<p>These services usually provide SDKs and client libraries in various programming languages, including C#, Python, Java, and more, to facilitate easy integration into your projects.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <div id="728909c4-09ab-4960-83dd-bcb03b2980d7" class="task done">
                <div class="task_contents">&check;
                    <span class="task_contents">ORM ライブラリーを探す</span>
                    <a href="#728909c4-09ab-4960-83dd-bcb03b2980d7" class="permalink">&infin;</a>
                </div>
                <div class="notes">
                    <div class="note">
                        <div class="note_contents">
                            <div class="note_ai_generated">
<p>Object-Relational Mapping (ORM) libraries in C# are used to map between object-oriented code and a relational database, effectively simplifying database operations. Here are some commonly used ORM libraries for C#:</p>
<h3>Entity Framework Core</h3>
<p>Entity Framework Core (EF Core) is a lightweight, extensible, open-source, and cross-platform version of Entity Framework data access technology. It's maintained by Microsoft and has extensive documentation and community support.</p>
<ul>
<li><strong>Features</strong>: Code-First, Database-First, and Model-First approaches; LINQ support, change tracking, migrations, etc.</li>
<li><strong>Official Website</strong>: <a href="https://docs.microsoft.com/en-us/ef/core/">Entity Framework Core</a></li>
</ul>
<h3>Entity Framework 6</h3>
<p>This is the older, full .NET Framework-only version of Entity Framework. It is feature-rich but not as lightweight as EF Core.</p>
<ul>
<li><strong>Features</strong>: Code-First, Database-First, and Model-First approaches; LINQ support, change tracking, migrations, etc.</li>
<li><strong>Official Website</strong>: <a href="https://docs.microsoft.com/en-us/ef/ef6/">Entity Framework 6</a></li>
</ul>
<h3>Dapper</h3>
<p>Dapper is a micro-ORM that provides a set of helper functions for executing queries and mapping results. It's known for high performance and is maintained by Stack Overflow.</p>
<ul>
<li><strong>Features</strong>: Lightweight, high-performance, direct SQL execution.</li>
<li><strong>Official Website</strong>: <a href="https://dapperlib.github.io/Dapper/">Dapper</a></li>
</ul>
<h3>NHibernate</h3>
<p>NHibernate is a mature ORM for .NET that is a port of Java's Hibernate. It's extremely flexible and configurable.</p>
<ul>
<li><strong>Features</strong>: Code-First and Database-First, LINQ support, caching, lazy loading, etc.</li>
<li><strong>Official Website</strong>: <a href="https://nhibernate.info/">NHibernate</a></li>
</ul>
<h3>LLBLGen Pro</h3>
<p>LLBLGen Pro is a commercial ORM for .NET that also supports code generation. It is known for its rich feature set and performance.</p>
<ul>
<li><strong>Features</strong>: Code-First and Database-First, LINQ support, caching, etc.</li>
<li><strong>Official Website</strong>: <a href="https://www.llblgen.com/">LLBLGen Pro</a></li>
</ul>
<h3>PetaPoco</h3>
<p>PetaPoco is another micro-ORM that was inspired by Rob Conery's Massive project but for a different database.</p>
<ul>
<li><strong>Features</strong>: Lightweight, T4-based code generation, and supports multiple databases.</li>
<li><strong>Official Website</strong>: <a href="https://github.com/CollaboratingPlatypus/PetaPoco">PetaPoco</a></li>
</ul>
<h3>Insight.Database</h3>
<p>Insight.Database is a fast and lightweight micro-ORM for .NET. It aims to provide the ease-of-use of an ORM with almost native performance levels by compiling SQL queries and C# data-access code together.</p>
<ul>
<li><strong>Features</strong>: Auto-generation of code from SQL, stored procedure support, etc.</li>
<li><strong>Official Website</strong>: <a href="https://github.com/jonwagner/Insight.Database">Insight.Database</a></li>
</ul>
<h3>Simple.Data</h3>
<p>Simple.Data is a lightweight framework that uses the dynamic features of .NET 4 to provide an expressive, ORM-like API for data access.</p>
<ul>
<li><strong>Features</strong>: Dynamic type support, query syntax, extensible to different database types.</li>
<li><strong>Official Website</strong>: <a href="https://github.com/markrendle/Simple.Data">Simple.Data</a></li>
</ul>
<h3>ServiceStack.OrmLite</h3>
<p>ServiceStack.OrmLite is a configuration-free, lightweight ORM for .NET. It's part of the ServiceStack ecosystem.</p>
<ul>
<li><strong>Features</strong>: POCO-driven, API similar to Dapper, supports complex types, etc.</li>
<li><strong>Official Website</strong>: <a href="https://github.com/ServiceStack/ServiceStack.OrmLite">ServiceStack.OrmLite</a></li>
</ul>
<p>Each of these libraries has its own strengths, weaknesses, and best-use cases. Your choice will depend on your project needs, performance requirements, and the type of application you're building.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </body>
</html>
